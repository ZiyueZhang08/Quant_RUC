#!/usr/bin/env python
# coding: utf-8

# 1.å¯¹æ•°æ®é¢„å¤„ç†ï¼Œè°ƒåŠ¨å¤§æ¨¡å‹å¯¹å®¢æˆ·åé¦ˆè¿›è¡Œæƒ…æ„Ÿè¯„åˆ†

# In[4]:


import numpy as np
import pandas as pd
import torch
import os
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification


# In[ ]:


class SentimentAnalyzer:
    def __init__(self, model_path):
        """åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨ï¼šåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆä»…åŠ è½½ä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´ï¼‰"""
        print("=" * 50)
        print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        self.device = self._get_device()
        self.tokenizer = self._load_tokenizer(model_path)
        self.model = self._load_model(model_path)
        self.batch_size = 64 if self.device.type == "cuda" else 16  # è‡ªåŠ¨é€‚é…è®¾å¤‡
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ä½¿ç”¨è®¾å¤‡ï¼š{self.device} | æ‰¹é‡å¤„ç†å¤§å°ï¼š{self.batch_size}")
        print("=" * 50 + "\n")

    def _get_device(self):
        """è·å–è¿è¡Œè®¾å¤‡ï¼ˆGPUä¼˜å…ˆï¼‰"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆå¤„ç†é€Ÿåº¦å¯èƒ½è¾ƒæ…¢ï¼‰")
            return torch.device("cpu")

    def _load_tokenizer(self, model_path):
        """åŠ è½½åˆ†è¯å™¨ï¼ˆå«è·¯å¾„éªŒè¯ï¼‰"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{model_path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path, local_files_only=True
            )
            return tokenizer
        except Exception as e:
            raise RuntimeError(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼š{e}")

    def _load_model(self, model_path):
        """åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ï¼ˆå«å¼‚å¸¸å¤„ç†ï¼‰"""
        try:
            model = AutoModelForSequenceClassification.from_pretrained(
                model_path, local_files_only=True
            ).to(self.device)
            model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            return model
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")

    def _read_input_file(self, input_path):
        """è¯»å–è¾“å…¥CSVæ–‡ä»¶ï¼ˆæ”¯æŒå¤šç¼–ç ï¼Œå¤„ç†ç©ºå€¼ï¼‰"""
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_path}")
        if not input_path.endswith(".csv"):
            raise ValueError("ä»…æ”¯æŒCSVæ ¼å¼æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åç¼€")

        # å°è¯•å¸¸è§ç¼–ç ï¼Œè§£å†³ä¸­æ–‡ä¹±ç 
        encodings = ["utf-8-sig", "gbk", "utf-8"]
        for encoding in encodings:
            try:
                df = pd.read_csv(input_path, encoding=encoding)
                print(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼ˆç¼–ç ï¼š{encoding}ï¼‰ï¼Œå…±{len(df)}è¡Œæ•°æ®")
                return df
            except UnicodeDecodeError:
                continue
        raise ValueError("æ— æ³•è§£ææ–‡ä»¶ç¼–ç ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–å°è¯•è½¬æ¢ç¼–ç ")

    def _validate_output_path(self, output_input):
        """éªŒè¯å¹¶å¤„ç†è¾“å‡ºè·¯å¾„ï¼ˆè‡ªåŠ¨è¡¥å…¨CSVåç¼€ï¼Œåˆ›å»ºç›®å½•ï¼‰"""
        # å¤„ç†ç”¨æˆ·è¾“å…¥ï¼šè‹¥åªç»™ç›®å½•ï¼Œé»˜è®¤ç”Ÿæˆæ–‡ä»¶åï¼›è‹¥ç»™æ–‡ä»¶åï¼Œè¡¥å…¨CSVåç¼€
        if os.path.isdir(output_input):
            # è¾“å…¥æ˜¯ç›®å½•ï¼Œé»˜è®¤æ–‡ä»¶å
            default_name = "æƒ…æ„Ÿåˆ†æç»“æœ_" + pd.Timestamp.now().strftime("%Y%m%d%H%M%S") + ".csv"
            output_path = os.path.join(output_input, default_name)
        else:
            # è¾“å…¥æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œè¡¥å…¨CSVåç¼€
            output_path = output_input if output_input.endswith(".csv") else output_input + ".csv"

        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
        output_dir = os.path.dirname(output_path)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"âœ… è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•ï¼š{output_dir}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            overwrite = input(f"âš ï¸ æ–‡ä»¶ {output_path} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿï¼ˆy/nï¼‰").strip().lower()
            if overwrite != "y":
                new_name = input("è¯·è¾“å…¥æ–°çš„æ–‡ä»¶åï¼ˆæ— éœ€åç¼€ï¼Œé»˜è®¤CSVï¼‰ï¼š").strip()
                output_path = os.path.join(output_dir, new_name + ".csv")
        return output_path

    def _batch_analyze(self, texts):
        """æ‰¹é‡æƒ…æ„Ÿåˆ†æï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
        all_probs = []
        total_batches = len(texts) // self.batch_size + (1 if len(texts) % self.batch_size != 0 else 0)

        print(f"\nğŸš€ å¼€å§‹æƒ…æ„Ÿåˆ†æï¼ˆå…±{len(texts)}æ¡æ–‡æœ¬ï¼Œ{total_batches}æ‰¹ï¼‰")
        with tqdm(total=total_batches, desc="åˆ†æè¿›åº¦") as pbar:
            for i in range(0, len(texts), self.batch_size):
                batch_texts = texts[i:i+self.batch_size]
                # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…éæ–‡æœ¬æ•°æ®æŠ¥é”™
                batch_texts = [str(text) for text in batch_texts]

                # åˆ†è¯å¹¶æ¨ç†
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    truncation=True,
                    max_length=128,
                    padding=True
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
                all_probs.extend(probs.cpu().numpy())

                pbar.update(1)
        return all_probs

    def analyze_single_file(self):
        """å•æ¬¡æ–‡ä»¶åˆ†ææµç¨‹ï¼šè¾“å…¥è·¯å¾„â†’éªŒè¯â†’åˆ†æâ†’ä¿å­˜"""
        # 1. è¯¢é—®è¾“å…¥æ–‡ä»¶è·¯å¾„
        while True:
            input_path = input("\nè¯·è¾“å…¥éœ€è¦åˆ†æçš„CSVæ–‡ä»¶è·¯å¾„ï¼ˆç¤ºä¾‹ï¼šC:\\data\\input.csvï¼‰ï¼š").strip()
            try:
                df = self._read_input_file(input_path)
                break
            except Exception as e:
                print(f"âŒ è¾“å…¥é”™è¯¯ï¼š{e}ï¼Œè¯·é‡æ–°è¾“å…¥")

        # 2. ç¡®è®¤å¾…åˆ†æåˆ—ï¼ˆé»˜è®¤æœ€åä¸€åˆ—ï¼Œæ”¯æŒç”¨æˆ·é€‰æ‹©ï¼‰
        feedback_col = df.columns[-1]
        change_col = input(f"\né»˜è®¤åˆ†ææœ€åä¸€åˆ—ã€{feedback_col}ã€‘ï¼Œæ˜¯å¦éœ€è¦æ›´æ¢åˆ—ï¼Ÿï¼ˆy/nï¼‰").strip().lower()
        if change_col == "y":
            print(f"å¯é€‰åˆ—åï¼š{list(df.columns)}")
            while True:
                new_col = input("è¯·è¾“å…¥è¦åˆ†æçš„åˆ—åï¼š").strip()
                if new_col in df.columns:
                    feedback_col = new_col
                    break
                else:
                    print(f"âŒ åˆ—åã€{new_col}ã€‘ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
        print(f"ğŸ“ ç¡®è®¤åˆ†æåˆ—ï¼šã€{feedback_col}ã€‘")

        # 3. è¯¢é—®è¾“å‡ºè·¯å¾„å’Œæ–‡ä»¶å
        while True:
            output_input = input("\nè¯·è¾“å…¥ç»“æœä¿å­˜è·¯å¾„ï¼ˆç¤ºä¾‹ï¼šC:\\data\\result æˆ– C:\\data\\my_result.csvï¼‰ï¼š").strip()
            try:
                output_path = self._validate_output_path(output_input)
                break
            except Exception as e:
                print(f"âŒ è¾“å‡ºè·¯å¾„é”™è¯¯ï¼š{e}ï¼Œè¯·é‡æ–°è¾“å…¥")

        # 4. æå–æ–‡æœ¬å¹¶åˆ†æ
        texts = df[feedback_col].fillna("").tolist()
        all_probs = self._batch_analyze(texts)

        # 5. è§£æç»“æœå¹¶ä¿å­˜
        df["æƒ…æ„Ÿ_è´Ÿé¢æ¦‚ç‡"] = [round(p[0], 3) for p in all_probs]
        df["æƒ…æ„Ÿ_æ­£é¢æ¦‚ç‡"] = [round(p[1], 3) for p in all_probs]
        df["æƒ…æ„Ÿæ ‡ç­¾"] = ["æ­£é¢" if p[1] > 0.5 else "è´Ÿé¢" for p in all_probs]
        df["æƒ…æ„Ÿå¾—åˆ†"] = [round(p[1], 3) for p in all_probs]

        # ä¿å­˜æ–‡ä»¶
        try:
            df.to_csv(output_path, index=False, encoding="utf-8-sig")
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")

            # æ˜¾ç¤ºæƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡
            label_count = df["æƒ…æ„Ÿæ ‡ç­¾"].value_counts()
            print(f"\nğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡ï¼š")
            for label, count in label_count.items():
                print(f"- {label}ï¼š{count}æ¡ï¼ˆå æ¯”{count/len(df):.2%}ï¼‰")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜å¤±è´¥ï¼š{e}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å ç”¨")

    def run(self):
        """ä¸»ç¨‹åºå…¥å£ï¼šå¾ªç¯å¤„ç†æ–‡ä»¶ï¼Œæ”¯æŒç»§ç»­/é€€å‡º"""
        print("ğŸ‰ æ¬¢è¿ä½¿ç”¨äº¤äº’å¼æƒ…æ„Ÿåˆ†æç¨‹åºï¼")
        while True:
            self.analyze_single_file()

            # è¯¢é—®æ˜¯å¦ç»§ç»­
            continue_flag = input("\næ˜¯å¦éœ€è¦åˆ†æå…¶ä»–æ–‡ä»¶ï¼Ÿï¼ˆy/nï¼‰").strip().lower()
            if continue_flag != "y":
                print("\nğŸ‘‹ ç¨‹åºç»“æŸï¼Œæ„Ÿè°¢ä½¿ç”¨ï¼")
                break


# --------------------------
# ç¨‹åºå¯åŠ¨å…¥å£ï¼ˆéœ€é…ç½®æ¨¡å‹è·¯å¾„ï¼‰
# --------------------------
if __name__ == "__main__":
    # é…ç½®ä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå›ºå®šä¸€æ¬¡ï¼Œæ— éœ€æ¯æ¬¡è¾“å…¥ï¼‰
    MODEL_PATH = "C:\\Users\\Administrator\\Desktop\\work\\lecture-python-programming.myst-main\\lectures\\model\\Erlangshen-RoBERTa-330M-Sentiment"

    try:
        # åˆå§‹åŒ–åˆ†æå™¨å¹¶å¯åŠ¨
        analyzer = SentimentAnalyzer(MODEL_PATH)
        analyzer.run()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºåˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        print("è¯·æ£€æŸ¥ï¼š1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼›2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼›3. ä¾èµ–åº“æ˜¯å¦å®‰è£…")


# 2.æ­£å¼è¿›è¡Œæ•°æ®å¤„ç†ï¼ŒåŠ è½½åº“å’Œè®¾ç½®æ—¥å¿—æ–¹ä¾¿æ’æŸ¥bug

# In[5]:


import re
import warnings
import logging
from datetime import datetime
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_absolute_error
import xgboost as xgb

# -------------------------- æ—¥å¿—é…ç½® --------------------------
import os
if not os.path.exists('logs'):
    os.makedirs('logs')
if not os.path.exists('predictions'):
    os.makedirs('predictions')

log_filename = f"logs/prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

warnings.filterwarnings('ignore')



# 3.å®šä¹‰ç‰¹å¾å¤„ç†çš„å‡½æ•°

# In[6]:


# -------------------------- å·¥å…·å‡½æ•° --------------------------
def extract_numeric(text, default=np.nan):
    if pd.isna(text) or str(text).strip() in ["", "æ— "]:
        return default
    numeric_match = re.search(r'(\d+\.?\d*)', str(text))
    return float(numeric_match.group(1)) if numeric_match else default


def extract_area(area_str):
    if pd.isna(area_str):
        return np.nan
    try:
        return float(str(area_str).split('ã¡')[0])
    except:
        return extract_numeric(area_str)


def parse_room_info(room_str, is_price=True):
    room, hall, bath = pd.NA, pd.NA, pd.NA
    kitchen = pd.NA if is_price else None
    if pd.isna(room_str) or str(room_str).strip() in ['', 'Â·']:
        result = [room, hall, bath]
        return result + [kitchen] if is_price else result
    s = str(room_str)
    room_match = re.search(r'(\d+)(å®¤|æˆ¿é—´|å±…å®¤)', s)
    hall_match = re.search(r'(\d+)å…', s)
    bath_match = re.search(r'(\d+)å«', s)
    room = int(room_match.group(1)) if room_match else 0
    hall = int(hall_match.group(1)) if hall_match else 0
    bath = int(bath_match.group(1)) if bath_match else 0
    if is_price:
        kitchen_match = re.search(r'(\d+)å¨', s)
        kitchen = int(kitchen_match.group(1)) if kitchen_match else 0
        return [room, hall, kitchen, bath]
    return [room, hall, bath]


def extract_floor(floor_str):
    if pd.isna(floor_str):
        return np.nan, np.nan
    try:
        floor_str = str(floor_str).strip()
        if 'å…±' in floor_str:
            total_match = re.search(r'å…±(\d+)å±‚', floor_str)
            total_num = int(total_match.group(1)) if total_match else np.nan
            if 'åº•' in floor_str:
                current_num = 1
            elif 'é¡¶' in floor_str:
                current_num = total_num if not np.isnan(total_num) else np.nan
            elif 'é«˜' in floor_str and not np.isnan(total_num):
                current_num = int(total_num * 0.8)
            elif 'ä¸­' in floor_str and not np.isnan(total_num):
                current_num = int(total_num * 0.5)
            elif 'ä½' in floor_str and not np.isnan(total_num):
                current_num = int(total_num * 0.2)
            else:
                current_match = re.search(r'(\d+)', floor_str)
                current_num = int(current_match.group(1)) if current_match else np.nan
            return current_num, total_num
        elif '/' in floor_str and 'å±‚' in floor_str:
            parts = floor_str.split('/')
            current_num = int(parts[0]) if parts[0].isdigit() else np.nan
            total_num = int(parts[1].replace('å±‚', '')) if parts[1].replace('å±‚', '').isdigit() else np.nan
            return current_num, total_num
    except Exception as e:
        logger.warning(f"æ¥¼å±‚æå–å¤±è´¥: {floor_str}, é”™è¯¯: {str(e)}")
        return np.nan, np.nan


def process_orientation(orientation_str):
    if pd.isna(orientation_str):
        return 0, 0, 0, 0
    return (
        1 if 'ä¸œ' in orientation_str else 0,
        1 if 'å—' in orientation_str else 0,
        1 if 'è¥¿' in orientation_str else 0,
        1 if 'åŒ—' in orientation_str else 0
    )


def process_decoration(decoration_str):
    if pd.isna(decoration_str):
        return 0, 0, 0, 0, 0
    return (
        1 if 'æ¯›å¯' in decoration_str else 0,
        1 if 'ç®€è£…' in decoration_str else 0,
        1 if 'ä¸­è£…' in decoration_str else 0,
        1 if 'ç²¾è£…' in decoration_str else 0,
        1 if 'è±ªè£…' in decoration_str else 0
    )


def relative_mae(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mean_abs = np.mean(np.abs(y_true))
    if mean_abs == 0:
        logger.warning("ç›®æ ‡å˜é‡å‡å€¼ä¸º0ï¼Œæ— æ³•è®¡ç®—ç›¸å¯¹MAE")
        return 0
    return mae / mean_abs


# æˆ¿ä»·é…å¥—è®¾æ–½å’Œäº¤é€šç‰¹å¾å¤„ç†
def process_price_features(df):
    price_surrounding_categories = {
        'åŒ»è¯ç±»': ['åŒ»é™¢', 'è¯åº—', 'è¯Šæ‰€', 'ä¸­åŒ»é™¢', 'ä¸‰ç”²', 'è¯æˆ¿'],
        'å•†è¶…ç±»': ['å¹¿åœº', 'å•†åœˆ', 'ä¸‡è¾¾', 'åæ¶¦', 'æ°¸è¾‰', 'ç‰©ç¾', 'ç™¾è´§', 'å¤§æ¶¦å‘', 
                  'å•†åœº', 'å•†åŸ', 'æ²ƒå°”ç›', 'å®¶ä¹ç¦', 'ç›’é©¬', 'ç”Ÿé²œ', 'è¶…å¸‚'],
        'æ•™è‚²ç±»': ['å¹¼å„¿å›­', 'å¤§å­¦åŸ', 'å›¾ä¹¦é¦†'],
        'ç”Ÿæ´»æœåŠ¡ç±»': ['å¸‚åœº', 'èœåœº', 'ä¾¿åˆ©åº—', 'é‚®å±€', 'é‚®æ”¿', 'é¥­åº—', 'é¤é¦†', 'ç¾é£Ÿè¡—', 'ç†å‘åº—'],
        'å¨±ä¹ç±»': ['å½±é™¢', 'å½±åŸ', 'ktv', 'é…’å§', 'ç«é”…', 'éº¦å½“åŠ³', 'è‚¯å¾·åŸº', 'æ˜Ÿå·´å…‹'],
        'ä¼‘é—²ç±»': ['èŠ±å›­', 'ç¯®çƒåœº', 'å¥èº«æˆ¿', 'æ¸¸æ³³æ± ', 'ä½“è‚²é¦†', 'åšç‰©é¦†', 'æ¨±èŠ±', 'æ­¥è¡Œè¡—'],
        'å•†ä¸šç±»': ['é‡‘èè¡—', 'å•†é“º']
    }

    traffic_keywords = ['å…¬äº¤', 'åœ°é“', 'å…±äº«å•è½¦']

    if 'å‘¨è¾¹é…å¥—' in df.columns:
        facility_raw = df['å‘¨è¾¹é…å¥—'].astype(str)
        for cat_name, keywords in price_surrounding_categories.items():
            df[f'surrounding_{cat_name}'] = facility_raw.apply(
                lambda x: 1 if any(kw in x for kw in keywords) else 0
            )
        logger.info(f"æˆ¿ä»·å‘¨è¾¹é…å¥—ç‰¹å¾å¤„ç†å®Œæˆï¼ˆ{len(price_surrounding_categories)}ä¸ªå¤§ç±»ï¼‰")
    else:
        logger.warning("æˆ¿ä»·æ•°æ®ä¸­æœªæ‰¾åˆ°'å‘¨è¾¹é…å¥—'åˆ—ï¼Œè·³è¿‡è¯¥ç‰¹å¾å¤„ç†")

    if 'äº¤é€šå‡ºè¡Œ' in df.columns:
        traffic_raw = df['äº¤é€šå‡ºè¡Œ'].astype(str)
        for keyword in traffic_keywords:
            df[f'traffic_{keyword}'] = traffic_raw.apply(lambda x: 1 if keyword in x else 0)
        logger.info(f"æˆ¿ä»·äº¤é€šç‰¹å¾å¤„ç†å®Œæˆï¼ˆä¿ç•™ï¼š{traffic_keywords}ï¼‰")
    else:
        logger.warning("æˆ¿ä»·æ•°æ®ä¸­æœªæ‰¾åˆ°'äº¤é€šå‡ºè¡Œ'åˆ—ï¼Œè·³è¿‡è¯¥ç‰¹å¾å¤„ç†")

    return df


# æˆ¿ç§Ÿé…å¥—è®¾æ–½å¤„ç†ï¼ˆç”Ÿæˆè™šæ‹Ÿå˜é‡ï¼‰
def process_rent_facilities(df):
    rent_facility_keywords = [
        'å®¶å…·', 'å®¶ç”µ', 'åºŠ', 'è¡£æŸœ', 'æ²™å‘', 'æ¡Œå­',  # å®¶å…·ç±»
        'ç©ºè°ƒ', 'å†°ç®±', 'æ´—è¡£æœº', 'ç”µè§†', 'çƒ­æ°´å™¨',     # å®¶ç”µç±»
        'å®½å¸¦', 'WiFi', 'ç½‘ç»œ',                         # ç½‘ç»œç±»
        'æš–æ°”', 'åœ°æš–', 'å£æŒ‚ç‚‰', 'ç©ºè°ƒåˆ¶çƒ­',           # ä¾›æš–ç±»
        'ç”µæ¢¯', 'åœè½¦ä½', 'é˜³å°', 'ç‹¬ç«‹å«ç”Ÿé—´'          # å…¶ä»–ä¾¿åˆ©è®¾æ–½
    ]

    if 'é…å¥—è®¾æ–½' in df.columns:
        facility_raw = df['é…å¥—è®¾æ–½'].astype(str)
        for keyword in rent_facility_keywords:
            df[f'facility_{keyword}'] = facility_raw.apply(
                lambda x: 1 if keyword in x else 0
            )
        logger.info(f"æˆ¿ç§Ÿé…å¥—è®¾æ–½å¤„ç†å®Œæˆï¼ˆç”Ÿæˆ{len(rent_facility_keywords)}ä¸ªè™šæ‹Ÿå˜é‡ï¼‰")
    else:
        logger.warning("æˆ¿ç§Ÿæ•°æ®ä¸­æœªæ‰¾åˆ°'é…å¥—è®¾æ–½'åˆ—ï¼Œè·³è¿‡è¯¥ç‰¹å¾å¤„ç†")
    return df




# In[7]:


# -------------------------- æ•°æ®é¢„å¤„ç†ï¼ˆå¼•å…¥å¯¹æ•°è½¬æ¢å¤„ç†å³åä»·æ ¼ï¼‰ --------------------------
def preprocess_data(df, target_col, is_train=True, is_price=True, fill_values=None, log_params=None):
    try:
        df_clean = df.copy()
        logger.info(f"å¼€å§‹é¢„å¤„ç†{'è®­ç»ƒ' if is_train else 'æµ‹è¯•'}{'æˆ¿ä»·' if is_price else 'æˆ¿ç§Ÿ'}æ•°æ®ï¼ŒåŸå§‹æ ·æœ¬æ•°: {len(df_clean)}")

        # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ­£æ•°ï¼ˆå¯¹æ•°è½¬æ¢å‰æï¼‰
        if is_train and target_col in df_clean.columns:
            # è¿‡æ»¤ä»·æ ¼<=0çš„å¼‚å¸¸å€¼ï¼ˆæ— æ³•è¿›è¡Œå¯¹æ•°è½¬æ¢ï¼‰
            invalid_mask = (df_clean[target_col] <= 0)
            if invalid_mask.sum() > 0:
                logger.warning(f"å‘ç°{invalid_mask.sum()}æ¡ä»·æ ¼<=0çš„è®°å½•ï¼Œå·²è¿‡æ»¤ï¼ˆå¯¹æ•°è½¬æ¢éœ€æ­£å€¼ï¼‰")
                df_clean = df_clean[~invalid_mask].copy()

            # è®­ç»ƒé›†ï¼šè®¡ç®—å¯¹æ•°è½¬æ¢å‚æ•°å¹¶å¤„ç†å¼‚å¸¸å€¼
            if log_params is None:
                log_params = {}
            # å¯¹ä»·æ ¼è¿›è¡Œå¯¹æ•°è½¬æ¢ï¼ˆ+1é¿å…log(0)ï¼Œå¯¹ç»“æœå½±å“å¯å¿½ç•¥ï¼‰
            df_clean[f'{target_col}_log'] = np.log1p(df_clean[target_col])
            log_params['used_log'] = True
            logger.info(f"å¯¹ç›®æ ‡å˜é‡{target_col}è¿›è¡Œå¯¹æ•°è½¬æ¢ï¼ˆlog1pï¼‰ï¼Œç¼“è§£å³ååˆ†å¸ƒ")

            # åŸºäºå¯¹æ•°è½¬æ¢åçš„å€¼å¤„ç†å¼‚å¸¸å€¼ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæ›¿ä»£åŸå§‹ä»·æ ¼çš„IQRï¼‰
            Q1_log = df_clean[f'{target_col}_log'].quantile(0.25)
            Q3_log = df_clean[f'{target_col}_log'].quantile(0.75)
            IQR_log = Q3_log - Q1_log
            df_clean = df_clean[
                (df_clean[f'{target_col}_log'] >= Q1_log - 1.5*IQR_log) & 
                (df_clean[f'{target_col}_log'] <= Q3_log + 1.5*IQR_log)
            ]
            logger.info(f"åŸºäºå¯¹æ•°è½¬æ¢å€¼å¤„ç†å¼‚å¸¸å€¼åæ ·æœ¬æ•°: {len(df_clean)}")
            log_params['iqr_log'] = (Q1_log, Q3_log, IQR_log)  # ä¿å­˜å¯¹æ•°è½¬æ¢åçš„IQRå‚æ•°

        # å¤„ç†æƒ…æ„Ÿå¾—åˆ†
        sentiment_cols = [col for col in df_clean.columns if 'æƒ…æ„Ÿå¾—åˆ†' in col or 'sentiment' in col.lower()]
        if sentiment_cols:
            if len(sentiment_cols) > 1:
                df_clean['æƒ…æ„Ÿå¾—åˆ†'] = df_clean[sentiment_cols].mean(axis=1)
                logger.info(f"åˆå¹¶å¤šä¸ªæƒ…æ„Ÿå¾—åˆ†åˆ—: {sentiment_cols} â†’ æƒ…æ„Ÿå¾—åˆ†")
            else:
                df_clean = df_clean.rename(columns={sentiment_cols[0]: 'æƒ…æ„Ÿå¾—åˆ†'})
        else:
            logger.warning("æœªæ£€æµ‹åˆ°æƒ…æ„Ÿå¾—åˆ†åˆ—")

        # é¢ç§¯ç‰¹å¾
        area_cols = [col for col in df_clean.columns if 'é¢ç§¯' in col]
        for col in area_cols:
            df_clean[f'{col}_æ•°å€¼'] = df_clean[col].apply(extract_area)
        logger.info(f"å¤„ç†é¢ç§¯ç‰¹å¾: {area_cols}")

        # æˆ·å‹ç‰¹å¾
        layout_col = 'æˆ¿å±‹æˆ·å‹' if is_price and 'æˆ¿å±‹æˆ·å‹' in df_clean.columns else 'æˆ·å‹' if 'æˆ·å‹' in df_clean.columns else None
        if layout_col:
            room_features = df_clean[layout_col].apply(lambda x: parse_room_info(x, is_price))
            if is_price:
                df_clean[['å®¤', 'å…', 'å¨', 'å«']] = pd.DataFrame(room_features.tolist(), index=df_clean.index)
            else:
                df_clean[['å®¤', 'å…', 'å«']] = pd.DataFrame(room_features.tolist(), index=df_clean.index)
            for col in ['å®¤', 'å…', 'å«'] + (['å¨'] if is_price else []):
                if col in df_clean.columns:
                    df_clean[col] = df_clean[col].fillna(df_clean[col].median() if is_train else 0)
            logger.info(f"å¤„ç†æˆ·å‹ç‰¹å¾: {layout_col}")
        else:
            logger.warning(f"{'æˆ¿ä»·' if is_price else 'æˆ¿ç§Ÿ'}æ•°æ®ä¸­æœªæ‰¾åˆ°æˆ·å‹åˆ—ï¼Œè·³è¿‡è¯¥ç‰¹å¾å¤„ç†")

        # æ¥¼å±‚ç‰¹å¾
        floor_col = 'æ‰€åœ¨æ¥¼å±‚' if 'æ‰€åœ¨æ¥¼å±‚' in df_clean.columns else 'æ¥¼å±‚' if 'æ¥¼å±‚' in df_clean.columns else None
        if floor_col:
            df_clean[['å½“å‰æ¥¼å±‚', 'æ€»æ¥¼å±‚']] = pd.DataFrame(
                df_clean[floor_col].apply(extract_floor).tolist(), index=df_clean.index
            )
            df_clean['æ¥¼å±‚æ¯”ä¾‹'] = df_clean.apply(
                lambda row: row['å½“å‰æ¥¼å±‚'] / row['æ€»æ¥¼å±‚'] 
                if row['æ€»æ¥¼å±‚'] != 0 and not pd.isna(row['æ€»æ¥¼å±‚']) and not pd.isna(row['å½“å‰æ¥¼å±‚']) 
                else np.nan, axis=1
            )
            logger.info(f"å¤„ç†æ¥¼å±‚ç‰¹å¾: {floor_col}")
        else:
            logger.warning(f"{'æˆ¿ä»·' if is_price else 'æˆ¿ç§Ÿ'}æ•°æ®ä¸­æœªæ‰¾åˆ°æ¥¼å±‚åˆ—ï¼Œè·³è¿‡è¯¥ç‰¹å¾å¤„ç†")

        # æœå‘ç‰¹å¾ï¼ˆåŒºåˆ†åˆ—åï¼‰
        orient_col = 'æˆ¿å±‹æœå‘' if is_price else 'æœå‘'
        if orient_col in df_clean.columns:
            df_clean[['æœä¸œ', 'æœå—', 'æœè¥¿', 'æœåŒ—']] = pd.DataFrame(
                df_clean[orient_col].apply(process_orientation).tolist(), index=df_clean.index
            )
            logger.info(f"å¤„ç†{orient_col}ç‰¹å¾")
        else:
            logger.warning(f"æœªæ‰¾åˆ°'{orient_col}'åˆ—ï¼Œè·³è¿‡æœå‘å¤„ç†")

        # è£…ä¿®ç‰¹å¾ï¼ˆåŒºåˆ†åˆ—åï¼‰
        deco_col = 'è£…ä¿®æƒ…å†µ' if is_price else 'è£…ä¿®'
        if deco_col in df_clean.columns:
            df_clean[['æ¯›å¯', 'ç®€è£…', 'ä¸­è£…', 'ç²¾è£…', 'è±ªè£…']] = pd.DataFrame(
                df_clean[deco_col].apply(process_decoration).tolist(), index=df_clean.index
            )
            logger.info(f"å¤„ç†{deco_col}ç‰¹å¾")
        else:
            logger.warning(f"æœªæ‰¾åˆ°'{deco_col}'åˆ—ï¼Œè·³è¿‡è£…ä¿®å¤„ç†")

        # ç‰¹æœ‰ç‰¹å¾å¤„ç†
        if is_price:
            df_clean = process_price_features(df_clean)
        else:
            df_clean = process_rent_facilities(df_clean)

        # æå–æ•°å€¼åˆ—ï¼ˆç¼ºå¤±å€¼å¡«å……ä¸¥æ ¼åŒºåˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼‰
        numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()
        if 'æƒ…æ„Ÿå¾—åˆ†' in df_clean.columns and 'æƒ…æ„Ÿå¾—åˆ†' not in numeric_cols:
            try:
                df_clean['æƒ…æ„Ÿå¾—åˆ†'] = pd.to_numeric(df_clean['æƒ…æ„Ÿå¾—åˆ†'])
                numeric_cols.append('æƒ…æ„Ÿå¾—åˆ†')
                logger.info("æƒ…æ„Ÿå¾—åˆ†è½¬æ¢ä¸ºæ•°å€¼å‹")
            except:
                logger.warning("æƒ…æ„Ÿå¾—åˆ†æ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œå·²æ’é™¤")

        # è®­ç»ƒé›†ï¼šè®¡ç®—å¡«å……å€¼
        if is_train:
            if target_col in numeric_cols:
                numeric_cols.remove(target_col)
            if f'{target_col}_log' in numeric_cols:
                numeric_cols.remove(f'{target_col}_log')  # æ’é™¤å¯¹æ•°è½¬æ¢åˆ—
            fill_values = {}
            for col in numeric_cols:
                if col == 'æƒ…æ„Ÿå¾—åˆ†':
                    fill_val = df_clean[col].mean() if not np.isnan(df_clean[col].mean()) else 0
                else:
                    fill_val = df_clean[col].median() if not np.isnan(df_clean[col].median()) else 0
                fill_values[col] = fill_val
                df_clean[col].fillna(fill_val, inplace=True)
            logger.info(f"è®­ç»ƒé›†è®¡ç®—å¡«å……å€¼å®Œæˆï¼ˆ{len(fill_values)}ä¸ªç‰¹å¾ï¼‰")
        # æµ‹è¯•é›†ï¼šä½¿ç”¨è®­ç»ƒé›†å¡«å……å€¼
        else:
            if fill_values is None:
                raise ValueError("æµ‹è¯•é›†é¢„å¤„ç†å¿…é¡»ä¼ å…¥è®­ç»ƒé›†çš„fill_values")
            for col in numeric_cols:
                if col in fill_values:
                    df_clean[col].fillna(fill_values[col], inplace=True)
                else:
                    df_clean[col].fillna(0, inplace=True)
            logger.info(f"æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†å¡«å……å€¼å®Œæˆï¼ˆ{len(numeric_cols)}ä¸ªç‰¹å¾ï¼‰")

        # æœ€ç»ˆç¼ºå¤±å€¼å…œåº•
        df_clean[numeric_cols] = df_clean[numeric_cols].fillna(0)

        # æå–ID
        if 'ID' not in df_clean.columns:
            logger.warning("æœªæ‰¾åˆ°IDåˆ—ï¼Œè‡ªåŠ¨ç”ŸæˆID")
            ids = pd.Series(range(len(df_clean)), name='ID')
        else:
            ids = df_clean['ID']

        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡ï¼ˆè®­ç»ƒé›†è¿”å›å¯¹æ•°è½¬æ¢åçš„ç›®æ ‡ï¼‰
        if is_train:
            X = df_clean.select_dtypes(include=['int64', 'float64']).drop(
                columns=[target_col, f'{target_col}_log'], errors='ignore'
            )
            y = df_clean[f'{target_col}_log'] if f'{target_col}_log' in df_clean.columns else df_clean[target_col]
            logger.info(f"\n===== è®­ç»ƒé›†å¤„ç†åç‰¹å¾ï¼ˆå…±{len(X.columns)}ä¸ªï¼‰ =====")
            logger.info(f"ç‰¹å¾åˆ—è¡¨: {list(X.columns)}")
            print(f"\n===== è®­ç»ƒé›†å¤„ç†åç‰¹å¾ =====")
            print(list(X.columns))
            return X, y, ids, fill_values, log_params  # æ–°å¢è¿”å›logå‚æ•°
        else:
            X = df_clean.select_dtypes(include=['int64', 'float64'])
            logger.info(f"\n===== æµ‹è¯•é›†å¤„ç†åç‰¹å¾ï¼ˆå…±{len(X.columns)}ä¸ªï¼‰ =====")
            logger.info(f"ç‰¹å¾åˆ—è¡¨: {list(X.columns)}")
            print(f"\n===== æµ‹è¯•é›†å¤„ç†åç‰¹å¾ =====")
            print(list(X.columns))
            return X, None, ids

    except Exception as e:
        logger.error(f"é¢„å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        raise




# In[8]:


# -------------------------- æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹ï¼ˆé€‚é…å¯¹æ•°è½¬æ¢ï¼‰ --------------------------
def predict_and_evaluate(train_path, test_path, target_name, target_col, is_price=True):
    try:
        logger.info(f"\n===== å¼€å§‹é¢„æµ‹{target_name} =====")

        # è¯»å–æ•°æ®
        def read_csv(path):
            encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin-1', 'iso-8859-1']
            for encoding in encodings:
                try:
                    df = pd.read_csv(path, encoding=encoding)
                    logger.info(f"æˆåŠŸè¯»å–{path}ï¼Œç¼–ç : {encoding}ï¼Œæ ·æœ¬æ•°: {len(df)}")
                    logger.info(f"æ•°æ®åˆ—å: {df.columns.tolist()}")
                    return df
                except:
                    continue
            raise Exception(f"æ— æ³•è¯»å–æ–‡ä»¶: {path}ï¼ˆå·²å°è¯•ç¼–ç : {encodings}ï¼‰")

        train_df = read_csv(train_path)
        test_df = read_csv(test_path)

        # é¢„å¤„ç†ï¼ˆè®­ç»ƒé›†è·å–å¡«å……å€¼å’Œå¯¹æ•°å‚æ•°ï¼‰
        X_train, y_train_log, _, fill_values, log_params = preprocess_data(
            train_df, target_col, is_train=True, is_price=is_price
        )
        X_test, _, test_ids = preprocess_data(
            test_df, target_col, is_train=False, is_price=is_price, fill_values=fill_values
        )

        # ç‰¹å¾å¯¹é½
        common_feats = X_train.columns.intersection(X_test.columns)
        X_train = X_train[common_feats]
        X_test = X_test[common_feats]
        logger.info(f"\n===== å…±åŒç‰¹å¾ï¼ˆå…±{len(common_feats)}ä¸ªï¼‰ =====")
        logger.info(f"å…±åŒç‰¹å¾åˆ—è¡¨: {list(common_feats)}")
        print(f"\n===== å…±åŒç‰¹å¾ =====")
        print(list(common_feats))

        # æ¨¡å‹å®šä¹‰
        models = {
            'OLS': LinearRegression(),
            'å¥—ç´¢': Lasso(alpha=0.1, random_state=42),
            'å²­å›å½’': Ridge(alpha=1.0, random_state=42),
            'XGBoost': xgb.XGBRegressor(
                objective='reg:squarederror', n_estimators=100, 
                learning_rate=0.1, max_depth=5, random_state=42, verbosity=0
            )
        }

        # å…¨å±€ç‰¹å¾é€‰æ‹©å’Œæ ‡å‡†åŒ–
        k = min(30, len(common_feats))
        global_selector = SelectKBest(f_regression, k=k)
        X_train_selected = global_selector.fit_transform(X_train, y_train_log)
        X_test_selected = global_selector.transform(X_test)
        selected_feats = [common_feats[i] for i in range(len(common_feats)) if global_selector.get_support()[i]]
        logger.info(f"\n===== å…¨å±€é€‰æ‹©ç‰¹å¾ï¼ˆå…±{len(selected_feats)}ä¸ªï¼‰ =====")
        logger.info(f"ç‰¹å¾åˆ—è¡¨: {selected_feats}")
        print(f"\n===== å…¨å±€é€‰æ‹©ç‰¹å¾ =====")
        print(selected_feats)

        global_scaler = StandardScaler()
        X_train_scaled = global_scaler.fit_transform(X_train_selected)
        X_test_scaled = global_scaler.transform(X_test_selected)

        # åˆ†å‰²éªŒè¯é›†ï¼ˆåŸºäºå¯¹æ•°è½¬æ¢åçš„ç›®æ ‡ï¼‰
        X_tr, X_val, y_tr_log, y_val_log = train_test_split(
            X_train_scaled, y_train_log, test_size=0.2, random_state=42
        )
        # è¿˜åŸéªŒè¯é›†åŸå§‹ä»·æ ¼ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        y_val_original = np.expm1(y_val_log)  # expm1 = exp(x) - 1ï¼Œå¯¹åº”log1pçš„é€†æ“ä½œ

        # è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°
        metrics = {name: {'æ ·æœ¬å†…': None, 'æ ·æœ¬å¤–': None, '6å€äº¤å‰éªŒè¯': None} for name in models}
        predictions = {}

        for name, model in models.items():
            logger.info(f"\n----- è®­ç»ƒ{name}æ¨¡å‹ -----")
            # åˆæ­¥è®­ç»ƒï¼ˆåŸºäºå¯¹æ•°ç›®æ ‡ï¼‰
            model.fit(X_tr, y_tr_log)
            # æ ·æœ¬å†…è¯„ä¼°ï¼ˆè¿˜åŸä¸ºåŸå§‹ä»·æ ¼ï¼‰
            y_tr_pred_log = model.predict(X_tr)
            y_tr_pred_original = np.expm1(y_tr_pred_log)
            y_tr_original = np.expm1(y_tr_log)
            metrics[name]['æ ·æœ¬å†…'] = round(relative_mae(y_tr_original, y_tr_pred_original), 4)
            # æ ·æœ¬å¤–è¯„ä¼°ï¼ˆè¿˜åŸä¸ºåŸå§‹ä»·æ ¼ï¼‰
            y_val_pred_log = model.predict(X_val)
            y_val_pred_original = np.expm1(y_val_pred_log)
            metrics[name]['æ ·æœ¬å¤–'] = round(relative_mae(y_val_original, y_val_pred_original), 4)

            # äº¤å‰éªŒè¯ï¼ˆåŸºäºå¯¹æ•°ç›®æ ‡ï¼Œè¯„ä¼°æ—¶è¿˜åŸï¼‰
            kf = KFold(n_splits=6, shuffle=True, random_state=42)
            cv_rmae = []
            for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):
                logger.info(f"äº¤å‰éªŒè¯fold {fold+1}/6")
                X_fold_train = X_train.iloc[train_idx]
                y_fold_train_log = y_train_log.iloc[train_idx]
                X_fold_val = X_train.iloc[val_idx]
                y_fold_val_log = y_train_log.iloc[val_idx]
                y_fold_val_original = np.expm1(y_fold_val_log)  # è¿˜åŸçœŸå®å€¼

                # ç‰¹å¾é€‰æ‹©å’Œæ ‡å‡†åŒ–ï¼ˆfoldå†…ï¼‰
                fold_selector = SelectKBest(f_regression, k=k)
                X_fold_train_selected = fold_selector.fit_transform(X_fold_train, y_fold_train_log)
                X_fold_val_selected = fold_selector.transform(X_fold_val)
                fold_scaler = StandardScaler()
                X_fold_train_scaled = fold_scaler.fit_transform(X_fold_train_selected)
                X_fold_val_scaled = fold_scaler.transform(X_fold_val_selected)

                # è®­ç»ƒå’Œé¢„æµ‹ï¼ˆå¯¹æ•°ç©ºé—´ï¼‰
                model.fit(X_fold_train_scaled, y_fold_train_log)
                y_fold_pred_log = model.predict(X_fold_val_scaled)
                y_fold_pred_original = np.expm1(y_fold_pred_log)  # è¿˜åŸé¢„æµ‹å€¼

                cv_rmae.append(relative_mae(y_fold_val_original, y_fold_pred_original))

            metrics[name]['6å€äº¤å‰éªŒè¯'] = round(np.mean(cv_rmae), 4)
            logger.info(f"{name}æŒ‡æ ‡: æ ·æœ¬å†…{metrics[name]['æ ·æœ¬å†…']}, æ ·æœ¬å¤–{metrics[name]['æ ·æœ¬å¤–']}, äº¤å‰éªŒè¯{metrics[name]['6å€äº¤å‰éªŒè¯']}")

            # æµ‹è¯•é›†é¢„æµ‹ï¼ˆå¯¹æ•°ç©ºé—´â†’è¿˜åŸä¸ºåŸå§‹ä»·æ ¼ï¼‰
            model.fit(X_train_scaled, y_train_log)
            test_pred_log = model.predict(X_test_scaled)
            test_pred_original = np.expm1(test_pred_log)  # æ ¸å¿ƒï¼šå°†å¯¹æ•°é¢„æµ‹è½¬æ¢å›åŸå§‹ä»·æ ¼
            test_pred_original = np.maximum(test_pred_original, 0)  # ç¡®ä¿éè´Ÿ
            predictions[name] = pd.DataFrame({
                'ID': test_ids,
                'price': test_pred_original
            })
            logger.info(f"{name}æµ‹è¯•é›†é¢„æµ‹å®Œæˆï¼ˆå·²è¿˜åŸä¸ºåŸå§‹ä»·æ ¼å°ºåº¦ï¼‰")

        metrics_df = pd.DataFrame(metrics).T
        logger.info(f"\n{target_name}æ¨¡å‹æŒ‡æ ‡æ±‡æ€»:\n{metrics_df}")
        return metrics_df, predictions

    except Exception as e:
        logger.error(f"{target_name}å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        raise




# In[9]:


# -------------------------- ä¸»ç¨‹åº --------------------------
def main():
    try:
        logger.info("===== ç¨‹åºå¯åŠ¨ =====")
        config = {
            'æˆ¿ä»·': {
                'train_path': 'train_price.csv',
                'test_path': 'test_price.csv',
                'target_col': 'Price',
                'target_name': 'æˆ¿ä»·',
                'is_price': True
            },
            'æˆ¿ç§Ÿ': {
                'train_path': 'train_rent.csv',
                'test_path': 'test_rent.csv',
                'target_col': 'Price',
                'target_name': 'æˆ¿ç§Ÿ',
                'is_price': False
            }
        }

        # å¤„ç†æˆ¿ä»·å’Œæˆ¿ç§Ÿ
        print("\n===== å¼€å§‹å¤„ç†æˆ¿ä»·æ•°æ® =====")
        price_metrics, price_preds = predict_and_evaluate(**config['æˆ¿ä»·'])
        print("\n===== å¼€å§‹å¤„ç†æˆ¿ç§Ÿæ•°æ® =====")
        rent_metrics, rent_preds = predict_and_evaluate(** config['æˆ¿ç§Ÿ'])

        # åˆå¹¶åŒæ¨¡å‹é¢„æµ‹ç»“æœ
        if price_preds and rent_preds:
            for model_name in price_preds.keys():
                if model_name in rent_preds:
                    merged_df = pd.concat([price_preds[model_name], rent_preds[model_name]], ignore_index=True)
                    save_path = f"predictions/{model_name}_åˆå¹¶é¢„æµ‹.csv"
                    merged_df.to_csv(save_path, index=False, encoding='utf-8-sig')
                    logger.info(f"{model_name}åˆå¹¶é¢„æµ‹ä¿å­˜: {save_path}ï¼ˆå…±{len(merged_df)}æ¡ï¼‰")
                    print(f"{model_name}åˆå¹¶é¢„æµ‹ä¿å­˜: {save_path}")

        # ä¿å­˜æŒ‡æ ‡
        price_metrics.to_csv('æˆ¿ä»·æ¨¡å‹æŒ‡æ ‡.csv', encoding='utf-8-sig')
        rent_metrics.to_csv('æˆ¿ç§Ÿæ¨¡å‹æŒ‡æ ‡.csv', encoding='utf-8-sig')
        print("\n===== æˆ¿ä»·æ¨¡å‹RMAEæŒ‡æ ‡ =====")
        print(price_metrics)
        print("\n===== æˆ¿ç§Ÿæ¨¡å‹RMAEæŒ‡æ ‡ =====")
        print(rent_metrics)
        logger.info("æ‰€æœ‰æŒ‡æ ‡ä¿å­˜å®Œæˆ")

    except Exception as e:
        logger.critical(f"ç¨‹åºè¿è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        print(f"é”™è¯¯: {str(e)}, æ—¥å¿—è¯·æŸ¥çœ‹: {log_filename}")


if __name__ == "__main__":
    main()


# In[ ]:




