{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1e7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- (辅助函数 - 来自 Cell 1) ---\n",
    "def parse_build_year(s):\n",
    "    if pd.isnull(s): return np.nan\n",
    "    s = str(s); years = re.findall(r'(\\d{4})', s)\n",
    "    if not years: return np.nan\n",
    "    return np.mean([float(y) for y in years]) if len(years) > 1 else float(years[0])\n",
    "\n",
    "def parse_first_number(s, default_val=np.nan):\n",
    "    if pd.isnull(s): return default_val\n",
    "    s = str(s).replace('暂无', '0')\n",
    "    match = re.search(r'(\\d+\\.?\\d*)', s)\n",
    "    return float(match.group(1)) if match else default_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360818f1-99c5-4d68-a8d4-7c7a57071dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: 加载并进行通用预处理... ---\n",
      "--- 正在转换 Price 为 log_price... ---\n",
      "--- 正在解析 房屋户型 (提取 室, 厅, 卫)... ---\n",
      "已生成 '室' (众数填充: 3.0), '厅' (众数填充: 2.0), '卫' (众数填充: 1.0) 特征。\n",
      "--- 正在映射 装修情况 为有序数值... ---\n",
      "已生成 '装修情况_mapped' 特征 (缺失值填充为 1)。\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 1: 加载并进行通用预处理 (来自 Cell 2-5) ---\n",
    "print(\"--- STAGE 1: 加载并进行通用预处理... ---\")\n",
    "try:\n",
    "    df_combined = pd.read_csv('combined_price.csv') \n",
    "except FileNotFoundError:\n",
    "    print(\"错误: 未找到 'combined_price.csv'。\")\n",
    "    # exit()\n",
    "\n",
    "# 目标变量\n",
    "if 'Price' in df_combined.columns:\n",
    "    print(\"--- 正在转换 Price 为 log_price... ---\")\n",
    "    df_combined['log_price'] = np.log1p(df_combined['Price']) \n",
    "    df_combined = df_combined.drop('Price', axis=1, errors='ignore')\n",
    "\n",
    "# --- (通用解析 - 不依赖统计) ---\n",
    "# (这些解析逻辑对 train 和 test 同样适用，在拆分前完成)\n",
    "# --- 供水供电重新编码 (新增代码) ---\n",
    "if '供水' in df_combined.columns:\n",
    "    # 将混合类别拆分为独立特征\n",
    "    df_combined['供水_商水'] = df_combined['供水'].str.contains('商水', na=False).astype(int)\n",
    "    df_combined['供水_民水'] = df_combined['供水'].str.contains('民水', na=False).astype(int)\n",
    "    # 删除原始的供水列，避免后续One-Hot编码产生混合类别\n",
    "    df_combined = df_combined.drop('供水', axis=1)\n",
    "\n",
    "if '供电' in df_combined.columns:\n",
    "    df_combined['供电_商电'] = df_combined['供电'].str.contains('商电', na=False).astype(int)\n",
    "    df_combined['供电_民电'] = df_combined['供电'].str.contains('民电', na=False).astype(int)\n",
    "    df_combined = df_combined.drop('供电', axis=1)\n",
    "\n",
    "# 面积 (来自 Cell 4)\n",
    "if '建筑面积' in df_combined.columns:\n",
    "    df_combined['建筑面积'] = df_combined['建筑面积'].astype(str).str.replace('㎡', '', regex=False)\n",
    "    df_combined['log_建筑面积'] = np.log1p(pd.to_numeric(df_combined['建筑面积'], errors='coerce'))\n",
    "\n",
    "# 梯户比例 (来自 Cell 4)\n",
    "if '梯户比例' in df_combined.columns:\n",
    "    chinese_num_map = { \"一\": \"1\", \"二\": \"2\", \"两\": \"2\", \"三\": \"3\", \"四\": \"4\", \"五\": \"5\", \"六\": \"6\", \"七\": \"7\", \"八\": \"8\", \"九\": \"9\", \"十\": \"10\", \"十一\": \"11\", \"十二\": \"12\", \"十三\": \"13\", \"十四\": \"14\", \"十五\": \"15\", \"十六\": \"16\", \"十七\": \"17\", \"十八\": \"18\", \"十九\": \"19\", \"二十\": \"20\", \"二十一\": \"21\", \"二十二\": \"22\", \"二十三\": \"23\", \"二十四\": \"24\", \"二十五\": \"25\", \"二十六\": \"26\", \"二十七\": \"27\", \"二十八\": \"28\", \"二十九\": \"29\", \"三十\": \"30\", \"三十一\": \"31\", \"三十二\": \"32\", \"三十三\": \"33\", \"三十四\": \"34\", \"三十五\": \"35\", \"三十六\": \"36\", \"三十七\": \"37\", \"三十八\": \"38\", \"三十九\": \"39\", \"四十\": \"40\", \"四十一\": \"41\", \"四十二\": \"42\", \"四十三\": \"43\", \"四十四\": \"44\", \"四十五\": \"45\", \"四十六\": \"46\", \"四十七\": \"47\", \"四十八\": \"48\", \"四十九\": \"49\", \"五十\": \"50\", \"五十一\": \"51\", \"五十二\": \"52\", \"五十三\": \"53\", \"五十四\": \"54\", \"五十五\": \"55\", \"五十六\": \"56\", \"五十七\": \"57\", \"五十八\": \"58\", \"五十九\": \"59\", \"六十\": \"60\", \"六十一\": \"61\", \"六十二\": \"62\", \"六十三\": \"63\", \"六十四\": \"64\", \"六十五\": \"65\", \"六十六\": \"66\", \"六十七\": \"67\", \"六十八\": \"68\", \"六十九\": \"69\", \"七十\": \"70\" }\n",
    "    temp_col = df_combined['梯户比例'].astype(str)\n",
    "    sorted_keys = sorted(chinese_num_map.keys(), key=len, reverse=True)\n",
    "    for char in sorted_keys: num_str = chinese_num_map[char]; temp_col = temp_col.str.replace(char, num_str)\n",
    "    df_combined['梯'] = temp_col.str.extract(r'(\\d+)梯').astype(float)\n",
    "    df_combined['户'] = temp_col.str.extract(r'(\\d+)户').astype(float)\n",
    "\n",
    "# 所在楼层 (来自 Cell 4)\n",
    "if '所在楼层' in df_combined.columns:\n",
    "    floor_str = df_combined['所在楼层'].astype(str) \n",
    "    df_combined['总楼层_temp'] = floor_str.str.extract(r'共(\\d+)层')[0].astype(float)\n",
    "    floor_position_str = floor_str.str.extract(r'(地下室|底层|顶层|低楼层|中楼层|高楼层)')[0]\n",
    "    floor_position_map = {'地下室': -1, '底层': 1, '低楼层': 2, '中楼层': 3, '高楼层': 4, '顶层': 5}\n",
    "    df_combined['楼层位置_mapped'] = floor_position_str.map(floor_position_map)\n",
    "\n",
    "# 房屋朝向 (来自 Cell 5)\n",
    "if '房屋朝向' in df_combined.columns:\n",
    "    df_combined['朝南'] = df_combined['房屋朝向'].str.contains('南', na=False).astype(int)\n",
    "    df_combined['朝北'] = df_combined['房屋朝向'].str.contains('北', na=False).astype(int)\n",
    "    df_combined['朝东'] = df_combined['房屋朝向'].str.contains('东', na=False).astype(int)\n",
    "    df_combined['朝西'] = df_combined['房屋朝向'].str.contains('西', na=False).astype(int)\n",
    "\n",
    "# 高缺失率数值 (仅解析) (来自 Cell 5)\n",
    "if '建筑年代' in df_combined.columns: df_combined['建筑年代_parsed'] = df_combined['建筑年代'].apply(parse_build_year)\n",
    "if '房屋总数' in df_combined.columns: df_combined['房屋总数_num'] = pd.to_numeric(df_combined['房屋总数'].astype(str).str.replace('户', '', regex=False), errors='coerce')\n",
    "if '楼栋总数' in df_combined.columns: df_combined['楼栋总数_num'] = pd.to_numeric(df_combined['楼栋总数'].astype(str).str.replace('栋', '', regex=False), errors='coerce')\n",
    "if '绿 化 率' in df_combined.columns: df_combined['绿化率_num'] = pd.to_numeric(df_combined['绿 化 率'].astype(str).str.replace('%', '', regex=False), errors='coerce') / 100.0\n",
    "if '容 积 率' in df_combined.columns: df_combined['容积率_num'] = pd.to_numeric(df_combined['容 积 率'], errors='coerce')\n",
    "if '燃气费' in df_combined.columns: df_combined['燃气费_num'] = df_combined['燃气费'].apply(parse_first_number)\n",
    "if '停车位' in df_combined.columns: df_combined['停车位_num'] = pd.to_numeric(df_combined['停车位'], errors='coerce')\n",
    "if '停车费用' in df_combined.columns: df_combined['停车费用_num'] = df_combined['停车费用'].apply(parse_first_number, default_val=0)\n",
    "\n",
    "# 日期 (仅解析) (来自 Cell 5)\n",
    "if '交易时间' in df_combined.columns: df_combined['交易时间'] = pd.to_datetime(df_combined['交易时间'], errors='coerce')\n",
    "if '上次交易' in df_combined.columns: df_combined['上次交易'] = pd.to_datetime(df_combined['上次交易'], errors='coerce')\n",
    "\n",
    "# 文本标志 (来自 Cell 6)\n",
    "text_cols = ['房屋优势', '周边配套', '交通出行', '核心卖点', '客户反馈']\n",
    "for col in text_cols:\n",
    "    if col in df_combined.columns:\n",
    "        df_combined[f'有_{col}'] = df_combined[col].notnull().astype(int)\n",
    "        if col == '核心卖点':\n",
    "             df_combined[f'{col}_长度'] = df_combined[col].str.len().fillna(0)\n",
    "\n",
    "if '物业类别' in df_combined.columns: df_combined['物业类别'] = df_combined['物业类别'].astype(str)\n",
    "\n",
    "# --- 特征工程: 解析 房屋户型 (提取 室, 厅, 卫) ---\n",
    "if '房屋户型' in df_combined.columns:\n",
    "    print(\"--- 正在解析 房屋户型 (提取 室, 厅, 卫)... ---\")\n",
    "    # 为了稳健地计算众数，先临时填充一下原始列的 NaN (可以用最高频的值)\n",
    "    huxing_mode = df_combined['房屋户型'].mode()[0] if not df_combined['房屋户型'].mode().empty else '2室1厅1卫' # 提供一个备用默认值\n",
    "    temp_huxing = df_combined['房屋户型'].fillna(huxing_mode)\n",
    "\n",
    "    # 计算提取后各部分的众数，用于填充提取失败的 NaN\n",
    "    # 使用 .iloc[0] 来确保即使有多个众数也只取第一个\n",
    "    # 添加检查，确保 .mode() 不为空\n",
    "    shi_series = temp_huxing.str.extract(r'(\\d+)室', expand=False).astype(float)\n",
    "    shi_mode = shi_series.mode().iloc[0] if not shi_series.mode().empty else 2.0 # 备用默认值 2室\n",
    "\n",
    "    ting_series = temp_huxing.str.extract(r'(\\d+)厅', expand=False).astype(float)\n",
    "    ting_mode = ting_series.mode().iloc[0] if not ting_series.mode().empty else 1.0 # 备用默认值 1厅\n",
    "\n",
    "    wei_series = temp_huxing.str.extract(r'(\\d+)卫', expand=False).astype(float)\n",
    "    wei_mode = wei_series.mode().iloc[0] if not wei_series.mode().empty else 1.0 # 备用默认值 1卫\n",
    "\n",
    "    # --- 执行提取并填充 ---\n",
    "    # 使用 .str.extract 并指定 expand=False 返回 Series，然后转换类型和填充\n",
    "    df_combined['室'] = df_combined['房屋户型'].str.extract(r'(\\d+)室', expand=False).astype(float).fillna(shi_mode)\n",
    "    df_combined['厅'] = df_combined['房屋户型'].str.extract(r'(\\d+)厅', expand=False).astype(float).fillna(ting_mode)\n",
    "    df_combined['卫'] = df_combined['房屋户型'].str.extract(r'(\\d+)卫', expand=False).astype(float).fillna(wei_mode)\n",
    "\n",
    "    # (可选) 处理特殊情况，例如 \"房间\" 可能需要映射到 '室'，根据需要调整\n",
    "    # 例如: df_combined.loc[df_combined['房屋户型'].str.contains('房间', na=False), '室'] = df_combined['房屋户型'].str.extract(r'(\\d+)房间', expand=False).astype(float).fillna(shi_mode)\n",
    "\n",
    "    print(f\"已生成 '室' (众数填充: {shi_mode}), '厅' (众数填充: {ting_mode}), '卫' (众数填充: {wei_mode}) 特征。\")\n",
    "\n",
    "# (确保后续步骤不再错误地删除 '室', '厅', '卫'，但要删除原始的 '房屋户型')\n",
    "# 在 STAGE 8 的 original_cols_to_drop 列表中，确保 '房屋户型' 在里面，但 '室', '厅', '卫' 不在里面。\n",
    "\n",
    "\n",
    "# --- 特征工程: 映射 装修情况 (有序编码) ---\n",
    "if '装修情况' in df_combined.columns:\n",
    "    print(\"--- 正在映射 装修情况 为有序数值... ---\")\n",
    "    map_zhuangxiu = {'毛坯': 0, '简装': 1, '精装': 2, '其他': 1} # 定义映射关系，'其他' 视为 '简装'\n",
    "\n",
    "    # 确定一个合理的填充值 (例如，使用 '简装'/'其他' 对应的 1 作为默认值)\n",
    "    # price_model 中也是将缺失值最终处理为 1\n",
    "    zhuangxiu_default_val = 1\n",
    "\n",
    "    # 应用映射，并使用 .fillna() 处理原始列中的 NaN 或不在映射字典中的值\n",
    "    df_combined['装修情况_mapped'] = df_combined['装修情况'].map(map_zhuangxiu).fillna(zhuangxiu_default_val)\n",
    "\n",
    "    # (可选) 确保结果是整数类型，如果需要的话\n",
    "    # df_combined['装修情况_mapped'] = df_combined['装修情况_mapped'].astype(int)\n",
    "\n",
    "    print(f\"已生成 '装修情况_mapped' 特征 (缺失值填充为 {zhuangxiu_default_val})。\")\n",
    "\n",
    "# (确保后续步骤不再错误地删除 '装修情况_mapped'，但要删除原始的 '装修情况')\n",
    "# 在 STAGE 8 的 original_cols_to_drop 列表中，确保 '装修情况' 在里面，但 '装修情况_mapped' 不在里面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ba7333-b79a-47d5-b941-ff9ca5eaed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: 拆分数据集... ---\n",
      "原始训练集维度: (103871, 85)\n",
      "原始测试集维度: (34017, 85)\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 2: 拆分数据集 (来自 Cell 6) ---\n",
    "print(\"\\n--- STAGE 2: 拆分数据集... ---\")\n",
    "if 'train' not in df_combined.columns:\n",
    "    raise KeyError(\"合并的数据集中必须包含 'train' 指示列 (1=train, 0=test)\")\n",
    "\n",
    "train_df = df_combined[df_combined['train'] == 1].copy()\n",
    "test_df = df_combined[df_combined['train'] == 0].copy()\n",
    "\n",
    "train_df = train_df.drop('train', axis=1)\n",
    "test_df = test_df.drop('train', axis=1)\n",
    "\n",
    "print(f\"原始训练集维度: {train_df.shape}\")\n",
    "print(f\"原始测试集维度: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355395e4-fa2e-4502-8403-7050e5b3c044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 3: 【新】 根据您的要求，清理训练集... ---\n",
      "--- 正在删除在 ['房屋户型', '建筑结构', '装修情况', '梯户比例', 'log_建筑面积', '总楼层_temp', '楼层位置_mapped'] 中包含缺失值的训练行... ---\n",
      "--- 训练集行数从 103871 减少到 101252 ---\n",
      "\n",
      "--- STAGE 4: 仅在【清理后】的训练集上计算所有填充和盖帽规则... ---\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 3: 【*新*】 清理训练集 ---\n",
    "print(\"\\n--- STAGE 3: 【新】 根据您的要求，清理训练集... ---\")\n",
    "\n",
    "# 定义缺失率较低的、必须有值的列 (来自 Cell 7 的 'cols_to_fill_mode_strict')\n",
    "critical_cols = [\n",
    "    '房屋户型', '建筑结构', '装修情况', '梯户比例', # 低缺失类别\n",
    "    'log_建筑面积', # 关键数值\n",
    "    '总楼层_temp', # 关键数值\n",
    "    '楼层位置_mapped' # 关键数值\n",
    "]\n",
    "# 确保只使用 train_df 中实际存在的列\n",
    "critical_cols_exist = [col for col in critical_cols if col in train_df.columns]\n",
    "\n",
    "if critical_cols_exist:\n",
    "    print(f\"--- 正在删除在 {critical_cols_exist} 中包含缺失值的训练行... ---\")\n",
    "    original_train_count = len(train_df)\n",
    "    train_df_cleaned = train_df.dropna(subset=critical_cols_exist)\n",
    "    new_train_count = len(train_df_cleaned)\n",
    "    print(f\"--- 训练集行数从 {original_train_count} 减少到 {new_train_count} ---\")\n",
    "else:\n",
    "    print(\"--- 未找到指定的关键列，跳过训练集行删除 ---\")\n",
    "    train_df_cleaned = train_df.copy() # 保持一致性\n",
    "\n",
    "# --- STAGE 4: 仅在【清理后】的训练集上计算所有规则 (来自 Cell 7) ---\n",
    "print(\"\\n--- STAGE 4: 仅在【清理后】的训练集上计算所有填充和盖帽规则... ---\")\n",
    "\n",
    "# 规则 A: 按板块的众数\n",
    "cols_to_fill_mode_strict = ['房屋户型', '建筑结构', '装修情况', '梯户比例']\n",
    "cols_to_fill_mode_other = ['物业类别', '建筑结构_comm', '配备电梯', '房屋朝向', '房屋用途','城市']\n",
    "cols_to_fill_mode_all = cols_to_fill_mode_strict + cols_to_fill_mode_other\n",
    "\n",
    "# 计算每个板块的众数\n",
    "train_df_cleaned['板块'] = train_df_cleaned['板块'].fillna(train_df_cleaned['板块'].mode()[0])\n",
    "grouped_modes = train_df_cleaned.groupby('板块')[cols_to_fill_mode_all].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "global_modes = train_df_cleaned[cols_to_fill_mode_all].mode().iloc[0]\n",
    "\n",
    "# 规则 B: 中位数\n",
    "cols_to_fill_median_strict = [\n",
    "    '建筑年代_parsed', '房屋总数_num', '楼栋总数_num', '绿化率_num', \n",
    "    '容积率_num', '燃气费_num', '停车位_num', '停车费用_num', \n",
    "    '总楼层_temp', '梯', '户', '楼层位置_mapped'\n",
    "]\n",
    "cols_to_fill_median_strict = [col for col in cols_to_fill_median_strict if col in train_df_cleaned.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecdda10f-adf9-439e-bc06-9bfaa8b01d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在确定最优K-Means聚类数量（使用 ['lon', 'lat']）... ---\n",
      "--- 最优聚类数量: 5 ---\n",
      "--- 所有规则已计算完毕 ---\n",
      "\n",
      "--- 正在计算 *早期可用特征* 的分箱边界 (基于 train_df_cleaned)... ---\n",
      "为 'log_建筑面积' 计算了 6 个分箱的边界: [2.62 4.12 4.39 4.52 4.67 4.87 6.22]\n",
      "--- 早期分箱边界计算完毕。将在 apply_rules 应用后计算 ['房龄', '持有天数'] 的边界。 ---\n"
     ]
    }
   ],
   "source": [
    "# 【修改】使用 train_df_cleaned\n",
    "# 填充 '板块' 以便分组 (仅在清理后的训练集上)\n",
    "train_df_cleaned['板块'] = train_df_cleaned['板块'].fillna(train_df_cleaned['板块'].mode()[0]) \n",
    "train_bankuai_mode = train_df_cleaned['板块'].mode()[0] # 规则\n",
    "\n",
    "# 【修改】使用 train_df_cleaned\n",
    "grouped_medians = train_df_cleaned.groupby('板块')[cols_to_fill_median_strict].median()\n",
    "global_medians = train_df_cleaned[cols_to_fill_median_strict].median()\n",
    "\n",
    "# 规则 C: 盖帽值\n",
    "cap_values = {}\n",
    "# 【修改】使用 train_df_cleaned\n",
    "if '持有天数' in train_df_cleaned.columns: cap_values['持有天数_lower'] = 0 \n",
    "if '总楼层_temp' in train_df_cleaned.columns: cap_values['总楼层_lower'] = 1 \n",
    "if '绿化率_num' in train_df_cleaned.columns: cap_values['绿化率_num_upper'] = 1.0 \n",
    "if '容积率_num' in train_df_cleaned.columns: cap_values['容积率_num_upper'] = train_df_cleaned['容积率_num'].quantile(0.99)\n",
    "if '户' in train_df_cleaned.columns:\n",
    "    p_99_hu_train = train_df_cleaned['户'].quantile(0.99)\n",
    "    cap_values['户_upper'] = min(p_99_hu_train if pd.notna(p_99_hu_train) else 50, 50) \n",
    "if '梯' in train_df_cleaned.columns:\n",
    "    p_99_ti_train = train_df_cleaned['梯'].quantile(0.99)\n",
    "    cap_values['梯_upper'] = min(p_99_ti_train if pd.notna(p_99_ti_train) else 20, 20)\n",
    "\n",
    "# 规则 D: K-Means 拟合\n",
    "kmeans_model = None\n",
    "coord_cols = []\n",
    "# 【修改】使用 train_df_cleaned\n",
    "if 'lon' in train_df_cleaned.columns and 'lat' in train_df_cleaned.columns and train_df_cleaned['lon'].notna().all() and train_df_cleaned['lat'].notna().all():\n",
    "    coord_cols = ['lon', 'lat']\n",
    "elif 'coord_x' in train_df_cleaned.columns and 'coord_y' in train_df_cleaned.columns:\n",
    "     train_df_cleaned['coord_x'] = train_df_cleaned['coord_x'].fillna(train_df_cleaned['coord_x'].median())\n",
    "     train_df_cleaned['coord_y'] = train_df_cleaned['coord_y'].fillna(train_df_cleaned['coord_y'].median())\n",
    "     coord_cols = ['coord_x', 'coord_y']\n",
    "\n",
    "# --- 新增：确定最优K-Means聚类数量 ---\n",
    "if coord_cols:\n",
    "    print(f\"--- 正在确定最优K-Means聚类数量（使用 {coord_cols}）... ---\")\n",
    "    \n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    # 准备坐标数据\n",
    "    coords = train_df_cleaned[coord_cols].values\n",
    "    \n",
    "    # 尝试不同的聚类数量\n",
    "    k_range = range(3, 12)  # 尝试3到11个聚类\n",
    "    wcss = []  # 簇内平方和\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=111, n_init=10)\n",
    "        kmeans.fit(coords)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    # 使用肘部法则确定最优k\n",
    "    # 计算每个k对应的斜率变化（二阶导数）\n",
    "    wcss_diff = [wcss[i-1] - wcss[i] for i in range(1, len(wcss))]\n",
    "    wcss_diff_diff = [wcss_diff[i-1] - wcss_diff[i] for i in range(1, len(wcss_diff))]\n",
    "    \n",
    "    # 找到斜率变化最大的点（肘部）\n",
    "    optimal_k = k_range[wcss_diff_diff.index(max(wcss_diff_diff)) + 2]\n",
    "    print(f\"--- 最优聚类数量: {optimal_k} ---\")\n",
    "    \n",
    "    # 使用最优k训练最终模型\n",
    "    kmeans_model = KMeans(n_clusters=10, random_state=111, n_init=10)\n",
    "    kmeans_model.fit(train_df_cleaned[coord_cols])\n",
    "else:\n",
    "    print(\"警告: 未找到合适的坐标列 (lon/lat 或 coord_x/y) 用于 KMeans。\")\n",
    "    kmeans_model = None\n",
    "\n",
    "print(\"--- 所有规则已计算完毕 ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 正在计算 *早期可用特征* 的分箱边界 (基于 train_df_cleaned)... ---\")\n",
    "\n",
    "bin_edges = {} # 用于存储所有特征的分箱边界\n",
    "# 只包含 STAGE 4 时已存在的特征\n",
    "features_to_bin_early = ['log_建筑面积']\n",
    "# 稍后在 apply_rules 计算后处理的特征\n",
    "features_to_bin_late = ['房龄', '持有天数']\n",
    "n_bins = 6 # 定义箱数\n",
    "\n",
    "# --- 计算早期特征的边界 ---\n",
    "for feature in features_to_bin_early:\n",
    "    if feature in train_df_cleaned.columns:\n",
    "        try:\n",
    "            _, edges = pd.qcut(train_df_cleaned[feature], q=n_bins, labels=False, retbins=True, duplicates='drop')\n",
    "            bin_edges[feature] = np.unique(edges) # 存储边界\n",
    "            print(f\"为 '{feature}' 计算了 {len(bin_edges[feature])-1} 个分箱的边界: {np.round(bin_edges[feature], 2)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"为 '{feature}' 计算分箱边界时出错: {e}. 跳过此特征。\")\n",
    "            if feature in bin_edges: del bin_edges[feature]\n",
    "    else:\n",
    "        print(f\"警告: 特征 '{feature}' 不在 train_df_cleaned 中，无法进行分箱。\")\n",
    "\n",
    "print(f\"--- 早期分箱边界计算完毕。将在 apply_rules 应用后计算 {features_to_bin_late} 的边界。 ---\")\n",
    "# --- >>> 修改代码结束 <<< ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f169d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 5: 定义应用规则的函数... ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- STAGE 5: 定义应用规则的函数 (来自 Cell 8 - *已修正结构*) ---\n",
    "# (这个函数现在至关重要，它将用于填充 test_df 和 train_df_cleaned 中的剩余高缺失列)\n",
    "print(\"\\n--- STAGE 5: 定义应用规则的函数... ---\")\n",
    "\n",
    "def apply_rules(df, is_test_set=False):\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 确保板块没有缺失值\n",
    "    df_processed['板块'] = df_processed['板块'].fillna(train_bankuai_mode)\n",
    "    \n",
    "    # 规则 A: 按板块众数填充分类变量\n",
    "    df_processed = df_processed.merge(grouped_modes, on='板块', how='left', suffixes=('', '_train_mode'))\n",
    "    \n",
    "    for col in cols_to_fill_mode_all:\n",
    "        mode_col_name = f'{col}_train_mode'\n",
    "        df_processed[col] = df_processed[col].fillna(df_processed[mode_col_name])\n",
    "        df_processed[col] = df_processed[col].fillna(global_modes[col])\n",
    "    \n",
    "    # 删除辅助列\n",
    "    helper_cols_mode = [f'{col}_train_mode' for col in cols_to_fill_mode_all]\n",
    "    df_processed = df_processed.drop(columns=helper_cols_mode, errors='ignore')\n",
    "    \n",
    "# --- >>> 新增代码开始: 应用分箱 <<< ---\n",
    "    print(\"--- (apply_rules) 正在应用分箱... ---\")\n",
    "    global bin_edges # 确保能访问到 STAGE 4 计算的边界\n",
    "\n",
    "    for feature, edges in bin_edges.items():\n",
    "        if feature in df_processed.columns:\n",
    "            binned_col_name = f'{feature}_binned'\n",
    "            try:\n",
    "                # 使用 pd.cut 应用分箱边界\n",
    "                # include_lowest=True 包含最小值\n",
    "                # labels=False 返回整数标签 (0, 1, 2, ...)\n",
    "                df_processed[binned_col_name] = pd.cut(df_processed[feature], bins=edges, labels=False, include_lowest=True)\n",
    "                # 将 NaN 填充为一个特殊的类别（例如 -1），或者用众数填充\n",
    "                # 这里选择填充 -1，表示缺失或超出边界\n",
    "                df_processed[binned_col_name] = df_processed[binned_col_name].fillna(-1).astype(int)\n",
    "                print(f\"已生成分箱特征: {binned_col_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"应用分箱到 '{feature}' 时出错: {e}\")\n",
    "        else:\n",
    "             print(f\"--- (apply_rules) 警告: 特征 '{feature}' 不在待处理数据框中，无法应用分箱。---\")\n",
    "    # --- >>> 新增代码结束 <<< ---\n",
    "\n",
    "\n",
    "    \n",
    "# --- >>> 新增代码开始 <<< ---\n",
    "    # --- 在填充 '配备电梯' 之后，进行二元映射 ---\n",
    "    if '配备电梯' in df_processed.columns:\n",
    "        print(\"--- 正在映射 '配备电梯' 为 0/1 ... ---\")\n",
    "        # 此时 '配备电梯' 列应该已经被上面的循环填充完毕\n",
    "        df_processed['配备电梯_mapped'] = df_processed['配备电梯'].map({'有': 1, '无': 0})\n",
    "\n",
    "        # 添加一个额外的 .fillna(0) 作为保险，处理极其罕见的填充失败或原始值不是'有'/'无'的情况\n",
    "        # 假设没有电梯 (0) 是更安全的默认值\n",
    "        if df_processed['配备电梯_mapped'].isnull().any():\n",
    "             print(\"警告: '配备电梯_mapped' 中发现 NaN，将用 0 填充。\")\n",
    "             df_processed['配备电梯_mapped'] = df_processed['配备电梯_mapped'].fillna(0)\n",
    "\n",
    "        # 确保是整数类型\n",
    "        df_processed['配备电梯_mapped'] = df_processed['配备电梯_mapped'].astype(int)\n",
    "\n",
    "    \n",
    "    # 规则 B: 按板块中位数填充数值变量（保持不变）\n",
    "    df_processed = df_processed.merge(grouped_medians, on='板块', how='left', suffixes=('', '_train_median'))\n",
    "    \n",
    "    for col in cols_to_fill_median_strict:\n",
    "        median_col_name = f'{col}_train_median'\n",
    "        df_processed[col] = df_processed[col].fillna(df_processed[median_col_name])\n",
    "        df_processed[col] = df_processed[col].fillna(global_medians[col])\n",
    "    \n",
    "    helper_cols_median = [f'{col}_train_median' for col in cols_to_fill_median_strict]\n",
    "    df_processed = df_processed.drop(columns=helper_cols_median, errors='ignore')\n",
    "\n",
    "    \n",
    "    if '总楼层_temp' in df_processed.columns:\n",
    "        df_processed.rename(columns={'总楼层_temp': '总楼层'}, inplace=True)\n",
    "\n",
    "    # 计算 '梯户比' 和 '楼层相对位置' (在填充后)\n",
    "    if '梯' in df_processed.columns and '户' in df_processed.columns:\n",
    "        df_processed['户'] = df_processed['户'].replace(0, 2) \n",
    "        df_processed['梯户比'] = df_processed['梯'] / df_processed['户']\n",
    "    if '楼层位置_mapped' in df_processed.columns and '总楼层' in df_processed.columns:\n",
    "        df_processed['总楼层'] = df_processed['总楼层'].clip(lower=1) \n",
    "        df_processed['楼层相对位置'] = df_processed['楼层位置_mapped'] / df_processed['总楼层']\n",
    "\n",
    "    # 计算 '房龄'\n",
    "    if '建筑年代_parsed' in df_processed.columns:\n",
    "        df_processed['房龄'] = 2025 - df_processed['建筑年代_parsed']\n",
    "\n",
    "    # --- 多标签二值化 (建筑结构_comm) ---\n",
    "    if '建筑结构_comm' in df_processed.columns:\n",
    "        main_types_jiegou_comm = ['塔楼', '板楼', '塔板结合', '平房']\n",
    "        for j_type in main_types_jiegou_comm:\n",
    "            df_processed[f'建筑结构_comm_{j_type}'] = df_processed['建筑结构_comm'].astype(str).str.contains(j_type, na=False).astype(int)\n",
    "            \n",
    "    # --- 特征工程: 多标签 (物业类别) ---\n",
    "    if '物业类别' in df_processed.columns:\n",
    "        main_types = ['普通住宅', '别墅', '写字楼', '商业', '公寓', '底商', '车库', '花园洋房', '平房', '新式里弄', '老公寓']\n",
    "        for prop_type in main_types:\n",
    "            df_processed[f'物业类型_{prop_type}'] = df_processed['物业类别'].astype(str).str.contains(prop_type, na=False).astype(int)\n",
    "    \n",
    "    # --- 特征工程: 多标签 (房屋用途) ---\n",
    "    if '房屋用途' in df_processed.columns:\n",
    "            main_types_yongtu = [\n",
    "                '普通住宅', '别墅', '商业办公类', '车库', '公寓', '酒店式公寓', \n",
    "                '四合院', '商务型公寓', '住宅式公寓', '商住两用', '新式里弄', \n",
    "                '老公寓', '花园洋房', '底商', '商业', '商务公寓', '写字楼', '住宅'\n",
    "            ]\n",
    "            for y_type in main_types_yongtu:\n",
    "                df_processed[f'房屋用途_{y_type}'] = df_processed['房屋用途'].astype(str).str.contains(y_type, na=False).astype(int)\n",
    "\n",
    "    # (!!! 以下是您 Cell 8 中在函数定义之外的逻辑，已移入函数内 !!!)\n",
    "    \n",
    "    # 规则 C: 盖帽\n",
    "    if '持有天数' in df_processed.columns: df_processed['持有天数'] = df_processed['持有天数'].clip(lower=cap_values.get('持有天数_lower'))\n",
    "    if '总楼层' in df_processed.columns: df_processed['总楼层'] = df_processed['总楼层'].clip(lower=cap_values.get('总楼层_lower'))\n",
    "    if '绿化率_num' in df_processed.columns: df_processed['绿化率_num'] = df_processed['绿化率_num'].clip(upper=cap_values.get('绿化率_num_upper'))\n",
    "    if '容积率_num' in df_processed.columns: df_processed['容积率_num'] = df_processed['容积率_num'].clip(upper=cap_values.get('容积率_num_upper'))\n",
    "    if '户' in df_processed.columns: df_processed['户'] = df_processed['户'].clip(upper=cap_values.get('户_upper'))\n",
    "    if '梯' in df_processed.columns: df_processed['梯'] = df_processed['梯'].clip(upper=cap_values.get('梯_upper'))\n",
    "    \n",
    "    if '梯户比' in df_processed.columns:\n",
    "         p_99_bihu_train_cap = cap_values.get('梯户比_upper')\n",
    "         if p_99_bihu_train_cap is not None:\n",
    "             df_processed['梯户比'] = df_processed['梯户比'].clip(upper=p_99_bihu_train_cap)\n",
    "\n",
    "    # 规则 D: K-Means 预测\n",
    "    if kmeans_model is not None and coord_cols:\n",
    "        for c_col in coord_cols:\n",
    "             if c_col not in df_processed.columns or df_processed[c_col].isnull().any():\n",
    "                  # (使用全局中位数填充坐标，以防万一)\n",
    "                  coord_median = global_medians.get(c_col, df_processed[c_col].median())\n",
    "                  df_processed[c_col] = df_processed[c_col].fillna(coord_median)\n",
    "        df_processed['location_cluster'] = kmeans_model.predict(df_processed[coord_cols])\n",
    "        df_processed['location_cluster'] = df_processed['location_cluster'].astype('category')\n",
    "\n",
    "    # 特征工程: 日期\n",
    "    if '交易时间' in df_processed.columns:\n",
    "        df_processed['交易年份'] = df_processed['交易时间'].dt.year\n",
    "        df_processed['交易月份'] = df_processed['交易时间'].dt.month\n",
    "    if '交易时间' in df_processed.columns and '上次交易' in df_processed.columns:\n",
    "         df_processed['持有天数'] = (df_processed['交易时间'] - df_processed['上次交易']).dt.days\n",
    "         df_processed['是否首次交易'] = df_processed['上次交易'].isnull().astype(int)\n",
    "         df_processed['持有天数'] = df_processed['持有天数'].fillna(0)\n",
    "         if '持有天数_lower' in cap_values:\n",
    "             df_processed['持有天数'] = df_processed['持有天数'].clip(lower=cap_values['持有天数_lower'])\n",
    "\n",
    "    df_processed = df_processed.drop(['coord_x', 'coord_y'], axis=1, errors='ignore')\n",
    "\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1554c026-06b3-466f-bb5d-124db65f7321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在计算 *后期生成特征* 的分箱边界... ---\n",
      "--- (需要先对 train_df_cleaned 应用一次 apply_rules 以生成特征) ---\n",
      "--- (apply_rules) 正在应用分箱... ---\n",
      "已生成分箱特征: log_建筑面积_binned\n",
      "--- 正在映射 '配备电梯' 为 0/1 ... ---\n",
      "为 '房龄' 计算了 5 个分箱的边界: [ 3.5 11.  14.5 17.  23.  92. ]\n",
      "为 '持有天数' 计算了 6 个分箱的边界: [-31098.        0.     1259.     2011.     2854.     4383.83  19790.  ]\n",
      "--- 所有分箱边界计算完毕 ---\n",
      "--- 完整的 bin_edges 字典: {'log_建筑面积': [2.62, 4.12, 4.39, 4.52, 4.67, 4.87, 6.22], '房龄': [3.5, 11.0, 14.5, 17.0, 23.0, 92.0], '持有天数': [-31098.0, 0.0, 1259.0, 2011.0, 2854.0, 4383.83, 19790.0]}\n"
     ]
    }
   ],
   "source": [
    "# --- >>> 新增代码开始: 计算后期特征的分箱边界 <<< ---\n",
    "print(\"\\n--- 正在计算 *后期生成特征* 的分箱边界... ---\")\n",
    "print(\"--- (需要先对 train_df_cleaned 应用一次 apply_rules 以生成特征) ---\")\n",
    "\n",
    "# 临时应用 apply_rules 以获得包含 '房龄' 和 '持有天数' 的训练集\n",
    "# 注意：这里调用 apply_rules 主要是为了生成列，里面的分箱应用会因为 bin_edges 不全而部分跳过，没关系\n",
    "temp_train_processed_for_bins = apply_rules(train_df_cleaned, is_test_set=False)\n",
    "\n",
    "# 现在 temp_train_processed_for_bins 中应该有 '房龄' 和 '持有天数' 了\n",
    "for feature in features_to_bin_late: # 使用之前定义的后期特征列表\n",
    "    if feature in temp_train_processed_for_bins.columns:\n",
    "        try:\n",
    "            # 确保数据无 NaN (apply_rules 应该已经处理了，这里是保险)\n",
    "            feature_data = temp_train_processed_for_bins[feature].dropna()\n",
    "            if not feature_data.empty:\n",
    "                _, edges = pd.qcut(feature_data, q=n_bins, labels=False, retbins=True, duplicates='drop')\n",
    "                bin_edges[feature] = np.unique(edges) # 更新 bin_edges 字典\n",
    "                print(f\"为 '{feature}' 计算了 {len(bin_edges[feature])-1} 个分箱的边界: {np.round(bin_edges[feature], 2)}\")\n",
    "            else:\n",
    "                print(f\"警告: 特征 '{feature}' 数据为空或全是 NaN，无法计算分箱边界。\")\n",
    "        except Exception as e:\n",
    "            print(f\"为 '{feature}' 计算分箱边界时出错: {e}. 跳过此特征。\")\n",
    "            if feature in bin_edges: del bin_edges[feature]\n",
    "    else:\n",
    "        print(f\"警告: 特征 '{feature}' 未在临时处理的训练集中生成，无法计算分箱边界。\")\n",
    "\n",
    "# 清理临时变量 (可选)\n",
    "del temp_train_processed_for_bins\n",
    "\n",
    "print(\"--- 所有分箱边界计算完毕 ---\")\n",
    "print(\"--- 完整的 bin_edges 字典:\", {k: np.round(v, 2).tolist() for k, v in bin_edges.items()}) # 打印确认\n",
    "# --- >>> 新增代码结束 <<< ---\n",
    "\n",
    "# (紧接着是 STAGE 6 的代码)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0308b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 6: 将规则应用于数据集... ---\n",
      "--- 正在处理 train_df_cleaned... ---\n",
      "--- (apply_rules) 正在应用分箱... ---\n",
      "已生成分箱特征: log_建筑面积_binned\n",
      "--- (apply_rules) 警告: 特征 '房龄' 不在待处理数据框中，无法应用分箱。---\n",
      "--- (apply_rules) 警告: 特征 '持有天数' 不在待处理数据框中，无法应用分箱。---\n",
      "--- 正在映射 '配备电梯' 为 0/1 ... ---\n",
      "--- 正在处理 test_df... ---\n",
      "--- (apply_rules) 正在应用分箱... ---\n",
      "已生成分箱特征: log_建筑面积_binned\n",
      "--- (apply_rules) 警告: 特征 '房龄' 不在待处理数据框中，无法应用分箱。---\n",
      "--- (apply_rules) 警告: 特征 '持有天数' 不在待处理数据框中，无法应用分箱。---\n",
      "--- 正在映射 '配备电梯' 为 0/1 ... ---\n",
      "\n",
      "--- STAGE 7: 执行 One-Hot 编码并对齐列... ---\n",
      "--- 将添加以下分箱列进行独热编码: ['log_建筑面积_binned', '房龄_binned', '持有天数_binned'] ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- STAGE 6: 应用规则到【清理后】的训练集和【原始】测试集 (来自 Cell 9) ---\n",
    "print(\"\\n--- STAGE 6: 将规则应用于数据集... ---\")\n",
    "\n",
    "# 【修改】对 train_df_cleaned 应用规则\n",
    "# (这将填充剩余的高缺失率列，如 容积率_num)\n",
    "print(\"--- 正在处理 train_df_cleaned... ---\")\n",
    "train_df_processed = apply_rules(train_df_cleaned, is_test_set=False)\n",
    "\n",
    "# 【修改】对 test_df 应用规则\n",
    "# (这将填充所有缺失列，包括低缺失和高缺失)\n",
    "print(\"--- 正在处理 test_df... ---\")\n",
    "test_df_processed = apply_rules(test_df, is_test_set=True)\n",
    "\n",
    "\n",
    "# --- STAGE 7: One-Hot 编码 (确保列对齐) (来自 Cell 9) ---\n",
    "print(\"\\n--- STAGE 7: 执行 One-Hot 编码并对齐列... ---\")\n",
    "\n",
    "categorical_cols = [\n",
    "    '建筑结构', '交易权属', '产权所属', '供水', '供电', 'location_cluster','区域'\n",
    "]\n",
    "\n",
    "# --- >>> 新增代码开始: 添加分箱后的列 <<< ---\n",
    "# 根据你在 features_to_bin 中选择的特征动态添加\n",
    "binned_cols_to_encode = [f'{feature}_binned' for feature in bin_edges.keys()]\n",
    "print(f\"--- 将添加以下分箱列进行独热编码: {binned_cols_to_encode} ---\")\n",
    "# --- >>> 新增代码结束 <<< ---\n",
    "\n",
    "categorical_cols = [col for col in categorical_cols if col in train_df_processed.columns] \n",
    "\n",
    "train_df_encoded = pd.get_dummies(train_df_processed, columns=categorical_cols, dummy_na=False)\n",
    "test_df_encoded = pd.get_dummies(test_df_processed, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "# (关键) 列对齐\n",
    "train_cols = train_df_encoded.drop('log_price', axis=1, errors='ignore').columns \n",
    "test_cols = test_df_encoded.columns\n",
    "\n",
    "cols_to_drop_from_test = list(set(test_cols) - set(train_cols) - set(['ID'])) \n",
    "test_df_aligned = test_df_encoded.drop(columns=cols_to_drop_from_test, errors='ignore')\n",
    "\n",
    "cols_to_add_to_test = list(set(train_cols) - set(test_cols))\n",
    "for col in cols_to_add_to_test:\n",
    "    test_df_aligned[col] = 0\n",
    "\n",
    "# 确保测试集的列顺序与训练集一致 (去掉 log_price)\n",
    "# (如果ID在train_cols中，需要去掉)\n",
    "train_cols_final_order = [col for col in train_cols if col != 'ID']\n",
    "test_df_aligned = test_df_aligned[train_cols_final_order] # 应用训练集顺序\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25af43f-b1ce-4a77-acad-17a15b9e19ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 8: 最终清理... ---\n",
      "\n",
      "--- 正在检查并移除零方差特征... ---\n",
      "找到以下零方差特征: ['有_客户反馈', '房屋用途_车库']\n",
      "已从 train_df_final 中移除 2 个零方差列。\n",
      "已从 test_df_final 中移除 2 个零方差列。\n",
      "移除零方差特征后，最终训练集维度: (101252, 225)\n",
      "移除零方差特征后，最终测试集维度: (34017, 225)\n",
      "移除后列匹配成功！特征数量: 224\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 8: 最终清理 (来自 Cell 10) ---\n",
    "print(\"\\n--- STAGE 8: 最终清理... ---\")\n",
    "\n",
    "original_cols_to_drop = [\n",
    "    '房屋户型', '装修情况', '梯户比例', '配备电梯', '所在楼层', '房屋朝向',\n",
    "    '建筑年代', '房屋总数', '楼栋总数', '绿 化 率', '容 积 率', '燃气费', \n",
    "    '停车位', '停车费用', '交易时间', '上次交易', '房屋用途',\n",
    "    '房屋优势', '周边配套', '交通出行', '核心卖点', '客户反馈', \n",
    "    '板块', '物业类别', '建筑结构', '交易权属', '房屋用途', '产权所属', \n",
    "    '建筑结构_comm', '供水', '供电', '区域', 'location_cluster', \n",
    "    '建筑年代_parsed', 'Price', '抵押信息', '区县', '板块_comm', \n",
    "    'coord_x', 'coord_y','城市'\n",
    "]\n",
    "original_cols_to_drop = list(set(original_cols_to_drop))\n",
    "\n",
    "train_df_final = train_df_encoded.drop(columns=original_cols_to_drop, errors='ignore')\n",
    "test_df_final = test_df_aligned.drop(columns=original_cols_to_drop, errors='ignore')\n",
    "\n",
    "# 删除所有剩余的 object 类型\n",
    "train_obj_cols = train_df_final.select_dtypes(include=['object']).columns\n",
    "test_obj_cols = test_df_final.select_dtypes(include=['object']).columns\n",
    "train_df_final = train_df_final.drop(columns=train_obj_cols, errors='ignore')\n",
    "test_df_final = test_df_final.drop(columns=test_obj_cols, errors='ignore')\n",
    "\n",
    "# 从最终训练集中移除 ID 列\n",
    "if 'ID' in train_df_final.columns:\n",
    "    train_df_final = train_df_final.drop('ID', axis=1, errors='ignore')\n",
    "\n",
    "# (确保 ID 仍然在最终测试集中)\n",
    "if 'ID' not in test_df_final.columns and 'ID' in test_df.columns:\n",
    "     test_df_final['ID'] = test_df['ID']\n",
    "\n",
    "\n",
    "# --- >>> 新增代码开始 <<< ---\n",
    "# --- 移除零方差特征 ---\n",
    "print(\"\\n--- 正在检查并移除零方差特征... ---\")\n",
    "\n",
    "# 1. 准备训练特征集 (不含目标变量 'log_price')\n",
    "X_train_features = train_df_final.drop('log_price', axis=1, errors='ignore')\n",
    "\n",
    "# 2. 计算训练特征集的方差\n",
    "try:\n",
    "    variances = X_train_features.var()\n",
    "\n",
    "    # 3. 找出方差为 0 (或非常接近 0) 的列\n",
    "    #    使用一个小的阈值来处理可能的浮点数精度问题\n",
    "    zero_variance_cols = variances[variances <= 1e-8].index.tolist()\n",
    "\n",
    "    if zero_variance_cols:\n",
    "        print(f\"找到以下零方差特征: {zero_variance_cols}\")\n",
    "\n",
    "        # 4. 从训练集和测试集中移除这些列\n",
    "        #    确保 'log_price' 不会被误删 (虽然理论上它方差不为0)\n",
    "        cols_to_drop_final_train = [col for col in zero_variance_cols if col in train_df_final.columns and col != 'log_price']\n",
    "        #    确保 'ID' 不会被误删\n",
    "        cols_to_drop_final_test = [col for col in zero_variance_cols if col in test_df_final.columns and col != 'ID']\n",
    "\n",
    "        if cols_to_drop_final_train:\n",
    "            train_df_final = train_df_final.drop(columns=cols_to_drop_final_train)\n",
    "            print(f\"已从 train_df_final 中移除 {len(cols_to_drop_final_train)} 个零方差列。\")\n",
    "\n",
    "        if cols_to_drop_final_test:\n",
    "            test_df_final = test_df_final.drop(columns=cols_to_drop_final_test)\n",
    "            print(f\"已从 test_df_final 中移除 {len(cols_to_drop_final_test)} 个零方差列。\")\n",
    "\n",
    "        # 再次打印最终维度\n",
    "        print(f\"移除零方差特征后，最终训练集维度: {train_df_final.shape}\")\n",
    "        print(f\"移除零方差特征后，最终测试集维度: {test_df_final.shape}\")\n",
    "\n",
    "        # 再次验证列匹配\n",
    "        train_features_after_drop = set(train_df_final.drop('log_price', axis=1, errors='ignore').columns)\n",
    "        test_features_after_drop = set(test_df_final.drop('ID', axis=1, errors='ignore').columns)\n",
    "        if train_features_after_drop == test_features_after_drop:\n",
    "            print(f\"移除后列匹配成功！特征数量: {len(train_features_after_drop)}\")\n",
    "        else:\n",
    "            print(\"警告：移除零方差列后，列仍然不匹配！请检查。\")\n",
    "\n",
    "    else:\n",
    "        print(\"未在训练集中找到零方差特征。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"计算或移除零方差特征时出错: {e}\")\n",
    "    print(\"跳过零方差特征移除步骤。\")\n",
    "\n",
    "# --- >>> 新增代码结束 <<< ---\n",
    "\n",
    "# (后续的代码，如 STAGE 9 模型训练，将使用更新后的 train_df_final 和 test_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32899653-44e5-41f6-816e-a0bb72b0f1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 处理完成 ---\n",
      "最终训练集维度: (101252, 225)\n",
      "最终测试集维度: (34017, 225)\n",
      "列匹配成功！特征数量: 224\n",
      "\n",
      "--- 最终训练集 (train_df_final) 的列 ---\n",
      "  1. lon\n",
      "  2. lat\n",
      "  3. 年份\n",
      "  4. log_price\n",
      "  5. 供水_商水\n",
      "  6. 供水_民水\n",
      "  7. 供电_商电\n",
      "  8. 供电_民电\n",
      "  9. log_建筑面积\n",
      " 10. 梯\n",
      " 11. 户\n",
      " 12. 总楼层\n",
      " 13. 楼层位置_mapped\n",
      " 14. 朝南\n",
      " 15. 朝北\n",
      " 16. 朝东\n",
      " 17. 朝西\n",
      " 18. 房屋总数_num\n",
      " 19. 楼栋总数_num\n",
      " 20. 绿化率_num\n",
      " 21. 容积率_num\n",
      " 22. 燃气费_num\n",
      " 23. 停车位_num\n",
      " 24. 停车费用_num\n",
      " 25. 有_房屋优势\n",
      " 26. 有_周边配套\n",
      " 27. 有_交通出行\n",
      " 28. 有_核心卖点\n",
      " 29. 核心卖点_长度\n",
      " 30. 室\n",
      " 31. 厅\n",
      " 32. 卫\n",
      " 33. 装修情况_mapped\n",
      " 34. log_建筑面积_binned\n",
      " 35. 配备电梯_mapped\n",
      " 36. 梯户比\n",
      " 37. 楼层相对位置\n",
      " 38. 房龄\n",
      " 39. 建筑结构_comm_塔楼\n",
      " 40. 建筑结构_comm_板楼\n",
      " 41. 建筑结构_comm_塔板结合\n",
      " 42. 建筑结构_comm_平房\n",
      " 43. 物业类型_普通住宅\n",
      " 44. 物业类型_别墅\n",
      " 45. 物业类型_写字楼\n",
      " 46. 物业类型_商业\n",
      " 47. 物业类型_公寓\n",
      " 48. 物业类型_底商\n",
      " 49. 物业类型_车库\n",
      " 50. 物业类型_花园洋房\n",
      " 51. 物业类型_平房\n",
      " 52. 物业类型_新式里弄\n",
      " 53. 物业类型_老公寓\n",
      " 54. 房屋用途_普通住宅\n",
      " 55. 房屋用途_别墅\n",
      " 56. 房屋用途_商业办公类\n",
      " 57. 房屋用途_公寓\n",
      " 58. 房屋用途_酒店式公寓\n",
      " 59. 房屋用途_四合院\n",
      " 60. 房屋用途_商务型公寓\n",
      " 61. 房屋用途_住宅式公寓\n",
      " 62. 房屋用途_商住两用\n",
      " 63. 房屋用途_新式里弄\n",
      " 64. 房屋用途_老公寓\n",
      " 65. 房屋用途_花园洋房\n",
      " 66. 房屋用途_底商\n",
      " 67. 房屋用途_商业\n",
      " 68. 房屋用途_商务公寓\n",
      " 69. 房屋用途_写字楼\n",
      " 70. 房屋用途_住宅\n",
      " 71. 交易年份\n",
      " 72. 交易月份\n",
      " 73. 持有天数\n",
      " 74. 是否首次交易\n",
      " 75. 建筑结构_未知结构\n",
      " 76. 建筑结构_框架结构\n",
      " 77. 建筑结构_混合结构\n",
      " 78. 建筑结构_砖木结构\n",
      " 79. 建筑结构_砖混结构\n",
      " 80. 建筑结构_钢混结构\n",
      " 81. 建筑结构_钢结构\n",
      " 82. 交易权属_一类经济适用房\n",
      " 83. 交易权属_二类经济适用房\n",
      " 84. 交易权属_动迁安置房\n",
      " 85. 交易权属_售后公房\n",
      " 86. 交易权属_商品房\n",
      " 87. 交易权属_回迁房\n",
      " 88. 交易权属_央产房\n",
      " 89. 交易权属_定向安置房\n",
      " 90. 交易权属_已购公房\n",
      " 91. 交易权属_房改房\n",
      " 92. 交易权属_拆迁还建房\n",
      " 93. 交易权属_私产\n",
      " 94. 交易权属_经济适用房\n",
      " 95. 交易权属_限价商品房\n",
      " 96. 交易权属_集资房\n",
      " 97. 产权所属_共有\n",
      " 98. 产权所属_非共有\n",
      " 99. location_cluster_0\n",
      "100. location_cluster_1\n",
      "101. location_cluster_2\n",
      "102. location_cluster_3\n",
      "103. location_cluster_4\n",
      "104. location_cluster_5\n",
      "105. location_cluster_6\n",
      "106. location_cluster_7\n",
      "107. location_cluster_8\n",
      "108. location_cluster_9\n",
      "109. 区域_3\n",
      "110. 区域_4\n",
      "111. 区域_5\n",
      "112. 区域_7\n",
      "113. 区域_8\n",
      "114. 区域_9\n",
      "115. 区域_10\n",
      "116. 区域_11\n",
      "117. 区域_12\n",
      "118. 区域_13\n",
      "119. 区域_14\n",
      "120. 区域_15\n",
      "121. 区域_16\n",
      "122. 区域_17\n",
      "123. 区域_18\n",
      "124. 区域_19\n",
      "125. 区域_20\n",
      "126. 区域_21\n",
      "127. 区域_22\n",
      "128. 区域_23\n",
      "129. 区域_24\n",
      "130. 区域_26\n",
      "131. 区域_27\n",
      "132. 区域_28\n",
      "133. 区域_29\n",
      "134. 区域_30\n",
      "135. 区域_31\n",
      "136. 区域_32\n",
      "137. 区域_33\n",
      "138. 区域_34\n",
      "139. 区域_35\n",
      "140. 区域_36\n",
      "141. 区域_37\n",
      "142. 区域_38\n",
      "143. 区域_39\n",
      "144. 区域_41\n",
      "145. 区域_42\n",
      "146. 区域_43\n",
      "147. 区域_44\n",
      "148. 区域_45\n",
      "149. 区域_46\n",
      "150. 区域_47\n",
      "151. 区域_49\n",
      "152. 区域_50\n",
      "153. 区域_51\n",
      "154. 区域_52\n",
      "155. 区域_53\n",
      "156. 区域_54\n",
      "157. 区域_55\n",
      "158. 区域_56\n",
      "159. 区域_57\n",
      "160. 区域_58\n",
      "161. 区域_59\n",
      "162. 区域_60\n",
      "163. 区域_62\n",
      "164. 区域_63\n",
      "165. 区域_64\n",
      "166. 区域_65\n",
      "167. 区域_66\n",
      "168. 区域_67\n",
      "169. 区域_68\n",
      "170. 区域_69\n",
      "171. 区域_70\n",
      "172. 区域_71\n",
      "173. 区域_72\n",
      "174. 区域_73\n",
      "175. 区域_74\n",
      "176. 区域_76\n",
      "177. 区域_77\n",
      "178. 区域_78\n",
      "179. 区域_80\n",
      "180. 区域_81\n",
      "181. 区域_82\n",
      "182. 区域_84\n",
      "183. 区域_85\n",
      "184. 区域_86\n",
      "185. 区域_87\n",
      "186. 区域_88\n",
      "187. 区域_89\n",
      "188. 区域_90\n",
      "189. 区域_91\n",
      "190. 区域_92\n",
      "191. 区域_93\n",
      "192. 区域_94\n",
      "193. 区域_95\n",
      "194. 区域_96\n",
      "195. 区域_99\n",
      "196. 区域_101\n",
      "197. 区域_102\n",
      "198. 区域_103\n",
      "199. 区域_104\n",
      "200. 区域_105\n",
      "201. 区域_106\n",
      "202. 区域_107\n",
      "203. 区域_108\n",
      "204. 区域_109\n",
      "205. 区域_110\n",
      "206. 区域_111\n",
      "207. 区域_112\n",
      "208. 区域_113\n",
      "209. 区域_114\n",
      "210. 区域_115\n",
      "211. 区域_117\n",
      "212. 区域_118\n",
      "213. 区域_119\n",
      "214. 区域_120\n",
      "215. 区域_121\n",
      "216. 区域_122\n",
      "217. 区域_123\n",
      "218. 区域_124\n",
      "219. 区域_125\n",
      "220. 区域_126\n",
      "221. 区域_127\n",
      "222. 区域_128\n",
      "223. 区域_129\n",
      "224. 区域_130\n",
      "225. 区域_131\n",
      "\n",
      "--- 最终测试集 (test_df_final) 的列 ---\n",
      "  1. lon\n",
      "  2. lat\n",
      "  3. 年份\n",
      "  4. 供水_商水\n",
      "  5. 供水_民水\n",
      "  6. 供电_商电\n",
      "  7. 供电_民电\n",
      "  8. log_建筑面积\n",
      "  9. 梯\n",
      " 10. 户\n",
      " 11. 总楼层\n",
      " 12. 楼层位置_mapped\n",
      " 13. 朝南\n",
      " 14. 朝北\n",
      " 15. 朝东\n",
      " 16. 朝西\n",
      " 17. 房屋总数_num\n",
      " 18. 楼栋总数_num\n",
      " 19. 绿化率_num\n",
      " 20. 容积率_num\n",
      " 21. 燃气费_num\n",
      " 22. 停车位_num\n",
      " 23. 停车费用_num\n",
      " 24. 有_房屋优势\n",
      " 25. 有_周边配套\n",
      " 26. 有_交通出行\n",
      " 27. 有_核心卖点\n",
      " 28. 核心卖点_长度\n",
      " 29. 室\n",
      " 30. 厅\n",
      " 31. 卫\n",
      " 32. 装修情况_mapped\n",
      " 33. log_建筑面积_binned\n",
      " 34. 配备电梯_mapped\n",
      " 35. 梯户比\n",
      " 36. 楼层相对位置\n",
      " 37. 房龄\n",
      " 38. 建筑结构_comm_塔楼\n",
      " 39. 建筑结构_comm_板楼\n",
      " 40. 建筑结构_comm_塔板结合\n",
      " 41. 建筑结构_comm_平房\n",
      " 42. 物业类型_普通住宅\n",
      " 43. 物业类型_别墅\n",
      " 44. 物业类型_写字楼\n",
      " 45. 物业类型_商业\n",
      " 46. 物业类型_公寓\n",
      " 47. 物业类型_底商\n",
      " 48. 物业类型_车库\n",
      " 49. 物业类型_花园洋房\n",
      " 50. 物业类型_平房\n",
      " 51. 物业类型_新式里弄\n",
      " 52. 物业类型_老公寓\n",
      " 53. 房屋用途_普通住宅\n",
      " 54. 房屋用途_别墅\n",
      " 55. 房屋用途_商业办公类\n",
      " 56. 房屋用途_公寓\n",
      " 57. 房屋用途_酒店式公寓\n",
      " 58. 房屋用途_四合院\n",
      " 59. 房屋用途_商务型公寓\n",
      " 60. 房屋用途_住宅式公寓\n",
      " 61. 房屋用途_商住两用\n",
      " 62. 房屋用途_新式里弄\n",
      " 63. 房屋用途_老公寓\n",
      " 64. 房屋用途_花园洋房\n",
      " 65. 房屋用途_底商\n",
      " 66. 房屋用途_商业\n",
      " 67. 房屋用途_商务公寓\n",
      " 68. 房屋用途_写字楼\n",
      " 69. 房屋用途_住宅\n",
      " 70. 交易年份\n",
      " 71. 交易月份\n",
      " 72. 持有天数\n",
      " 73. 是否首次交易\n",
      " 74. 建筑结构_未知结构\n",
      " 75. 建筑结构_框架结构\n",
      " 76. 建筑结构_混合结构\n",
      " 77. 建筑结构_砖木结构\n",
      " 78. 建筑结构_砖混结构\n",
      " 79. 建筑结构_钢混结构\n",
      " 80. 建筑结构_钢结构\n",
      " 81. 交易权属_一类经济适用房\n",
      " 82. 交易权属_二类经济适用房\n",
      " 83. 交易权属_动迁安置房\n",
      " 84. 交易权属_售后公房\n",
      " 85. 交易权属_商品房\n",
      " 86. 交易权属_回迁房\n",
      " 87. 交易权属_央产房\n",
      " 88. 交易权属_定向安置房\n",
      " 89. 交易权属_已购公房\n",
      " 90. 交易权属_房改房\n",
      " 91. 交易权属_拆迁还建房\n",
      " 92. 交易权属_私产\n",
      " 93. 交易权属_经济适用房\n",
      " 94. 交易权属_限价商品房\n",
      " 95. 交易权属_集资房\n",
      " 96. 产权所属_共有\n",
      " 97. 产权所属_非共有\n",
      " 98. location_cluster_0\n",
      " 99. location_cluster_1\n",
      "100. location_cluster_2\n",
      "101. location_cluster_3\n",
      "102. location_cluster_4\n",
      "103. location_cluster_5\n",
      "104. location_cluster_6\n",
      "105. location_cluster_7\n",
      "106. location_cluster_8\n",
      "107. location_cluster_9\n",
      "108. 区域_3\n",
      "109. 区域_4\n",
      "110. 区域_5\n",
      "111. 区域_7\n",
      "112. 区域_8\n",
      "113. 区域_9\n",
      "114. 区域_10\n",
      "115. 区域_11\n",
      "116. 区域_12\n",
      "117. 区域_13\n",
      "118. 区域_14\n",
      "119. 区域_15\n",
      "120. 区域_16\n",
      "121. 区域_17\n",
      "122. 区域_18\n",
      "123. 区域_19\n",
      "124. 区域_20\n",
      "125. 区域_21\n",
      "126. 区域_22\n",
      "127. 区域_23\n",
      "128. 区域_24\n",
      "129. 区域_26\n",
      "130. 区域_27\n",
      "131. 区域_28\n",
      "132. 区域_29\n",
      "133. 区域_30\n",
      "134. 区域_31\n",
      "135. 区域_32\n",
      "136. 区域_33\n",
      "137. 区域_34\n",
      "138. 区域_35\n",
      "139. 区域_36\n",
      "140. 区域_37\n",
      "141. 区域_38\n",
      "142. 区域_39\n",
      "143. 区域_41\n",
      "144. 区域_42\n",
      "145. 区域_43\n",
      "146. 区域_44\n",
      "147. 区域_45\n",
      "148. 区域_46\n",
      "149. 区域_47\n",
      "150. 区域_49\n",
      "151. 区域_50\n",
      "152. 区域_51\n",
      "153. 区域_52\n",
      "154. 区域_53\n",
      "155. 区域_54\n",
      "156. 区域_55\n",
      "157. 区域_56\n",
      "158. 区域_57\n",
      "159. 区域_58\n",
      "160. 区域_59\n",
      "161. 区域_60\n",
      "162. 区域_62\n",
      "163. 区域_63\n",
      "164. 区域_64\n",
      "165. 区域_65\n",
      "166. 区域_66\n",
      "167. 区域_67\n",
      "168. 区域_68\n",
      "169. 区域_69\n",
      "170. 区域_70\n",
      "171. 区域_71\n",
      "172. 区域_72\n",
      "173. 区域_73\n",
      "174. 区域_74\n",
      "175. 区域_76\n",
      "176. 区域_77\n",
      "177. 区域_78\n",
      "178. 区域_80\n",
      "179. 区域_81\n",
      "180. 区域_82\n",
      "181. 区域_84\n",
      "182. 区域_85\n",
      "183. 区域_86\n",
      "184. 区域_87\n",
      "185. 区域_88\n",
      "186. 区域_89\n",
      "187. 区域_90\n",
      "188. 区域_91\n",
      "189. 区域_92\n",
      "190. 区域_93\n",
      "191. 区域_94\n",
      "192. 区域_95\n",
      "193. 区域_96\n",
      "194. 区域_99\n",
      "195. 区域_101\n",
      "196. 区域_102\n",
      "197. 区域_103\n",
      "198. 区域_104\n",
      "199. 区域_105\n",
      "200. 区域_106\n",
      "201. 区域_107\n",
      "202. 区域_108\n",
      "203. 区域_109\n",
      "204. 区域_110\n",
      "205. 区域_111\n",
      "206. 区域_112\n",
      "207. 区域_113\n",
      "208. 区域_114\n",
      "209. 区域_115\n",
      "210. 区域_117\n",
      "211. 区域_118\n",
      "212. 区域_119\n",
      "213. 区域_120\n",
      "214. 区域_121\n",
      "215. 区域_122\n",
      "216. 区域_123\n",
      "217. 区域_124\n",
      "218. 区域_125\n",
      "219. 区域_126\n",
      "220. 区域_127\n",
      "221. 区域_128\n",
      "222. 区域_129\n",
      "223. 区域_130\n",
      "224. 区域_131\n",
      "225. ID\n"
     ]
    }
   ],
   "source": [
    "# --- 最终结果 ---\n",
    "print(\"\\n--- 处理完成 ---\")\n",
    "print(f\"最终训练集维度: {train_df_final.shape}\")\n",
    "print(f\"最终测试集维度: {test_df_final.shape}\")\n",
    "\n",
    "# 验证列是否匹配 (除了 log_price 和 ID)\n",
    "train_features = set(train_df_final.drop('log_price', axis=1, errors='ignore').columns)\n",
    "test_features = set(test_df_final.drop('ID', axis=1, errors='ignore').columns)\n",
    "\n",
    "if train_features == test_features:\n",
    "    print(f\"列匹配成功！特征数量: {len(train_features)}\")\n",
    "else:\n",
    "    print(\"警告：列不匹配！\")\n",
    "    print(\"仅在训练集中的列:\", list(train_features - test_features))\n",
    "    print(\"仅在测试集中的列:\", list(test_features - train_features))\n",
    "\n",
    "# 打印最终训练集的列 (来自 Cell 11)\n",
    "print(\"\\n--- 最终训练集 (train_df_final) 的列 ---\")\n",
    "all_columns = train_df_final.columns.tolist()\n",
    "for i, col in enumerate(all_columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "# 打印最终测试集的列\n",
    "print(\"\\n--- 最终测试集 (test_df_final) 的列 ---\")\n",
    "all_columns_test = test_df_final.columns.tolist()\n",
    "for i, col in enumerate(all_columns_test, 1):\n",
    "    print(f\"{i:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895cdd24-ed42-4445-9745-af1355cb7927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在计算核心数值特征的相关系数矩阵... ---\n",
      "高度相关: 交易年份 和 年份, 相关系数: 0.887\n",
      "高度相关: 供电_商电 和 供水_商水, 相关系数: 0.905\n",
      "高度相关: 供电_民电 和 供水_民水, 相关系数: 0.990\n"
     ]
    }
   ],
   "source": [
    "# --- >>> 新增代码开始: 检查核心数值特征的相关系数 <<< ---\n",
    "print(\"\\n--- 正在计算核心数值特征的相关系数矩阵... ---\")\n",
    "\n",
    "# 选择你认为重要的、非独热编码产生的数值特征\n",
    "# (这是一个示例列表，你需要根据你的最终特征集调整)\n",
    "core_numerical_features = [\n",
    "    # --- 地理位置与时间 ---\n",
    "    'lon',                 # 经度 (连续)\n",
    "    'lat',                 # 纬度 (连续)\n",
    "    '年份',                # 原始数据中的年份 (离散/有序) - 注意：可能与 '交易年份' 相关\n",
    "    '交易年份',            # 从交易时间解析 (离散/有序)\n",
    "    '交易月份',            # 从交易时间解析 (离散/有序)\n",
    "\n",
    "    # --- 房屋基本属性 ---\n",
    "    'log_建筑面积',        # 对数转换后的建筑面积 (连续)\n",
    "    '室',                  # 房间数 (离散计数)\n",
    "    '厅',                  # 客厅数 (离散计数)\n",
    "    '卫',                  # 卫生间数 (离散计数)\n",
    "    '装修情况_mapped',     # 装修情况的有序映射 (0, 1, 2) - 如果你添加了\n",
    "    '配备电梯_mapped',     # 是否有电梯的二元映射 (0, 1) - 如果你添加了\n",
    "\n",
    "    # --- 楼层与结构相关 ---\n",
    "    '梯',                  # 电梯数量 (离散计数)\n",
    "    '户',                  # 每层户数 (离散计数)\n",
    "    '梯户比',              # 计算得到的比率 (连续)\n",
    "    '总楼层',              # 总楼层数 (离散计数)\n",
    "    '楼层位置_mapped',     # 楼层位置的有序映射 (-1 到 5)\n",
    "    '楼层相对位置',        # 计算得到的比率 (连续)\n",
    "\n",
    "    # --- 朝向 (二元) ---\n",
    "    '朝南',\n",
    "    '朝北',\n",
    "    '朝东',\n",
    "    '朝西',\n",
    "\n",
    "    # --- 社区/建筑属性 ---\n",
    "    '房屋总数_num',        # 小区房屋总数 (离散计数)\n",
    "    '楼栋总数_num',        # 小区楼栋总数 (离散计数)\n",
    "    '绿化率_num',          # 绿化率 (连续, 0-1)\n",
    "    '容积率_num',          # 容积率 (连续)\n",
    "    '房龄',                # 计算得到的房龄 (连续)\n",
    "\n",
    "    # --- 费用与交易相关 ---\n",
    "    '燃气费_num',          # 燃气费单价 (连续)\n",
    "    '停车位_num',          # 停车位数量 (离散计数)\n",
    "    '停车费用_num',        # 停车费用 (连续)\n",
    "    '持有天数',            # 房屋持有时间 (连续)\n",
    "    '是否首次交易',        # 是否为首次交易 (二元 0/1)\n",
    "\n",
    "    # --- 文本长度 ---\n",
    "    '核心卖点_长度',       # 核心卖点文本长度 (离散计数)\n",
    "\n",
    "    # --- 水电 (二元) ---\n",
    "    '供水_商水',\n",
    "    '供水_民水',\n",
    "    '供电_商电',\n",
    "    '供电_民电',\n",
    "\n",
    "    # --- (可选) 添加少量关键的多标签二值化特征 ---\n",
    "    # 如果你认为某几个物业类型或房屋用途特别重要，可以加进来看看\n",
    "    # 例如:\n",
    "    # '物业类型_别墅',\n",
    "    # '物业类型_普通住宅',\n",
    "    # '房屋用途_普通住宅', # 注意与物业类型的重叠\n",
    "]\n",
    "\n",
    "# 在你的代码中，使用这个列表前，务必用下面的代码确保所有列都实际存在于你的 DataFrame 中：\n",
    "# core_numerical_features_exist = [col for col in core_numerical_features if col in train_df_final.columns]\n",
    "# 确保这些列在最终的训练集中存在\n",
    "core_numerical_features_exist = [col for col in core_numerical_features if col in train_df_final.columns]\n",
    "\n",
    "if core_numerical_features_exist:\n",
    "    # 仅在训练集上计算\n",
    "    correlation_matrix = train_df_final[core_numerical_features_exist].corr()\n",
    "\n",
    "    # 打印出相关系数绝对值大于某个阈值 (例如 0.8) 的特征对\n",
    "    high_corr_threshold = 0.8\n",
    "    highly_correlated_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > high_corr_threshold:\n",
    "                pair = (correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j])\n",
    "                highly_correlated_pairs.append(pair)\n",
    "                print(f\"高度相关: {pair[0]} 和 {pair[1]}, 相关系数: {pair[2]:.3f}\")\n",
    "\n",
    "    if not highly_correlated_pairs:\n",
    "        print(f\"在核心数值特征中未发现绝对值大于 {high_corr_threshold} 的相关系数。\")\n",
    "\n",
    "    # (可选) 可视化热力图 (需要导入 matplotlib 和 seaborn)\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import seaborn as sns\n",
    "    # plt.figure(figsize=(12, 10)) # 可能需要调整大小\n",
    "    # sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
    "    # plt.title('核心数值特征相关系数热力图')\n",
    "    # plt.show() # 在 Jupyter 环境中显示\n",
    "\n",
    "else:\n",
    "    print(\"未找到用于计算相关系数的核心数值特征列。\")\n",
    "\n",
    "# --- >>> 新增代码结束 <<< ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ceca1f-59f1-4b93-91b4-aa4ae805799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在计算核心数值特征的方差膨胀因子 (VIF)... ---\n",
      "\n",
      "VIF 计算结果 (降序):\n",
      "        feature        VIF\n",
      "36        供电_民电  50.479866\n",
      "34        供水_民水  50.416182\n",
      "4          交易年份   8.350622\n",
      "3            年份   8.273602\n",
      "33        供水_商水   5.930448\n",
      "35        供电_商电   5.845839\n",
      "6      log_建筑面积   4.181203\n",
      "17       楼层相对位置   4.038084\n",
      "15          总楼层   3.974575\n",
      "1           lon   3.831563\n",
      "12            梯   3.794161\n",
      "7             室   3.311338\n",
      "13            户   3.099911\n",
      "2           lat   3.036556\n",
      "27      燃气费_num   2.466156\n",
      "11  配备电梯_mapped   2.328370\n",
      "9             卫   2.187537\n",
      "31       是否首次交易   1.988789\n",
      "30         持有天数   1.898004\n",
      "5          交易月份   1.829683\n",
      "26           房龄   1.820969\n",
      "29     停车费用_num   1.686970\n",
      "22     房屋总数_num   1.684611\n",
      "8             厅   1.682716\n",
      "14          梯户比   1.649626\n",
      "18           朝南   1.527906\n",
      "28      停车位_num   1.507909\n",
      "23     楼栋总数_num   1.498925\n",
      "16  楼层位置_mapped   1.466172\n",
      "20           朝东   1.391745\n",
      "19           朝北   1.391328\n",
      "25      容积率_num   1.258397\n",
      "21           朝西   1.227676\n",
      "24      绿化率_num   1.196067\n",
      "32      核心卖点_长度   1.165901\n",
      "10  装修情况_mapped   1.064002\n",
      "\n",
      "发现 VIF > 10 的特征: ['供电_民电', '供水_民水']\n",
      "建议考虑处理这些特征（例如，移除其中一个或进行组合）。\n"
     ]
    }
   ],
   "source": [
    "# --- >>> 新增代码开始: 计算核心数值特征的 VIF <<< ---\n",
    "print(\"\\n--- 正在计算核心数值特征的方差膨胀因子 (VIF)... ---\")\n",
    "\n",
    "# 需要导入库\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import pandas as pd\n",
    "\n",
    "# 同样使用上面定义的 core_numerical_features_exist 列表\n",
    "if core_numerical_features_exist:\n",
    "    # 1. 选择核心数值特征数据 (仅训练集)\n",
    "    X_vif_check = train_df_final[core_numerical_features_exist].copy()\n",
    "\n",
    "    # 2. 处理可能的无穷大或缺失值 (VIF 计算前需要)\n",
    "    X_vif_check = X_vif_check.replace([np.inf, -np.inf], np.nan) # 将无穷大替换为 NaN\n",
    "    if X_vif_check.isnull().any().any():\n",
    "        print(\"警告: 用于 VIF 计算的数据中存在 NaN，将用中位数填充。\")\n",
    "        X_vif_check = X_vif_check.fillna(X_vif_check.median()) # 用中位数填充 NaN\n",
    "\n",
    "    # 3. 添加常数项 (截距) - VIF 计算需要\n",
    "    X_vif_check_const = add_constant(X_vif_check)\n",
    "\n",
    "    # 4. 计算 VIF\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_vif_check_const.columns\n",
    "    # 逐个计算 VIF\n",
    "    try:\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_vif_check_const.values, i)\n",
    "                           for i in range(X_vif_check_const.shape[1])]\n",
    "    except Exception as e:\n",
    "         print(f\"计算VIF时出错（可能是完全共线性）: {e}\")\n",
    "         # 如果出错，可以尝试移除导致问题的列或简化特征集\n",
    "\n",
    "    # 5. 显示 VIF 结果 (排除常数项 'const')\n",
    "    vif_results = vif_data[vif_data[\"feature\"] != \"const\"].sort_values(by=\"VIF\", ascending=False)\n",
    "    print(\"\\nVIF 计算结果 (降序):\")\n",
    "    print(vif_results)\n",
    "\n",
    "    # 识别高 VIF 值\n",
    "    high_vif_threshold = 10 # 通常选择 5 或 10 作为阈值\n",
    "    high_vif_features = vif_results[vif_results[\"VIF\"] > high_vif_threshold][\"feature\"].tolist()\n",
    "\n",
    "    if high_vif_features:\n",
    "        print(f\"\\n发现 VIF > {high_vif_threshold} 的特征: {high_vif_features}\")\n",
    "        print(\"建议考虑处理这些特征（例如，移除其中一个或进行组合）。\")\n",
    "    else:\n",
    "        print(f\"\\n未发现 VIF > {high_vif_threshold} 的核心数值特征。\")\n",
    "\n",
    "else:\n",
    "    print(\"未找到用于计算 VIF 的核心数值特征列。\")\n",
    "\n",
    "# --- >>> 新增代码结束 <<< ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc74b626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 准备训练和测试数据 ---\n",
      "X_train_full (来自清理后的数据) 维度: (101252, 224)\n",
      "X_test 维度: (34017, 224)\n",
      "X_train_full 维度: (101252, 224)\n",
      "X_test 维度: (34017, 224)\n",
      "\n",
      "--- 2. 正在创建预处理管道 (StandardScaler)... ---\n",
      "\n",
      "--- 3. 正在训练 OLS (Linear Regression) 模型... ---\n",
      "--- 正在运行 6-fold Cross-Validation... ---\n",
      "\n",
      "--- OLS 模型性能 (MAE in Original Price) ---\n",
      "In-sample MAE:     453,136.21\n",
      "Out-of-sample MAE (local validation): 451,941.78\n",
      "6-Fold CV MAE:     454,604.57\n",
      "\n",
      "--- OLS 结果表格 (用于报告) ---\n",
      "| Metrics        |   In sample |   out of sample |   Cross-validation |\n",
      "|:---------------|------------:|----------------:|-------------------:|\n",
      "| OLS (新预处理) |  453,136.21 |      451,941.78 |         454,604.57 |\n",
      "\n",
      "--- 6. 正在生成 prediction_price_ols.csv 文件... ---\n",
      "--- 正在完整训练数据上重新拟合模型... ---\n",
      "预测结果已保存到 'prediction_price_ols.csv'\n",
      "\n",
      "--- OLS 模型处理完毕 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # 仅需要 StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 假设 train_df_final 和 test_df_final 是您上一步预处理代码的输出\n",
    "# train_df_final: 包含 log_price, (N_train_cleaned, N_features + 1)\n",
    "# test_df_final: 包含 ID, (N_test, N_features + 1)\n",
    "\n",
    "print(\"--- 1. 准备训练和测试数据 ---\")\n",
    "\n",
    "# a. 分离训练集的 X 和 y\n",
    "if 'log_price' in train_df_final.columns:\n",
    "    X_train_full = train_df_final.drop('log_price', axis=1)\n",
    "    y_train_full = train_df_final['log_price']\n",
    "    print(f\"X_train_full (来自清理后的数据) 维度: {X_train_full.shape}\")\n",
    "else:\n",
    "    raise ValueError(\"'log_price' 列未在最终训练集中找到！\")\n",
    "\n",
    "# b. 分离测试集的 X 和 ID\n",
    "if 'ID' in test_df_final.columns:\n",
    "    X_test = test_df_final.drop('ID', axis=1)\n",
    "    test_ids = test_df_final['ID'] \n",
    "    print(f\"X_test 维度: {X_test.shape}\")\n",
    "else:\n",
    "    print(\"警告: 最终测试集中未找到 ID 列。将生成虚拟 ID。\")\n",
    "    X_test = test_df_final.copy()\n",
    "    test_ids = pd.Series(range(len(X_test)), name=\"ID\")\n",
    "\n",
    "# c. 验证列匹配 (上一步已做过，这里再次确认)\n",
    "if not all(X_train_full.columns == X_test.columns):\n",
    "     print(\"警告: 训练集和测试集列不匹配，尝试重新对齐...\")\n",
    "     # (省略对齐代码 - 假设上一阶段已成功)\n",
    "     try:\n",
    "        X_test = X_test[X_train_full.columns]\n",
    "        if not all(X_train_full.columns == X_test.columns):\n",
    "             raise ValueError(\"列对齐失败\")\n",
    "        print(\"重新对齐成功。\")\n",
    "     except Exception as e:\n",
    "         raise ValueError(f\"训练集和测试集的特征列不匹配！请检查处理流程。{e}\")\n",
    "\n",
    "\n",
    "print(f\"X_train_full 维度: {X_train_full.shape}\")\n",
    "print(f\"X_test 维度: {X_test.shape}\")\n",
    "\n",
    "# --- 2. 创建预处理管道 (StandardScaler) ---\n",
    "print(\"\\n--- 2. 正在创建预处理管道 (StandardScaler)... ---\")\n",
    "# 因为所有列都已是数值型 (OHE 完成)，我们只需要对所有列进行缩放\n",
    "pipeline_ols = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# --- 3. 训练 OLS 模型 (使用 Pipeline) ---\n",
    "print(\"\\n--- 3. 正在训练 OLS (Linear Regression) 模型... ---\")\n",
    "# 拟合整个管道\n",
    "pipeline_ols.fit(X_train_full, y_train_full)\n",
    "\n",
    "# --- 4. 计算指标 (在原始价格水平上) ---\n",
    "\n",
    "# a. 反向转换真实的 y_train 值\n",
    "y_train_orig = np.expm1(y_train_full)\n",
    "\n",
    "# b. In-sample (样本内) MAE\n",
    "y_pred_train_log = pipeline_ols.predict(X_train_full) \n",
    "y_pred_train_orig = np.expm1(y_pred_train_log)\n",
    "mae_in_sample = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "\n",
    "# c. Out-of-sample (样本外) MAE - 使用临时的 test split\n",
    "X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=111\n",
    ")\n",
    "# (在 80% 数据上拟合管道)\n",
    "pipeline_ols.fit(X_train_temp, y_train_temp) \n",
    "y_pred_val_log = pipeline_ols.predict(X_val_temp) # 在 20% 数据上预测\n",
    "mae_out_of_sample = mean_absolute_error(np.expm1(y_val_temp), np.expm1(y_pred_val_log))\n",
    "del X_train_temp, X_val_temp, y_train_temp, y_val_temp # 清理\n",
    "\n",
    "# d. 6-fold Cross-validation (6 折交叉验证) MAE\n",
    "print(\"--- 正在运行 6-fold Cross-Validation... ---\")\n",
    "\n",
    "# (自定义 MAE 评估器)\n",
    "def original_price_mae_scorer(y_log, y_pred_log):\n",
    "    y_orig = np.expm1(y_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    y_pred_orig = np.nan_to_num(y_pred_orig, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred_orig = np.clip(y_pred_orig, 0, None)\n",
    "    return mean_absolute_error(y_orig, y_pred_orig)\n",
    "\n",
    "custom_mae_scorer = make_scorer(original_price_mae_scorer, greater_is_better=False)\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111) \n",
    "\n",
    "# (在完整的 X_train_full, y_train_full 上运行 CV)\n",
    "cv_scores = cross_val_score(\n",
    "    pipeline_ols, # 传递整个管道\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    cv=kf, \n",
    "    scoring=custom_mae_scorer,\n",
    "    n_jobs=-1 # 使用所有 CPU\n",
    ")\n",
    "mae_cv = -np.mean(cv_scores)\n",
    "\n",
    "# --- 5. 报告结果 ---\n",
    "print(\"\\n--- OLS 模型性能 (MAE in Original Price) ---\")\n",
    "print(f\"In-sample MAE:     {mae_in_sample:,.2f}\")\n",
    "print(f\"Out-of-sample MAE (local validation): {mae_out_of_sample:,.2f}\")\n",
    "print(f\"6-Fold CV MAE:     {mae_cv:,.2f}\")\n",
    "\n",
    "ols_results = {\n",
    "    'Metrics': 'OLS (新预处理)',\n",
    "    'In sample': mae_in_sample,\n",
    "    'out of sample': mae_out_of_sample, \n",
    "    'Cross-validation': mae_cv\n",
    "}\n",
    "\n",
    "print(\"\\n--- OLS 结果表格 (用于报告) ---\")\n",
    "print(pd.DataFrame([ols_results]).to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "\n",
    "# --- 6. 生成预测文件 (prediction_price_ols.csv) ---\n",
    "print(\"\\n--- 6. 正在生成 prediction_price_ols.csv 文件... ---\")\n",
    "\n",
    "# a. (重要) 在 *完整* 的训练数据上重新拟合管道\n",
    "print(\"--- 正在完整训练数据上重新拟合模型... ---\")\n",
    "pipeline_ols.fit(X_train_full, y_train_full) \n",
    "\n",
    "# b. 使用拟合好的管道预测 X_test\n",
    "y_pred_test_log = pipeline_ols.predict(X_test)\n",
    "\n",
    "# c. 反向转换并处理异常值\n",
    "y_pred_test_orig = np.expm1(y_pred_test_log)\n",
    "fallback_price = np.expm1(y_train_full.median()) \n",
    "y_pred_test_orig = np.nan_to_num(y_pred_test_orig, nan=fallback_price, posinf=np.finfo(np.float64).max, neginf=0.0) \n",
    "y_pred_test_orig = np.clip(y_pred_test_orig, 0, None) \n",
    "\n",
    "# d. 创建提交 DataFrame\n",
    "submission_df_ols = pd.DataFrame({'ID': test_ids, 'Price': y_pred_test_orig})\n",
    "\n",
    "# e. 保存为 CSV 文件\n",
    "submission_filename_ols = 'prediction_price_ols.csv'\n",
    "submission_df_ols.to_csv(submission_filename_ols, index=False)\n",
    "\n",
    "print(f\"预测结果已保存到 '{submission_filename_ols}'\")\n",
    "print(\"\\n--- OLS 模型处理完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe62138c-7ed3-400e-9bbf-d600cc40d05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 准备训练和测试数据 (来自 train_df_final 和 test_df_final) ---\n",
      "X_train_full 维度: (101252, 224)\n",
      "X_test 维度: (34017, 224)\n",
      "\n",
      "--- 4. 正在创建 Ridge 预处理管道... ---\n",
      "\n",
      "--- 5. 正在为 Ridge 运行 6-fold GridSearchCV... ---\n",
      "搜索的参数网格: {'regressor__alpha': [1046]}\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "\n",
      "--- GridSearchCV 训练完成 ---\n",
      "最佳 Alpha (正则化强度): 1046\n",
      "\n",
      "--- Ridge 模型性能 (MAE in Original Price) ---\n",
      "最佳 Alpha:        1046\n",
      "In-sample MAE:     453,952.22\n",
      "Out-of-sample MAE (80/20 split): 452,020.99\n",
      "6-Fold CV MAE:     455,186.89\n",
      "\n",
      "--- Ridge 结果表格 (用于报告) ---\n",
      "| Metrics          |   In sample |   out of sample |   Cross-validation |\n",
      "|:-----------------|------------:|----------------:|-------------------:|\n",
      "| Ridge (新预处理) |  453,952.22 |      452,020.99 |         455,186.89 |\n",
      "\n",
      "--- 9. 正在生成 prediction_price_ridge.csv 文件... ---\n",
      "预测结果已保存到 'prediction_price_ridge.csv'\n",
      "\n",
      "--- Ridge 模型处理完毕 ---\n"
     ]
    }
   ],
   "source": [
    "# --- [ 独立单元 - 运行 Ridge 模型 (带 GridSearchCV) ] ---\n",
    "# (请在 b9daea9b 单元格运行后，在新的单元格中运行此代码)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge # 1. 导入 Ridge\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- 1. 准备训练和测试数据 (来自 train_df_final 和 test_df_final) ---\")\n",
    "\n",
    "# --- 2. (来自 OLS 单元) 准备 X 和 y ---\n",
    "# 假设 train_df_final 和 test_df_final 已在内存中\n",
    "if 'log_price' in train_df_final.columns:\n",
    "    X_train_full = train_df_final.drop('log_price', axis=1)\n",
    "    y_train_full = train_df_final['log_price']\n",
    "else:\n",
    "    raise ValueError(\"'log_price' 列未在最终训练集中找到！\")\n",
    "\n",
    "if 'ID' in test_df_final.columns:\n",
    "    X_test = test_df_final.drop('ID', axis=1)\n",
    "    test_ids = test_df_final['ID']\n",
    "else:\n",
    "    raise ValueError(\"最终测试集中未找到 ID 列！\")\n",
    "\n",
    "# (安全检查) 确保列顺序一致\n",
    "if not all(X_train_full.columns == X_test.columns):\n",
    "    print(\"--- 警告: 训练集和测试集列不匹配，正在重新对齐... ---\")\n",
    "    try:\n",
    "        X_test = X_test[X_train_full.columns]\n",
    "        print(\"--- 重新对齐成功 ---\")\n",
    "    except Exception as e:\n",
    "         raise ValueError(f\"训练集和测试集的特征列不匹配！{e}\")\n",
    "\n",
    "print(f\"X_train_full 维度: {X_train_full.shape}\")\n",
    "print(f\"X_test 维度: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. (来自 OLS 单元) 定义自定义 MAE 评估器 ---\n",
    "def original_price_mae_scorer(y_log, y_pred_log):\n",
    "    \"\"\"\n",
    "    计算原始价格尺度上的 MAE。\n",
    "    \"\"\"\n",
    "    y_orig = np.expm1(y_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    \n",
    "    # 处理预测中的极端值或 NaN (以防万一)\n",
    "    y_pred_orig = np.nan_to_num(y_pred_orig, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred_orig = np.clip(y_pred_orig, 0, None)\n",
    "    \n",
    "    return mean_absolute_error(y_orig, y_pred_orig)\n",
    "\n",
    "# 创建一个 scikit-learn 'scorer' 对象\n",
    "custom_mae_scorer = make_scorer(original_price_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# 定义 6 折交叉验证 (KFold)\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "\n",
    "# --- 4. 【新】创建 Ridge 管道和 GridSearchCV ---\n",
    "print(\"\\n--- 4. 正在创建 Ridge 预处理管道... ---\")\n",
    "\n",
    "# 管道保持不变：先缩放，后回归\n",
    "\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    ('scaler',StandardScaler()), # <<< 2. 将 StandardScaler() 替换为 RobustScaler()\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "# 定义要搜索的 alpha (正则化强度) 网格\n",
    "# (alpha 越大，正则化越强)\n",
    "# 我们使用一个对数间隔的范围来测试\n",
    "param_grid_ridge = {\n",
    "    'regressor__alpha': [1046]\n",
    "}\n",
    "\n",
    "# --- 5. 【新】设置并运行 GridSearchCV ---\n",
    "print(\"\\n--- 5. 正在为 Ridge 运行 6-fold GridSearchCV... ---\")\n",
    "print(f\"搜索的参数网格: {param_grid_ridge}\")\n",
    "\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=pipeline_ridge,    # 我们的管道\n",
    "    param_grid=param_grid_ridge, # 要搜索的 alpha\n",
    "    scoring=custom_mae_scorer,   # 使用自定义的 MAE 评估器\n",
    "    cv=kf,                       # 6-fold\n",
    "    n_jobs=-1,                   # 使用所有 CPU 核心\n",
    "    verbose=2                    # 显示详细进度\n",
    ")\n",
    "\n",
    "# (***) 开始训练 (***)\n",
    "grid_search_ridge.fit(X_train_full, y_train_full)\n",
    "\n",
    "# --- 6. 【新】获取最佳模型和分数 ---\n",
    "print(\"\\n--- GridSearchCV 训练完成 ---\")\n",
    "best_ridge_model = grid_search_ridge.best_estimator_\n",
    "best_mae_cv_ridge = -grid_search_ridge.best_score_  # (Scorer 返回负值, 取反)\n",
    "best_alpha = grid_search_ridge.best_params_['regressor__alpha']\n",
    "\n",
    "print(f\"最佳 Alpha (正则化强度): {best_alpha}\")\n",
    "\n",
    "\n",
    "# --- 7. (来自 OLS 单元) 计算 In-sample 和 Out-of-sample MAE ---\n",
    "# (为了完成作业报告)\n",
    "\n",
    "# a. In-sample (样本内) MAE\n",
    "# (使用在 GCV 中找到的最佳模型)\n",
    "y_pred_train_log = best_ridge_model.predict(X_train_full)\n",
    "y_train_orig = np.expm1(y_train_full)\n",
    "y_pred_train_orig = np.expm1(y_pred_train_log)\n",
    "mae_in_sample = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "\n",
    "\n",
    "# b. Out-of-sample (样本外) MAE - 使用 80/20 划分\n",
    "# (作业要求)\n",
    "X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=111\n",
    ")\n",
    "\n",
    "# (我们必须在 80% 的数据上重新拟合最佳模型)\n",
    "# (GridSearchCV 内部的模型是在 k-fold 上训练的,\n",
    "#  而 best_ridge_model 是在 *全部* X_train_full 上 refit 的)\n",
    "\n",
    "# (创建一个使用最佳 alpha 的新管道)\n",
    "pipeline_ridge_best = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge(alpha=best_alpha)) \n",
    "])\n",
    "# (仅在 80% 的数据上拟合)\n",
    "pipeline_ridge_best.fit(X_train_temp, y_train_temp) \n",
    "\n",
    "# (在 20% 的数据上预测)\n",
    "y_pred_val_log = pipeline_ridge_best.predict(X_val_temp) \n",
    "mae_out_of_sample = mean_absolute_error(np.expm1(y_val_temp), np.expm1(y_pred_val_log))\n",
    "\n",
    "\n",
    "# --- 8. 报告结果 ---\n",
    "print(\"\\n--- Ridge 模型性能 (MAE in Original Price) ---\")\n",
    "print(f\"最佳 Alpha:        {best_alpha}\")\n",
    "print(f\"In-sample MAE:     {mae_in_sample:,.2f}\")\n",
    "print(f\"Out-of-sample MAE (80/20 split): {mae_out_of_sample:,.2f}\")\n",
    "print(f\"6-Fold CV MAE:     {best_mae_cv_ridge:,.2f}\")\n",
    "\n",
    "# (生成作业要求的表格)\n",
    "ridge_results = {\n",
    "    'Metrics': 'Ridge (新预处理)',\n",
    "    'In sample': mae_in_sample,\n",
    "    'out of sample': mae_out_of_sample, \n",
    "    'Cross-validation': best_mae_cv_ridge\n",
    "}\n",
    "print(\"\\n--- Ridge 结果表格 (用于报告) ---\")\n",
    "print(pd.DataFrame([ridge_results]).to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "\n",
    "\n",
    "# --- 9. 生成预测文件 (prediction_price_ridge.csv) ---\n",
    "print(\"\\n--- 9. 正在生成 prediction_price_ridge.csv 文件... ---\")\n",
    "\n",
    "# (GridSearchCV 对象在 .fit() 之后，会自动在 *全部* 训练数据上\n",
    "#  重新训练一个最佳模型，即 grid_search_ridge.best_estimator_)\n",
    "# (我们可以直接使用它来预测 X_test)\n",
    "y_pred_test_log = grid_search_ridge.predict(X_test)\n",
    "\n",
    "# c. 反向转换并处理异常值\n",
    "y_pred_test_orig = np.expm1(y_pred_test_log)\n",
    "fallback_price = np.expm1(y_train_full.median()) \n",
    "y_pred_test_orig = np.nan_to_num(y_pred_test_orig, nan=fallback_price, posinf=np.finfo(np.float64).max, neginf=0.0) \n",
    "y_pred_test_orig = np.clip(y_pred_test_orig, 0, None) \n",
    "\n",
    "# d. 创建提交 DataFrame\n",
    "submission_df_ridge = pd.DataFrame({'ID': test_ids, 'Price': y_pred_test_orig})\n",
    "\n",
    "# e. 保存为 CSV 文件\n",
    "submission_filename_ridge = 'prediction_price_ridge.csv'\n",
    "submission_df_ridge.to_csv(submission_filename_ridge, index=False)\n",
    "\n",
    "print(f\"预测结果已保存到 '{submission_filename_ridge}'\")\n",
    "print(\"\\n--- Ridge 模型处理完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ea2cb8-4d04-4301-ba12-1efe1244c8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 10. 正在进行 Ridge 模型残差分析 (基于训练集)... ---\n",
      "已计算残差，共 101252 个。\n",
      "\n",
      "--- 残差基本统计信息 (对数尺度) ---\n",
      "count    1.012520e+05\n",
      "mean    -2.039375e-15\n",
      "std      2.543364e-01\n",
      "min     -2.087020e+00\n",
      "25%     -1.523277e-01\n",
      "50%      3.170698e-03\n",
      "75%      1.503200e-01\n",
      "max      1.678598e+00\n",
      "\n",
      "--- 残差 vs. 预测值 分析 (按预测值分位数) ---\n",
      "按 5 个预测值分位数区间统计的残差:\n",
      "            mean    std  count\n",
      "quantile                      \n",
      "0        -0.0008 0.2356  20251\n",
      "1        -0.0125 0.2308  20250\n",
      "2        -0.0102 0.2493  20250\n",
      "3         0.0085 0.2677  20250\n",
      "4         0.0151 0.2834  20251\n",
      "\n",
      "--- 极端残差识别 ---\n",
      "最大的 10 个正残差 (预测值远低于实际值):\n",
      "78923   1.6786\n",
      "55518   1.5659\n",
      "56969   1.5558\n",
      "63247   1.3040\n",
      "5974    1.2308\n",
      "63767   1.1997\n",
      "69643   1.1968\n",
      "89960   1.1834\n",
      "81229   1.1821\n",
      "70051   1.1757\n",
      "\n",
      "最大的 10 个负残差 (预测值远高于实际值):\n",
      "6327    -2.0870\n",
      "88214   -1.6943\n",
      "89931   -1.3269\n",
      "93763   -1.2989\n",
      "96268   -1.2597\n",
      "71079   -1.2432\n",
      "69119   -1.2021\n",
      "68895   -1.1934\n",
      "68211   -1.1402\n",
      "27010   -1.1360\n",
      "\n",
      "--- 残差 vs. 关键特征 分析 (按特征分位数) ---\n",
      "\n",
      "--- 分析残差 vs. 'log_建筑面积' ---\n",
      "按 'log_建筑面积' 的 5 个分位数区间统计的残差均值:\n",
      "            mean  count\n",
      "quantile               \n",
      "0         0.0042  20261\n",
      "1        -0.0002  20240\n",
      "2         0.0011  20263\n",
      "3        -0.0190  20238\n",
      "4         0.0138  20250\n",
      "\n",
      "--- 分析残差 vs. '房龄' ---\n",
      "按 '房龄' 的 5 个分位数区间统计的残差均值:\n",
      "            mean  count\n",
      "quantile               \n",
      "0         0.0325  20258\n",
      "1        -0.0155  48804\n",
      "2         0.0006  12289\n",
      "3         0.0045  19901\n",
      "\n",
      "--- 残差分析完毕 ---\n"
     ]
    }
   ],
   "source": [
    "# --- [ 独立单元 - Ridge 模型残差分析 ] ---\n",
    "# (请在你运行 Ridge 模型的代码之后，在新的单元格中运行此代码)\n",
    "# 假设以下变量已在之前的单元格中计算并可用:\n",
    "# y_train_full: 真实的 log_price (训练集)\n",
    "# y_pred_train_log: 使用 best_ridge_model 在 X_train_full 上的预测 log_price\n",
    "# X_train_full: 最终的训练特征 DataFrame (用于对照分析)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- 10. 正在进行 Ridge 模型残差分析 (基于训练集)... ---\")\n",
    "\n",
    "# --- a. 计算残差 (对数尺度) ---\n",
    "if 'y_train_full' in locals() and 'y_pred_train_log' in locals():\n",
    "    residuals_log = y_train_full - y_pred_train_log\n",
    "    print(f\"已计算残差，共 {len(residuals_log)} 个。\")\n",
    "else:\n",
    "    raise NameError(\"需要先运行 Ridge 模型训练单元以获得 y_train_full 和 y_pred_train_log\")\n",
    "\n",
    "# --- b. 残差基本统计信息 ---\n",
    "print(\"\\n--- 残差基本统计信息 (对数尺度) ---\")\n",
    "# 使用 Series 的 describe() 方法获取统计量\n",
    "residual_stats = pd.Series(residuals_log).describe()\n",
    "print(residual_stats.to_string())\n",
    "# 检查：均值是否接近 0？标准差多大？最大/最小值表明是否有极端偏差？\n",
    "\n",
    "# --- c. 残差 vs. 预测值 分析 (检查非线性和异方差性) ---\n",
    "print(\"\\n--- 残差 vs. 预测值 分析 (按预测值分位数) ---\")\n",
    "try:\n",
    "    # 将预测值分为 N 个分位数区间 (例如 5 个)\n",
    "    num_quantiles = 5\n",
    "    predicted_quantiles = pd.qcut(y_pred_train_log, q=num_quantiles, labels=False, duplicates='drop')\n",
    "\n",
    "    # 按分位数区间分组计算残差的均值和标准差\n",
    "    grouped_residuals = pd.DataFrame({'residual': residuals_log, 'quantile': predicted_quantiles})\n",
    "    quantile_analysis = grouped_residuals.groupby('quantile')['residual'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "    print(f\"按 {num_quantiles} 个预测值分位数区间统计的残差:\")\n",
    "    print(quantile_analysis.to_string(float_format=\"%.4f\"))\n",
    "    # 检查：\n",
    "    # 1. 各区间 'mean' 是否都接近 0？如果系统性偏离0（例如随区间递增/减），可能存在未捕捉的非线性关系。\n",
    "    # 2. 各区间 'std' 是否大致相等？如果标准差随区间系统性变化（例如变大或变小），可能存在异方差性。\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"无法按预测值分位数分析残差（可能是预测值分布问题）: {e}\")\n",
    "except NameError:\n",
    "     print(\"无法按预测值分位数分析残差，请确保 y_pred_train_log 已定义。\")\n",
    "\n",
    "\n",
    "# --- d. 识别极端残差 (潜在异常值) ---\n",
    "print(\"\\n--- 极端残差识别 ---\")\n",
    "n_extreme = 10 # 显示最大/最小的 N 个残差\n",
    "largest_positive_residuals = pd.Series(residuals_log).nlargest(n_extreme)\n",
    "largest_negative_residuals = pd.Series(residuals_log).nsmallest(n_extreme)\n",
    "\n",
    "print(f\"最大的 {n_extreme} 个正残差 (预测值远低于实际值):\")\n",
    "print(largest_positive_residuals.to_string(float_format=\"%.4f\"))\n",
    "print(f\"\\n最大的 {n_extreme} 个负残差 (预测值远高于实际值):\")\n",
    "print(largest_negative_residuals.to_string(float_format=\"%.4f\"))\n",
    "# 检查：这些极端残差对应的样本点是否有特殊之处？（需要对照原始数据或特征值分析）\n",
    "\n",
    "# --- e. 残差 vs. 关键特征 分析 (检查特定特征的拟合情况) ---\n",
    "print(\"\\n--- 残差 vs. 关键特征 分析 (按特征分位数) ---\")\n",
    "key_features_for_residual_analysis = ['log_建筑面积', '房龄'] # 选择你关心的几个核心特征\n",
    "\n",
    "for feature in key_features_for_residual_analysis:\n",
    "    if feature in X_train_full.columns:\n",
    "        print(f\"\\n--- 分析残差 vs. '{feature}' ---\")\n",
    "        try:\n",
    "            # 将特征值分为 N 个分位数区间\n",
    "            feature_quantiles = pd.qcut(X_train_full[feature], q=num_quantiles, labels=False, duplicates='drop')\n",
    "\n",
    "            # 按特征分位数区间分组计算残差均值\n",
    "            grouped_residuals_feature = pd.DataFrame({'residual': residuals_log, 'quantile': feature_quantiles})\n",
    "            feature_quantile_analysis = grouped_residuals_feature.groupby('quantile')['residual'].agg(['mean', 'count'])\n",
    "\n",
    "            print(f\"按 '{feature}' 的 {num_quantiles} 个分位数区间统计的残差均值:\")\n",
    "            print(feature_quantile_analysis.to_string(float_format=\"%.4f\"))\n",
    "            # 检查：各区间 'mean' 是否都接近 0？如果残差均值随特征值系统性变化，说明模型对该特征的拟合可能仍有改进空间（例如，非线性关系未完全捕捉）。\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"无法按 '{feature}' 的分位数分析残差（可能是特征值分布问题）: {e}\")\n",
    "        except Exception as e_gen:\n",
    "             print(f\"分析残差 vs. '{feature}' 时出错: {e_gen}\")\n",
    "    else:\n",
    "        print(f\"特征 '{feature}' 不在 X_train_full 中，跳过分析。\")\n",
    "\n",
    "print(\"\\n--- 残差分析完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342e967b-a371-4b05-b7a0-84a066b315bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 准备训练和测试数据 (来自 train_df_final 和 test_df_final) ---\n",
      "X_train_full 维度: (101252, 224)\n",
      "X_test 维度: (34017, 224)\n",
      "\n",
      "--- 4. 正在创建 Lasso 预处理管道... ---\n",
      "\n",
      "--- 5. 正在为 Lasso 运行 6-fold GridSearchCV... ---\n",
      "搜索的参数网格: {'regressor__alpha': [0.001]}\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "\n",
      "--- GridSearchCV 训练完成 ---\n",
      "最佳 Alpha (正则化强度): 0.001\n",
      "Lasso 特征选择: 保留了 192 / 224 个特征\n",
      "\n",
      "--- Lasso 模型性能 (MAE in Original Price) ---\n",
      "最佳 Alpha:        0.001\n",
      "In-sample MAE:     454,916.36\n",
      "Out-of-sample MAE (80/20 split): 452,899.78\n",
      "6-Fold CV MAE:     456,015.32\n",
      "Lasso 保留特征: 192 / 224\n",
      "\n",
      "--- Lasso 结果表格 (用于报告) ---\n",
      "| Metrics          |   In sample |   out of sample |   Cross-validation |\n",
      "|:-----------------|------------:|----------------:|-------------------:|\n",
      "| Lasso (新预处理) |  454,916.36 |      452,899.78 |         456,015.32 |\n",
      "\n",
      "--- 9. 正在生成 prediction_price_lasso.csv 文件... ---\n",
      "预测结果已保存到 'prediction_price_lasso.csv'\n",
      "\n",
      "--- Lasso 模型处理完毕 ---\n"
     ]
    }
   ],
   "source": [
    "# --- [ 独立单元 - 运行 Lasso 模型 (带 GridSearchCV) ] ---\n",
    "# (请在 b9daea9b 单元格运行后，在新的单元格中运行此代码)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso # 1. 导入 Lasso\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- 1. 准备训练和测试数据 (来自 train_df_final 和 test_df_final) ---\")\n",
    "\n",
    "# --- 2. (来自 OLS 单元) 准备 X 和 y ---\n",
    "# 假设 train_df_final 和 test_df_final 已在内存中\n",
    "if 'log_price' in train_df_final.columns:\n",
    "    X_train_full = train_df_final.drop('log_price', axis=1)\n",
    "    y_train_full = train_df_final['log_price']\n",
    "else:\n",
    "    raise ValueError(\"'log_price' 列未在最终训练集中找到！\")\n",
    "\n",
    "if 'ID' in test_df_final.columns:\n",
    "    X_test = test_df_final.drop('ID', axis=1)\n",
    "    test_ids = test_df_final['ID']\n",
    "else:\n",
    "    raise ValueError(\"最终测试集中未找到 ID 列！\")\n",
    "\n",
    "# (安全检查) 确保列顺序一致\n",
    "if not all(X_train_full.columns == X_test.columns):\n",
    "    print(\"--- 警告: 训练集和测试集列不匹配，正在重新对齐... ---\")\n",
    "    try:\n",
    "        X_test = X_test[X_train_full.columns]\n",
    "        print(\"--- 重新对齐成功 ---\")\n",
    "    except Exception as e:\n",
    "         raise ValueError(f\"训练集和测试集的特征列不匹配！{e}\")\n",
    "\n",
    "print(f\"X_train_full 维度: {X_train_full.shape}\")\n",
    "print(f\"X_test 维度: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. (来自 OLS 单元) 定义自定义 MAE 评估器 ---\n",
    "def original_price_mae_scorer(y_log, y_pred_log):\n",
    "    \"\"\"\n",
    "    计算原始价格尺度上的 MAE。\n",
    "    \"\"\"\n",
    "    y_orig = np.expm1(y_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    \n",
    "    # 处理预测中的极端值或 NaN (以防万一)\n",
    "    y_pred_orig = np.nan_to_num(y_pred_orig, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred_orig = np.clip(y_pred_orig, 0, None)\n",
    "    \n",
    "    return mean_absolute_error(y_orig, y_pred_orig)\n",
    "\n",
    "# 创建一个 scikit-learn 'scorer' 对象\n",
    "custom_mae_scorer = make_scorer(original_price_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# 定义 6 折交叉验证 (KFold)\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "\n",
    "# --- 4. 【新】创建 Lasso 管道和 GridSearchCV ---\n",
    "print(\"\\n--- 4. 正在创建 Lasso 预处理管道... ---\")\n",
    "\n",
    "# 管道保持不变：先缩放，后回归\n",
    "pipeline_lasso = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    # 2. 使用 Lasso() 并增加 max_iter\n",
    "    ('regressor', Lasso(max_iter=3000, random_state=111)) \n",
    "])\n",
    "\n",
    "# 定义要搜索的 alpha (正则化强度) 网格\n",
    "# (Lasso 的 alpha 通常需要设置得非常小)\n",
    "param_grid_lasso = {\n",
    "    'regressor__alpha': [0.001]\n",
    "}\n",
    "\n",
    "# --- 5. 【新】设置并运行 GridSearchCV ---\n",
    "print(\"\\n--- 5. 正在为 Lasso 运行 6-fold GridSearchCV... ---\")\n",
    "print(f\"搜索的参数网格: {param_grid_lasso}\")\n",
    "\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=pipeline_lasso,    # 我们的管道\n",
    "    param_grid=param_grid_lasso, # 要搜索的 alpha\n",
    "    scoring=custom_mae_scorer,   # 使用自定义的 MAE 评估器\n",
    "    cv=kf,                       # 6-fold\n",
    "    n_jobs=-1,                   # 使用所有 CPU 核心\n",
    "    verbose=2                    # 显示详细进度\n",
    ")\n",
    "\n",
    "# (***) 开始训练 (***)\n",
    "grid_search_lasso.fit(X_train_full, y_train_full)\n",
    "\n",
    "# --- 6. 【新】获取最佳模型和分数 ---\n",
    "print(\"\\n--- GridSearchCV 训练完成 ---\")\n",
    "best_lasso_model = grid_search_lasso.best_estimator_\n",
    "best_mae_cv_lasso = -grid_search_lasso.best_score_  # (Scorer 返回负值, 取反)\n",
    "best_alpha = grid_search_lasso.best_params_['regressor__alpha']\n",
    "\n",
    "print(f\"最佳 Alpha (正则化强度): {best_alpha}\")\n",
    "\n",
    "# (【新】Lasso 独有) 检查有多少特征被保留\n",
    "try:\n",
    "    # 提取 Lasso 回归器\n",
    "    lasso_regressor = best_lasso_model.named_steps['regressor']\n",
    "    # 获取系数\n",
    "    coefficients = lasso_regressor.coef_\n",
    "    # 计算非零系数的数量\n",
    "    n_features_selected = np.sum(coefficients != 0)\n",
    "    total_features = len(coefficients)\n",
    "    print(f\"Lasso 特征选择: 保留了 {n_features_selected} / {total_features} 个特征\")\n",
    "except Exception as e:\n",
    "    print(f\"无法获取Lasso系数: {e}\")\n",
    "\n",
    "\n",
    "# --- 7. (来自 OLS 单元) 计算 In-sample 和 Out-of-sample MAE ---\n",
    "# (为了完成作业报告)\n",
    "\n",
    "# a. In-sample (样本内) MAE\n",
    "y_pred_train_log = best_lasso_model.predict(X_train_full)\n",
    "y_train_orig = np.expm1(y_train_full)\n",
    "y_pred_train_orig = np.expm1(y_pred_train_log)\n",
    "mae_in_sample = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "\n",
    "\n",
    "# b. Out-of-sample (样本外) MAE - 使用 80/20 划分\n",
    "X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=111\n",
    ")\n",
    "\n",
    "# (创建一个使用最佳 alpha 的新管道)\n",
    "pipeline_lasso_best = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Lasso(alpha=best_alpha, max_iter=3000, random_state=111)) \n",
    "])\n",
    "# (仅在 80% 的数据上拟合)\n",
    "pipeline_lasso_best.fit(X_train_temp, y_train_temp) \n",
    "\n",
    "# (在 20% 的数据上预测)\n",
    "y_pred_val_log = pipeline_lasso_best.predict(X_val_temp) \n",
    "mae_out_of_sample = mean_absolute_error(np.expm1(y_val_temp), np.expm1(y_pred_val_log))\n",
    "\n",
    "\n",
    "# --- 8. 报告结果 ---\n",
    "print(\"\\n--- Lasso 模型性能 (MAE in Original Price) ---\")\n",
    "print(f\"最佳 Alpha:        {best_alpha}\")\n",
    "print(f\"In-sample MAE:     {mae_in_sample:,.2f}\")\n",
    "print(f\"Out-of-sample MAE (80/20 split): {mae_out_of_sample:,.2f}\")\n",
    "print(f\"6-Fold CV MAE:     {best_mae_cv_lasso:,.2f}\")\n",
    "print(f\"Lasso 保留特征: {n_features_selected} / {total_features}\")\n",
    "\n",
    "# (生成作业要求的表格)\n",
    "lasso_results = {\n",
    "    'Metrics': 'Lasso (新预处理)',\n",
    "    'In sample': mae_in_sample,\n",
    "    'out of sample': mae_out_of_sample, \n",
    "    'Cross-validation': best_mae_cv_lasso\n",
    "}\n",
    "print(\"\\n--- Lasso 结果表格 (用于报告) ---\")\n",
    "print(pd.DataFrame([lasso_results]).to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "\n",
    "\n",
    "# --- 9. 生成预测文件 (prediction_price_lasso.csv) ---\n",
    "print(\"\\n--- 9. 正在生成 prediction_price_lasso.csv 文件... ---\")\n",
    "\n",
    "# (使用 GCV 找到的最佳模型预测 X_test)\n",
    "y_pred_test_log = grid_search_lasso.predict(X_test)\n",
    "\n",
    "# c. 反向转换并处理异常值\n",
    "y_pred_test_orig = np.expm1(y_pred_test_log)\n",
    "fallback_price = np.expm1(y_train_full.median()) \n",
    "y_pred_test_orig = np.nan_to_num(y_pred_test_orig, nan=fallback_price, posinf=np.finfo(np.float64).max, neginf=0.0) \n",
    "y_pred_test_orig = np.clip(y_pred_test_orig, 0, None) \n",
    "\n",
    "# d. 创建提交 DataFrame\n",
    "submission_df_lasso = pd.DataFrame({'ID': test_ids, 'Price': y_pred_test_orig})\n",
    "\n",
    "# e. 保存为 CSV 文件\n",
    "submission_filename_lasso = 'prediction_price_lasso.csv'\n",
    "submission_df_lasso.to_csv(submission_filename_lasso, index=False)\n",
    "\n",
    "print(f\"预测结果已保存到 '{submission_filename_lasso}'\")\n",
    "print(\"\\n--- Lasso 模型处理完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00168650-3730-4d70-adb9-6bf4450c4a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 准备训练和测试数据 (来自 train_df_final 和 test_df_final) ---\n",
      "X_train_full 维度: (101252, 224)\n",
      "X_test 维度: (34017, 224)\n",
      "\n",
      "--- 4. 正在创建 随机森林 预处理管道... ---\n",
      "\n",
      "--- 6. 正在为 随机森林 运行 6-fold RandomizedSearchCV... ---\n",
      "将随机尝试 10 种参数组合\n",
      "Fitting 6 folds for each of 10 candidates, totalling 60 fits\n",
      "\n",
      "--- RandomizedSearchCV 训练完成 --- (耗时: 1010.22 秒)\n",
      "\n",
      "最佳参数组合: {'regressor__n_estimators': 300, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__max_features': 1.0, 'regressor__max_depth': 30}\n",
      "\n",
      "--- 随机森林 模型性能 (MAE in Original Price) ---\n",
      "最佳参数: {'regressor__n_estimators': 300, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 5, 'regressor__max_features': 1.0, 'regressor__max_depth': 30}\n",
      "In-sample MAE:     145,877.78\n",
      "Out-of-sample MAE (80/20 split): 141,760.66\n",
      "6-Fold CV MAE (来自搜索): 225,818.19\n",
      "\n",
      "--- 随机森林 结果表格 (用于报告) ---\n",
      "\n",
      "| Metrics       |   In sample |   out of sample |   Cross-validation |\n",
      "|:--------------|------------:|----------------:|-------------------:|\n",
      "| Random Forest |  145,877.78 |      141,760.66 |         225,818.19 |\n",
      "\n",
      "--- 10. 正在生成 prediction_price_rf.csv 文件... ---\n",
      "预测结果已保存到 'prediction_price_rf.csv'\n",
      "\n",
      "--- 随机森林 模型处理完毕 ---\n"
     ]
    }
   ],
   "source": [
    "# --- [ 独立单元 - 运行 随机森林 (Random Forest) 模型 (带 RandomizedSearchCV) ] ---\n",
    "# (请在 STAGE 8 清理完成，获得 train_df_final 和 test_df_final 后，在新的单元格中运行此代码)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler # 可以选择你最终使用的 Scaler\n",
    "from sklearn.ensemble import RandomForestRegressor # 1. 导入 RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import time # 用于记录时间\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- 1. 准备训练和测试数据 (来自 train_df_final 和 test_df_final) ---\")\n",
    "\n",
    "# --- 2. 准备 X 和 y ---\n",
    "# 假设 train_df_final 和 test_df_final 已在内存中\n",
    "if 'log_price' in train_df_final.columns:\n",
    "    X_train_full = train_df_final.drop('log_price', axis=1)\n",
    "    y_train_full = train_df_final['log_price']\n",
    "else:\n",
    "    raise ValueError(\"'log_price' 列未在最终训练集中找到！\")\n",
    "\n",
    "if 'ID' in test_df_final.columns:\n",
    "    X_test = test_df_final.drop('ID', axis=1)\n",
    "    test_ids = test_df_final['ID']\n",
    "else:\n",
    "    raise ValueError(\"最终测试集中未找到 ID 列！\")\n",
    "\n",
    "# (安全检查) 确保列顺序一致\n",
    "if not all(X_train_full.columns == X_test.columns):\n",
    "    print(\"--- 警告: 训练集和测试集列不匹配，正在重新对齐... ---\")\n",
    "    try:\n",
    "        X_test = X_test[X_train_full.columns]\n",
    "        print(\"--- 重新对齐成功 ---\")\n",
    "    except Exception as e:\n",
    "         raise ValueError(f\"训练集和测试集的特征列不匹配！{e}\")\n",
    "\n",
    "print(f\"X_train_full 维度: {X_train_full.shape}\")\n",
    "print(f\"X_test 维度: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. 定义自定义 MAE 评估器 ---\n",
    "# (这段代码在你之前的单元格中应该已经定义过了，这里为了独立性再次包含)\n",
    "def original_price_mae_scorer(y_log, y_pred_log):\n",
    "    \"\"\"计算原始价格尺度上的 MAE。\"\"\"\n",
    "    y_orig = np.expm1(y_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    y_pred_orig = np.nan_to_num(y_pred_orig, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred_orig = np.clip(y_pred_orig, 0, None)\n",
    "    return mean_absolute_error(y_orig, y_pred_orig)\n",
    "\n",
    "custom_mae_scorer = make_scorer(original_price_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# 定义 6 折交叉验证 (KFold)\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "\n",
    "# --- 4. 【新】创建 随机森林 管道和 RandomizedSearchCV ---\n",
    "print(\"\\n--- 4. 正在创建 随机森林 预处理管道... ---\")\n",
    "\n",
    "# 选择 Scaler (根据你之前的选择，StandardScaler 或 RobustScaler)\n",
    "# scaler_to_use = StandardScaler()\n",
    "scaler_to_use = RobustScaler() # 如果你决定使用 RobustScaler\n",
    "\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('scaler', scaler_to_use),\n",
    "    # 2. 使用 RandomForestRegressor()\n",
    "    # n_jobs=-1 使用所有 CPU 核心加速训练\n",
    "    # random_state=111 保证结果可复现\n",
    "    ('regressor', RandomForestRegressor(n_jobs=-1, random_state=111))\n",
    "])\n",
    "\n",
    "# --- 5. 【新】定义随机森林的超参数搜索空间 ---\n",
    "# RandomizedSearchCV 比 GridSearchCV 更快地探索大范围参数\n",
    "param_dist_rf = {\n",
    "    'regressor__n_estimators': [100, 200, 300],       # 树的数量\n",
    "    'regressor__max_depth': [10, 20, 30, None],       # 树的最大深度 (None 表示不限制)\n",
    "    'regressor__min_samples_split': [2, 5, 10],     # 节点分裂所需的最小样本数\n",
    "    'regressor__min_samples_leaf': [1, 3, 5],        # 叶节点所需的最小样本数\n",
    "    'regressor__max_features': ['sqrt', 'log2', 1.0] # 每次分裂考虑的特征比例/数量 ('sqrt', 'log2', 或 1.0 代表全部)\n",
    "}\n",
    "# 注意: 这个搜索空间可以根据你的计算资源和时间进行调整\n",
    "\n",
    "n_iter_search = 10 # 随机搜索的迭代次数 (尝试的参数组合数量)，可以适当增加以获得更好结果，但会增加时间\n",
    "\n",
    "# --- 6. 【新】设置并运行 RandomizedSearchCV ---\n",
    "print(\"\\n--- 6. 正在为 随机森林 运行 6-fold RandomizedSearchCV... ---\")\n",
    "print(f\"将随机尝试 {n_iter_search} 种参数组合\")\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=pipeline_rf,        # 我们的管道\n",
    "    param_distributions=param_dist_rf, # 要搜索的参数分布\n",
    "    n_iter=n_iter_search,         # 随机搜索次数\n",
    "    scoring=custom_mae_scorer,    # 使用自定义的 MAE 评估器\n",
    "    cv=kf,                        # 6-fold\n",
    "    n_jobs=-1,                    # 使用所有 CPU 核心 (GridSearchCV 本身也并行)\n",
    "    random_state=111,             # 保证随机搜索过程可复现\n",
    "    verbose=2                     # 显示详细进度\n",
    ")\n",
    "\n",
    "# (***) 开始训练 (***)\n",
    "start_time = time.time()\n",
    "random_search_rf.fit(X_train_full, y_train_full)\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- RandomizedSearchCV 训练完成 --- (耗时: {end_time - start_time:.2f} 秒)\")\n",
    "\n",
    "# --- 7. 【新】获取最佳模型和分数 ---\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "best_mae_cv_rf = -random_search_rf.best_score_  # (Scorer 返回负值, 取反)\n",
    "best_params_rf = random_search_rf.best_params_\n",
    "\n",
    "print(f\"\\n最佳参数组合: {best_params_rf}\")\n",
    "\n",
    "\n",
    "# --- 8. 计算 In-sample 和 Out-of-sample MAE ---\n",
    "# (为了与其他模型比较)\n",
    "\n",
    "# a. In-sample (样本内) MAE\n",
    "y_pred_train_log_rf = best_rf_model.predict(X_train_full)\n",
    "y_train_orig = np.expm1(y_train_full) # 原始 y_train\n",
    "y_pred_train_orig_rf = np.expm1(y_pred_train_log_rf)\n",
    "mae_in_sample_rf = mean_absolute_error(y_train_orig, y_pred_train_orig_rf)\n",
    "\n",
    "# b. Out-of-sample (样本外) MAE - 使用 80/20 划分\n",
    "# (注意：这里的 OOS MAE 是基于一次 80/20 划分的估计，CV MAE 通常更可靠)\n",
    "X_train_temp_rf, X_val_temp_rf, y_train_temp_rf, y_val_temp_rf = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=111\n",
    ")\n",
    "# 需要用找到的最佳参数重新拟合模型 (因为 best_rf_model 是在全数据上 refit 的)\n",
    "# 这里直接使用 best_rf_model 在验证集上预测可能稍微高估性能，但更方便\n",
    "y_pred_val_log_rf = best_rf_model.predict(X_val_temp_rf) # 使用已在全数据训练的最佳模型\n",
    "mae_out_of_sample_rf = mean_absolute_error(np.expm1(y_val_temp_rf), np.expm1(y_pred_val_log_rf))\n",
    "\n",
    "\n",
    "# --- 9. 报告结果 ---\n",
    "print(\"\\n--- 随机森林 模型性能 (MAE in Original Price) ---\")\n",
    "print(f\"最佳参数: {best_params_rf}\")\n",
    "print(f\"In-sample MAE:     {mae_in_sample_rf:,.2f}\")\n",
    "print(f\"Out-of-sample MAE (80/20 split): {mae_out_of_sample_rf:,.2f}\")\n",
    "print(f\"6-Fold CV MAE (来自搜索): {best_mae_cv_rf:,.2f}\")\n",
    "\n",
    "# (生成结果表格)\n",
    "rf_results = {\n",
    "    'Metrics': 'Random Forest',\n",
    "    'In sample': mae_in_sample_rf,\n",
    "    'out of sample': mae_out_of_sample_rf,\n",
    "    'Cross-validation': best_mae_cv_rf\n",
    "}\n",
    "print(\"\\n--- 随机森林 结果表格 (用于报告) ---\\n\")\n",
    "# 将新结果添加到之前的结果 DataFrame (如果存在的话) 或单独打印\n",
    "# 假设 all_results_df 是之前包含 OLS, Ridge 等结果的 DataFrame\n",
    "# all_results_df = pd.concat([all_results_df, pd.DataFrame([rf_results])], ignore_index=True)\n",
    "# print(all_results_df.to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "# 或者单独打印\n",
    "print(pd.DataFrame([rf_results]).to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "\n",
    "\n",
    "# --- 10. 生成预测文件 (prediction_price_rf.csv) ---\n",
    "print(\"\\n--- 10. 正在生成 prediction_price_rf.csv 文件... ---\")\n",
    "\n",
    "# (RandomizedSearchCV 对象在 .fit() 之后，会自动在 *全部* 训练数据上\n",
    "#  用最佳参数重新训练一个模型，存储在 best_estimator_ 中)\n",
    "# (我们可以直接使用它来预测 X_test)\n",
    "y_pred_test_log_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# c. 反向转换并处理异常值\n",
    "y_pred_test_orig_rf = np.expm1(y_pred_test_log_rf)\n",
    "fallback_price_rf = np.expm1(y_train_full.median()) # 仍然使用中位数作为备用\n",
    "y_pred_test_orig_rf = np.nan_to_num(y_pred_test_orig_rf, nan=fallback_price_rf, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "y_pred_test_orig_rf = np.clip(y_pred_test_orig_rf, 0, None)\n",
    "\n",
    "# d. 创建提交 DataFrame\n",
    "submission_df_rf = pd.DataFrame({'ID': test_ids, 'Price': y_pred_test_orig_rf})\n",
    "\n",
    "# e. 保存为 CSV 文件\n",
    "submission_filename_rf = 'prediction_price_rf.csv'\n",
    "submission_df_rf.to_csv(submission_filename_rf, index=False)\n",
    "\n",
    "print(f\"预测结果已保存到 '{submission_filename_rf}'\")\n",
    "print(\"\\n--- 随机森林 模型处理完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c33f330-ec3a-46a0-924a-b5e3d792201a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 准备训练和测试数据 ('rent' data) ---\n",
      "--- 正在转换 ['区县', '板块'] 为 'category' 类型 ---\n",
      "X_train_full 维度: (98890, 141)\n",
      "X_test 维度: (9773, 141)\n",
      "\n",
      "--- 2. 正在创建预处理管道... ---\n",
      "\n",
      "--- 4. 正在使用 GridSearchCV 寻找最佳 Ridge Alpha... ---\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "--- GridSearchCV 完成 ---\n",
      "最佳 Ridge Alpha: 1000\n",
      "对应的最佳 CV MAE (原始价格): 123,206.70\n",
      "\n",
      "--- Ridge 模型性能 ('rent' data, MAE in Original Price) ---\n",
      "Best Alpha:        1000\n",
      "In-sample MAE:     122,038.54\n",
      "Out-of-sample MAE (local validation): 123,175.82\n",
      "6-Fold CV MAE:     123,206.70\n",
      "\n",
      "--- Ridge 结果表格 ('rent' data, 用于报告) ---\n",
      "| Metrics   |   In sample |   out of sample |   Cross-validation |\n",
      "|:----------|------------:|----------------:|-------------------:|\n",
      "| Ridge     |  122,038.54 |      123,175.82 |         123,206.70 |\n",
      "\n",
      "--- 7. 正在使用最佳 Alpha 重新训练模型并生成 prediction_rent_ridge.csv 文件... ---\n",
      "预测结果已保存到 'prediction_rent_ridge.csv'\n",
      "\n",
      "--- Ridge 模型 ('rent' data) 处理完毕 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge # 使用 Ridge\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# !!! 警告: TargetEncoder 需要 category_encoders 库\n",
    "# 在 Jupyter 中，您可能需要先运行: !pip install category_encoders\n",
    "try:\n",
    "    from category_encoders import TargetEncoder\n",
    "except ImportError:\n",
    "    print(\"错误: TargetEncoder 未找到。\")\n",
    "    print(\"请先在您的环境中运行: pip install category_encoders\")\n",
    "    TargetEncoder = None # Define as None if not found\n",
    "\n",
    "# --- (假设 train_df_final 和 test_df_final 是 'rent' 数据处理后的 DataFrame) ---\n",
    "# train_df_final: 包含 log_price, 不含 ID\n",
    "# test_df_final: 包含 ID, 不含 log_price\n",
    "\n",
    "print(\"--- 1. 准备训练和测试数据 ('rent' data) ---\")\n",
    "\n",
    "# a. 分离训练集的 X 和 y\n",
    "if 'log_price' in train_df_final.columns:\n",
    "    X_train_full = train_df_final.drop('log_price', axis=1)\n",
    "    y_train_full = train_df_final['log_price']\n",
    "else:\n",
    "    raise ValueError(\"'log_price' 列未在最终训练集中找到！\")\n",
    "\n",
    "# b. 分离测试集的 X 和 ID\n",
    "if 'ID' in test_df_final.columns:\n",
    "    X_test = test_df_final.drop('ID', axis=1)\n",
    "    test_ids = test_df_final['ID']\n",
    "else:\n",
    "    print(\"警告: 最终测试集中未找到 ID 列。将生成虚拟 ID。\")\n",
    "    X_test = test_df_final.copy()\n",
    "    test_ids = pd.Series(range(len(X_test)), name=\"ID\")\n",
    "\n",
    "# c. 验证列匹配 (保险起见)\n",
    "if not all(X_train_full.columns == X_test.columns):\n",
    "    print(\"警告: 训练集和测试集列不匹配，尝试重新对齐...\")\n",
    "    # (省略对齐代码 - 假设上一步已完成)\n",
    "    train_cols_set = set(X_train_full.columns)\n",
    "    test_cols_set = set(X_test.columns)\n",
    "    if train_cols_set != test_cols_set:\n",
    "         raise ValueError(\"训练集和测试集的特征列不匹配！请检查处理流程。\")\n",
    "    else:\n",
    "         X_test = X_test[X_train_full.columns]\n",
    "         print(\"列已对齐。\")\n",
    "\n",
    "# d. 转换 TargetEncoder 列为 category 类型\n",
    "target_encode_cols = ['区县', '板块'] # Rent 数据需要 Target Encoding 的列\n",
    "target_encode_cols = [col for col in target_encode_cols if col in X_train_full.columns]\n",
    "if target_encode_cols and TargetEncoder is None:\n",
    "     raise ImportError(\"TargetEncoder is required but not found.\")\n",
    "if target_encode_cols:\n",
    "    print(f\"--- 正在转换 {target_encode_cols} 为 'category' 类型 ---\")\n",
    "    for col in target_encode_cols:\n",
    "        X_train_full[col] = X_train_full[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(f\"X_train_full 维度: {X_train_full.shape}\")\n",
    "print(f\"X_test 维度: {X_test.shape}\")\n",
    "\n",
    "# --- 2. 创建预处理管道 (与 OLS/Lasso 脚本相同) ---\n",
    "print(\"\\n--- 2. 正在创建预处理管道... ---\")\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in target_encode_cols]\n",
    "transformers = []\n",
    "if target_encode_cols:\n",
    "    transformers.append(('target_encoder', TargetEncoder(), target_encode_cols))\n",
    "transformers.append(('standard_scaler', StandardScaler(), numeric_cols))\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "\n",
    "# --- 3. 定义 Ridge 模型和超参数网格 ---\n",
    "# (创建 Pipeline，Regressor 更改为 Ridge)\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(random_state=111)) # 使用 Ridge\n",
    "])\n",
    "\n",
    "# 定义要搜索的 alpha (L2 正则化强度) 值\n",
    "param_grid_ridge = {\n",
    "    'regressor__alpha': [1000] # 例如: 0.001, 0.01, 0.1, 1, 10, 100, 1000\n",
    "}\n",
    "\n",
    "# --- 4. 使用 GridSearchCV 寻找最佳 Alpha ---\n",
    "print(\"\\n--- 4. 正在使用 GridSearchCV 寻找最佳 Ridge Alpha... ---\")\n",
    "\n",
    "# 定义 MAE 评分器 (与之前相同)\n",
    "def original_price_mae_scorer(y_log, y_pred_log):\n",
    "    y_orig = np.expm1(y_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    y_pred_orig = np.nan_to_num(y_pred_orig, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred_orig = np.clip(y_pred_orig, 0, None)\n",
    "    return mean_absolute_error(y_orig, y_pred_orig)\n",
    "\n",
    "neg_mae_scorer = make_scorer(original_price_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# 使用 6 折交叉验证\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    pipeline_ridge,\n",
    "    param_grid_ridge,\n",
    "    cv=kf,\n",
    "    scoring=neg_mae_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1 # 显示进度\n",
    ")\n",
    "\n",
    "# 在完整的训练集上运行 GridSearch\n",
    "grid_search_ridge.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 获取最佳模型和最佳 alpha\n",
    "best_model_ridge = grid_search_ridge.best_estimator_\n",
    "best_alpha_ridge = grid_search_ridge.best_params_['regressor__alpha']\n",
    "\n",
    "print(f\"--- GridSearchCV 完成 ---\")\n",
    "print(f\"最佳 Ridge Alpha: {best_alpha_ridge}\")\n",
    "print(f\"对应的最佳 CV MAE (原始价格): {-grid_search_ridge.best_score_:,.2f}\")\n",
    "\n",
    "# --- 5. 计算指标 (使用最佳模型) ---\n",
    "\n",
    "# a. In-sample (样本内) MAE\n",
    "y_train_orig = np.expm1(y_train_full) # 真实的原始价格\n",
    "y_pred_train_log = best_model_ridge.predict(X_train_full)\n",
    "y_pred_train_orig = np.expm1(y_pred_train_log)\n",
    "mae_in_sample = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "\n",
    "# b. Out-of-sample (样本外) MAE - 使用临时的 test split\n",
    "X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=111\n",
    ")\n",
    "best_model_ridge.fit(X_train_temp, y_train_temp) # 用最佳 alpha 重新训练 (以正确fit TargetEncoder)\n",
    "y_pred_val_log = best_model_ridge.predict(X_val_temp)\n",
    "mae_out_of_sample = mean_absolute_error(np.expm1(y_val_temp), np.expm1(y_pred_val_log))\n",
    "del X_train_temp, X_val_temp, y_train_temp, y_val_temp # 清理\n",
    "\n",
    "# c. 6-fold Cross-validation MAE (GridSearch 已经计算过了)\n",
    "mae_cv = -grid_search_ridge.best_score_\n",
    "\n",
    "# --- 6. 报告结果 ---\n",
    "print(\"\\n--- Ridge 模型性能 ('rent' data, MAE in Original Price) ---\")\n",
    "print(f\"Best Alpha:        {best_alpha_ridge}\")\n",
    "print(f\"In-sample MAE:     {mae_in_sample:,.2f}\")\n",
    "print(f\"Out-of-sample MAE (local validation): {mae_out_of_sample:,.2f}\")\n",
    "print(f\"6-Fold CV MAE:     {mae_cv:,.2f}\")\n",
    "\n",
    "ridge_results_rent = {\n",
    "    'Metrics': 'Ridge',\n",
    "    'In sample': mae_in_sample,\n",
    "    'out of sample': mae_out_of_sample, # 本地验证集\n",
    "    'Cross-validation': mae_cv\n",
    "}\n",
    "\n",
    "print(\"\\n--- Ridge 结果表格 ('rent' data, 用于报告) ---\")\n",
    "print(pd.DataFrame([ridge_results_rent]).to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "\n",
    "# --- 7. 生成预测文件 (prediction_rent_ridge.csv) ---\n",
    "print(\"\\n--- 7. 正在使用最佳 Alpha 重新训练模型并生成 prediction_rent_ridge.csv 文件... ---\")\n",
    "\n",
    "# a. 使用找到的最佳 Alpha 在 *完整* 训练集上训练最终模型\n",
    "final_model_ridge = grid_search_ridge.best_estimator_ # GridSearch 默认会 refit\n",
    "# final_model_ridge.fit(X_train_full, y_train_full) # 可以选择显式 refit\n",
    "\n",
    "# b. 预测测试集\n",
    "y_pred_test_log = final_model_ridge.predict(X_test)\n",
    "\n",
    "# c. 反向转换并处理异常值\n",
    "y_pred_test_orig = np.expm1(y_pred_test_log)\n",
    "fallback_price = np.expm1(y_train_full.median()) # 使用训练集的中位数租金作为 fallback\n",
    "y_pred_test_orig = np.nan_to_num(y_pred_test_orig, nan=fallback_price, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "y_pred_test_orig = np.clip(y_pred_test_orig, 0, None) # 确保租金非负\n",
    "\n",
    "# d. 创建提交 DataFrame\n",
    "submission_df_ridge_rent = pd.DataFrame({'ID': test_ids, 'Price': y_pred_test_orig})\n",
    "\n",
    "# e. 保存为 CSV 文件\n",
    "submission_filename_ridge_rent = 'prediction_rent_ridge.csv'\n",
    "submission_df_ridge_rent.to_csv(submission_filename_ridge_rent, index=False)\n",
    "\n",
    "# Make the submission file available for download\n",
    "# from google.colab import files # Uncomment if in Colab\n",
    "# files.download(submission_filename_ridge_rent)\n",
    "\n",
    "print(f\"预测结果已保存到 '{submission_filename_ridge_rent}'\") #.\") # Comment out if not in Colab\n",
    "print(\"\\n--- Ridge 模型 ('rent' data) 处理完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7259b048-0db7-4a66-8600-73c4e5ed4311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 准备训练和测试数据 ('rent' data) ---\n",
      "--- 正在转换 ['区县', '板块'] 为 'category' 类型 ---\n",
      "X_train_full 维度: (98890, 141)\n",
      "X_test 维度: (9773, 141)\n",
      "\n",
      "--- 2. 正在创建预处理管道... ---\n",
      "\n",
      "--- 3. 正在定义 Lasso 模型和超参数网格... ---\n",
      "\n",
      "--- 4. 正在使用 GridSearchCV 寻找最佳 Lasso Alpha... ---\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "--- GridSearchCV 完成 ---\n",
      "最佳 Lasso Alpha: 0.0001\n",
      "对应的最佳 CV MAE (原始价格): 120,143.04\n",
      "\n",
      "--- 5. 正在计算 Lasso 指标... ---\n",
      "\n",
      "--- Lasso 模型性能 ('rent' data, MAE in Original Price) ---\n",
      "Best Alpha:        0.0001\n",
      "In-sample MAE:     119,219.66\n",
      "Out-of-sample MAE (local validation): 120,216.47\n",
      "6-Fold CV MAE:     120,143.04\n",
      "\n",
      "--- Lasso 结果表格 ('rent' data, 用于报告) ---\n",
      "| Metrics   |   In sample |   out of sample |   Cross-validation |\n",
      "|:----------|------------:|----------------:|-------------------:|\n",
      "| Lasso     |  119,219.66 |      120,216.47 |         120,143.04 |\n",
      "\n",
      "--- 7. 正在使用最佳 Alpha 重新训练模型并生成 prediction_rent_lasso.csv 文件... ---\n",
      "预测结果已保存到 'prediction_rent_lasso.csv'\n",
      "\n",
      "--- Lasso 模型 ('rent' data) 处理完毕 ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso # <-- 更改为 Lasso\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 确保 category_encoders 已安装\n",
    "try:\n",
    "    from category_encoders import TargetEncoder\n",
    "except ImportError:\n",
    "    print(\"错误: TargetEncoder 未找到。\")\n",
    "    print(\"请先在您的环境中运行: pip install category_encoders\")\n",
    "    TargetEncoder = None # Define as None if not found\n",
    "\n",
    "# --- (假设 train_df_final 和 test_df_final 已在内存中) ---\n",
    "\n",
    "print(\"--- 1. 准备训练和测试数据 ('rent' data) ---\")\n",
    "\n",
    "# a. 分离训练集的 X 和 y\n",
    "if 'log_price' in train_df_final.columns:\n",
    "    X_train_full = train_df_final.drop('log_price', axis=1)\n",
    "    y_train_full = train_df_final['log_price']\n",
    "else:\n",
    "    raise ValueError(\"'log_price' 列未在最终训练集中找到！\")\n",
    "\n",
    "# b. 分离测试集的 X 和 ID\n",
    "if 'ID' in test_df_final.columns:\n",
    "    X_test = test_df_final.drop('ID', axis=1)\n",
    "    test_ids = test_df_final['ID']\n",
    "else:\n",
    "    print(\"警告: 最终测试集中未找到 ID 列。将生成虚拟 ID。\")\n",
    "    X_test = test_df_final.copy()\n",
    "    test_ids = pd.Series(range(len(X_test)), name=\"ID\")\n",
    "\n",
    "# c. 验证列匹配 (保险起见)\n",
    "if not all(X_train_full.columns == X_test.columns):\n",
    "    print(\"警告: 训练集和测试集列不匹配，尝试重新对齐...\")\n",
    "    train_cols_set = set(X_train_full.columns)\n",
    "    test_cols_set = set(X_test.columns)\n",
    "    if train_cols_set != test_cols_set:\n",
    "         raise ValueError(\"训练集和测试集的特征列不匹配！请检查处理流程。\")\n",
    "    else:\n",
    "         X_test = X_test[X_train_full.columns]\n",
    "         print(\"列已对齐。\")\n",
    "\n",
    "# d. 转换 TargetEncoder 列为 category 类型\n",
    "target_encode_cols = ['区县', '板块'] # Rent 数据需要 Target Encoding 的列\n",
    "target_encode_cols = [col for col in target_encode_cols if col in X_train_full.columns]\n",
    "if target_encode_cols and TargetEncoder is None:\n",
    "     raise ImportError(\"TargetEncoder is required but not found.\")\n",
    "if target_encode_cols:\n",
    "    print(f\"--- 正在转换 {target_encode_cols} 为 'category' 类型 ---\")\n",
    "    for col in target_encode_cols:\n",
    "        X_train_full[col] = X_train_full[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(f\"X_train_full 维度: {X_train_full.shape}\")\n",
    "print(f\"X_test 维度: {X_test.shape}\")\n",
    "\n",
    "# --- 2. 创建预处理管道 (与 OLS/Ridge 脚本相同) ---\n",
    "print(\"\\n--- 2. 正在创建预处理管道... ---\")\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in target_encode_cols]\n",
    "transformers = []\n",
    "if target_encode_cols:\n",
    "    transformers.append(('target_encoder', TargetEncoder(), target_encode_cols))\n",
    "transformers.append(('standard_scaler', StandardScaler(), numeric_cols))\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "\n",
    "# --- 3. 定义 Lasso 模型和超参数网格 ---\n",
    "print(\"\\n--- 3. 正在定义 Lasso 模型和超参数网格... ---\")\n",
    "# (创建 Pipeline，Regressor 更改为 Lasso)\n",
    "pipeline_lasso = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso(random_state=111, max_iter=5000)) # <-- 使用 Lasso，增加 max_iter\n",
    "])\n",
    "\n",
    "# 定义要搜索的 alpha (L1 正则化强度) 值\n",
    "# 注意: Lasso 的 alpha 通常需要设置得非常小\n",
    "param_grid_lasso = {\n",
    "    'regressor__alpha': [0.0001] # <-- 示例网格，您可能需要调整\n",
    "}\n",
    "\n",
    "# --- 4. 使用 GridSearchCV 寻找最佳 Alpha ---\n",
    "print(\"\\n--- 4. 正在使用 GridSearchCV 寻找最佳 Lasso Alpha... ---\")\n",
    "\n",
    "# 定义 MAE 评分器 (与之前相同)\n",
    "def original_price_mae_scorer(y_log, y_pred_log):\n",
    "    y_orig = np.expm1(y_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    y_pred_orig = np.nan_to_num(y_pred_orig, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred_orig = np.clip(y_pred_orig, 0, None)\n",
    "    return mean_absolute_error(y_orig, y_pred_orig)\n",
    "\n",
    "neg_mae_scorer = make_scorer(original_price_mae_scorer, greater_is_better=False)\n",
    "\n",
    "# 使用 6 折交叉验证\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    pipeline_lasso,\n",
    "    param_grid_lasso,\n",
    "    cv=kf,\n",
    "    scoring=neg_mae_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1 # 显示进度\n",
    ")\n",
    "\n",
    "# 在完整的训练集上运行 GridSearch\n",
    "grid_search_lasso.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 获取最佳模型和最佳 alpha\n",
    "best_model_lasso = grid_search_lasso.best_estimator_\n",
    "best_alpha_lasso = grid_search_lasso.best_params_['regressor__alpha']\n",
    "\n",
    "print(f\"--- GridSearchCV 完成 ---\")\n",
    "print(f\"最佳 Lasso Alpha: {best_alpha_lasso}\")\n",
    "print(f\"对应的最佳 CV MAE (原始价格): {-grid_search_lasso.best_score_:,.2f}\")\n",
    "\n",
    "# --- 5. 计算指标 (使用最佳模型) ---\n",
    "print(\"\\n--- 5. 正在计算 Lasso 指标... ---\")\n",
    "\n",
    "# a. In-sample (样本内) MAE\n",
    "y_train_orig = np.expm1(y_train_full) # 真实的原始价格\n",
    "y_pred_train_log = best_model_lasso.predict(X_train_full)\n",
    "y_pred_train_orig = np.expm1(y_pred_train_log)\n",
    "mae_in_sample = mean_absolute_error(y_train_orig, y_pred_train_orig)\n",
    "\n",
    "# b. Out-of-sample (样本外) MAE - 使用临时的 test split\n",
    "X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=111\n",
    ")\n",
    "best_model_lasso.fit(X_train_temp, y_train_temp) # 用最佳 alpha 重新训练 (以正确fit TargetEncoder)\n",
    "y_pred_val_log = best_model_lasso.predict(X_val_temp)\n",
    "mae_out_of_sample = mean_absolute_error(np.expm1(y_val_temp), np.expm1(y_pred_val_log))\n",
    "del X_train_temp, X_val_temp, y_train_temp, y_val_temp # 清理\n",
    "\n",
    "# c. 6-fold Cross-validation MAE (GridSearch 已经计算过了)\n",
    "mae_cv = -grid_search_lasso.best_score_\n",
    "\n",
    "# --- 6. 报告结果 ---\n",
    "print(\"\\n--- Lasso 模型性能 ('rent' data, MAE in Original Price) ---\")\n",
    "print(f\"Best Alpha:        {best_alpha_lasso}\")\n",
    "print(f\"In-sample MAE:     {mae_in_sample:,.2f}\")\n",
    "print(f\"Out-of-sample MAE (local validation): {mae_out_of_sample:,.2f}\")\n",
    "print(f\"6-Fold CV MAE:     {mae_cv:,.2f}\")\n",
    "\n",
    "lasso_results_rent = {\n",
    "    'Metrics': 'Lasso',\n",
    "    'In sample': mae_in_sample,\n",
    "    'out of sample': mae_out_of_sample, # 本地验证集\n",
    "    'Cross-validation': mae_cv\n",
    "}\n",
    "\n",
    "print(\"\\n--- Lasso 结果表格 ('rent' data, 用于报告) ---\")\n",
    "print(pd.DataFrame([lasso_results_rent]).to_markdown(index=False, floatfmt=\",.2f\"))\n",
    "\n",
    "# --- 7. 生成预测文件 (prediction_rent_lasso.csv) ---\n",
    "print(\"\\n--- 7. 正在使用最佳 Alpha 重新训练模型并生成 prediction_rent_lasso.csv 文件... ---\")\n",
    "\n",
    "# a. 使用找到的最佳 Alpha 在 *完整* 训练集上训练最终模型\n",
    "# (GridSearch 默认会用最佳参数在完整数据上 refit)\n",
    "final_model_lasso = grid_search_lasso.best_estimator_ \n",
    "\n",
    "# b. 预测测试集\n",
    "y_pred_test_log = final_model_lasso.predict(X_test)\n",
    "\n",
    "# c. 反向转换并处理异常值\n",
    "y_pred_test_orig = np.expm1(y_pred_test_log)\n",
    "fallback_price = np.expm1(y_train_full.median()) # 使用训练集的中位数租金作为 fallback\n",
    "y_pred_test_orig = np.nan_to_num(y_pred_test_orig, nan=fallback_price, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "y_pred_test_orig = np.clip(y_pred_test_orig, 0, None) # 确保租金非负\n",
    "\n",
    "# d. 创建提交 DataFrame\n",
    "submission_df_lasso_rent = pd.DataFrame({'ID': test_ids, 'Price': y_pred_test_orig})\n",
    "\n",
    "# e. 保存为 CSV 文件\n",
    "submission_filename_lasso_rent = 'prediction_rent_lasso.csv'\n",
    "submission_df_lasso_rent.to_csv(submission_filename_lasso_rent, index=False)\n",
    "\n",
    "print(f\"预测结果已保存到 '{submission_filename_lasso_rent}'\")\n",
    "print(\"\\n--- Lasso 模型 ('rent' data) 处理完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657b53c-1198-4b41-a2c3-7372632a5fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
