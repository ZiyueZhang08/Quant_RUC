{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8d8fb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æˆ¿å±‹ä»·æ ¼é¢„æµ‹é¡¹ç›® ===\n",
      "âœ… æ•°æ®åŠ è½½æˆåŠŸ! å½¢çŠ¶: (103871, 55)\n"
     ]
    }
   ],
   "source": [
    "# === å¯¼å…¥åº“ ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== æˆ¿å±‹ä»·æ ¼é¢„æµ‹é¡¹ç›® ===\")\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "try:\n",
    "    df = pd.read_csv('ruc_Class25Q2_train_price.csv')  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„\n",
    "    print(f\"âœ… æ•°æ®åŠ è½½æˆåŠŸ! å½¢çŠ¶: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4134defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== åŸºç¡€æ•°æ®æ¸…æ´— ===\n",
      "åŸå§‹æ•°æ®å½¢çŠ¶: (103871, 55)\n",
      "åˆ é™¤æ•°æ®æ³„éœ²ç‰¹å¾: ['æˆ¿å±‹ä¼˜åŠ¿', 'æ ¸å¿ƒå–ç‚¹', 'æˆ·å‹ä»‹ç»', 'å‘¨è¾¹é…å¥—', 'äº¤é€šå‡ºè¡Œ', 'å®¢æˆ·åé¦ˆ']\n",
      "åˆ é™¤coord_yï¼ˆä¸laté«˜åº¦é‡å¤ï¼‰\n",
      "åˆ é™¤coord_xï¼ˆä¸loné«˜åº¦é‡å¤ï¼‰\n",
      "\n",
      "å¤„ç†ç¼ºå¤±å€¼...\n",
      "\n",
      "å¤„ç†ä»·æ ¼å¼‚å¸¸å€¼...\n",
      "ä»·æ ¼å¼‚å¸¸å€¼æ•°é‡: 624\n",
      "æ¸…æ´—åæ•°æ®å½¢çŠ¶: (103871, 48)\n"
     ]
    }
   ],
   "source": [
    "# === åŸºç¡€æ•°æ®æ¸…æ´— ===\n",
    "print(\"\\n=== åŸºç¡€æ•°æ®æ¸…æ´— ===\")\n",
    "df_clean = df.copy()\n",
    "print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {df_clean.shape}\")\n",
    "\n",
    "# 1. åˆ é™¤æ•°æ®æ³„éœ²ç‰¹å¾\n",
    "leakage_features = ['æˆ¿å±‹ä¼˜åŠ¿', 'æ ¸å¿ƒå–ç‚¹', 'æˆ·å‹ä»‹ç»', 'å‘¨è¾¹é…å¥—', 'äº¤é€šå‡ºè¡Œ', 'å®¢æˆ·åé¦ˆ']\n",
    "leakage_found = [col for col in leakage_features if col in df_clean.columns]\n",
    "print(f\"åˆ é™¤æ•°æ®æ³„éœ²ç‰¹å¾: {leakage_found}\")\n",
    "df_clean = df_clean.drop(leakage_found, axis=1)\n",
    "\n",
    "# 2. å¤„ç†é‡å¤åæ ‡\n",
    "if 'lat' in df_clean.columns and 'coord_y' in df_clean.columns:\n",
    "    corr = df_clean['lat'].corr(df_clean['coord_y'])\n",
    "    if abs(corr) > 0.95:\n",
    "        df_clean = df_clean.drop('coord_y', axis=1)\n",
    "        print(\"åˆ é™¤coord_yï¼ˆä¸laté«˜åº¦é‡å¤ï¼‰\")\n",
    "\n",
    "if 'lon' in df_clean.columns and 'coord_x' in df_clean.columns:\n",
    "    corr = df_clean['lon'].corr(df_clean['coord_x'])\n",
    "    if abs(corr) > 0.95:\n",
    "        df_clean = df_clean.drop('coord_x', axis=1)\n",
    "        print(\"åˆ é™¤coord_xï¼ˆä¸loné«˜åº¦é‡å¤ï¼‰\")\n",
    "\n",
    "# 3. å¤„ç†ç¼ºå¤±å€¼\n",
    "print(f\"\\nå¤„ç†ç¼ºå¤±å€¼...\")\n",
    "key_columns = ['Price', 'lon', 'lat', 'åŸå¸‚']\n",
    "for col in key_columns:\n",
    "    if col in df_clean.columns:\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=[col])\n",
    "        after = len(df_clean)\n",
    "        if before != after:\n",
    "            print(f\"  åˆ é™¤{col}ç¼ºå¤±å€¼: {before} -> {after}\")\n",
    "\n",
    "# 4. å¤„ç†ä»·æ ¼å¼‚å¸¸å€¼\n",
    "print(f\"\\nå¤„ç†ä»·æ ¼å¼‚å¸¸å€¼...\")\n",
    "Q1 = df_clean['Price'].quantile(0.05)\n",
    "Q3 = df_clean['Price'].quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df_clean[(df_clean['Price'] < lower_bound) | (df_clean['Price'] > upper_bound)]\n",
    "print(f\"ä»·æ ¼å¼‚å¸¸å€¼æ•°é‡: {len(outliers)}\")\n",
    "\n",
    "# ä¿å­˜åŸå§‹ä»·æ ¼\n",
    "df_clean['Price_original'] = df_clean['Price']\n",
    "df_clean['Price'] = df_clean['Price'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(f\"æ¸…æ´—åæ•°æ®å½¢çŠ¶: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "332012f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ•°æ®åˆ’åˆ† ===\n",
      "è®­ç»ƒé›†: (83096, 8)\n",
      "æµ‹è¯•é›†: (20775, 8)\n",
      "è®­ç»ƒé›†ä»·æ ¼å‡å€¼: 2236911.83\n",
      "æµ‹è¯•é›†ä»·æ ¼å‡å€¼: 2201642.17\n",
      "è®­ç»ƒé›†DataFrame: (83096, 48)\n",
      "æµ‹è¯•é›†DataFrame: (20775, 48)\n"
     ]
    }
   ],
   "source": [
    "# === æå‰åˆ’åˆ†æ•°æ®ï¼š80%è®­ç»ƒï¼Œ20%æµ‹è¯• ===\n",
    "print(\"\\n=== æ•°æ®åˆ’åˆ† ===\")\n",
    "\n",
    "# é€‰æ‹©åŸºç¡€ç‰¹å¾ç”¨äºåˆå§‹åˆ’åˆ†\n",
    "base_features = ['åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'Price', 'å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯', 'lon', 'lat', 'å¹´ä»½']\n",
    "available_features = [col for col in base_features if col in df_clean.columns]\n",
    "\n",
    "X_base = df_clean[available_features].drop('Price', axis=1)\n",
    "y = df_clean['Price']\n",
    "\n",
    "# æŒ‰ç…§è¦æ±‚åˆ’åˆ†ï¼š80%è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼Œrandom_state=111\n",
    "X_train_base, X_test_base, y_train, y_test = train_test_split(\n",
    "    X_base, y, test_size=0.2, random_state=111, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {X_train_base.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test_base.shape}\")\n",
    "print(f\"è®­ç»ƒé›†ä»·æ ¼å‡å€¼: {y_train.mean():.2f}\")\n",
    "print(f\"æµ‹è¯•é›†ä»·æ ¼å‡å€¼: {y_test.mean():.2f}\")\n",
    "\n",
    "# åˆ›å»ºå®Œæ•´çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†DataFrame\n",
    "train_indices = X_train_base.index\n",
    "test_indices = X_test_base.index\n",
    "\n",
    "df_train = df_clean.loc[train_indices].copy()\n",
    "df_test = df_clean.loc[test_indices].copy()\n",
    "\n",
    "print(f\"è®­ç»ƒé›†DataFrame: {df_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†DataFrame: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41501802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ•°æ®ç±»å‹æ£€æŸ¥ ===\n",
      "å»ºç­‘é¢ç§¯: object\n",
      "  æ ·æœ¬å€¼: ['58.99ã¡', '143.13ã¡', '60.8ã¡']\n",
      "  å”¯ä¸€å€¼æ•°é‡: 14840\n",
      "  ç¼ºå¤±å€¼: 0\n",
      "---\n",
      "å¥—å†…é¢ç§¯: object\n",
      "  æ ·æœ¬å€¼: [nan, nan, nan]\n",
      "  å”¯ä¸€å€¼æ•°é‡: 9183\n",
      "  ç¼ºå¤±å€¼: 54355\n",
      "---\n",
      "lon: float64\n",
      "  æ ·æœ¬å€¼: [117.74178820804342, 121.64464606677905, 117.41903342958464]\n",
      "  å”¯ä¸€å€¼æ•°é‡: 83096\n",
      "  ç¼ºå¤±å€¼: 0\n",
      "---\n",
      "lat: float64\n",
      "  æ ·æœ¬å€¼: [40.527387835143415, 32.27751007320103, 40.98489206418464]\n",
      "  å”¯ä¸€å€¼æ•°é‡: 83096\n",
      "  ç¼ºå¤±å€¼: 0\n",
      "---\n",
      "å¹´ä»½: float64\n",
      "  æ ·æœ¬å€¼: [2021.0, 2021.0, 2021.0]\n",
      "  å”¯ä¸€å€¼æ•°é‡: 8\n",
      "  ç¼ºå¤±å€¼: 0\n",
      "---\n",
      "æ£€æŸ¥æ•°æ®ç±»å‹é—®é¢˜...\n",
      "âš ï¸  å»ºç­‘é¢ç§¯ æœ‰ 83096 ä¸ªå€¼æ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹\n",
      "   é—®é¢˜æ ·æœ¬: ['58.99ã¡', '143.13ã¡', '60.8ã¡']\n",
      "âš ï¸  å¥—å†…é¢ç§¯ æœ‰ 28741 ä¸ªå€¼æ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹\n",
      "   é—®é¢˜æ ·æœ¬: ['66.16ã¡', '73.61ã¡', '78.35ã¡']\n"
     ]
    }
   ],
   "source": [
    "# === æ•°æ®ç±»å‹æ£€æŸ¥ ===\n",
    "print(\"=== æ•°æ®ç±»å‹æ£€æŸ¥ ===\")\n",
    "\n",
    "# æ£€æŸ¥å…³é”®æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹\n",
    "numeric_columns_to_check = ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯', 'lon', 'lat', 'å¹´ä»½']\n",
    "for col in numeric_columns_to_check:\n",
    "    if col in df_train.columns:\n",
    "        print(f\"{col}: {df_train[col].dtype}\")\n",
    "        print(f\"  æ ·æœ¬å€¼: {df_train[col].head(3).tolist()}\")\n",
    "        print(f\"  å”¯ä¸€å€¼æ•°é‡: {df_train[col].nunique()}\")\n",
    "        print(f\"  ç¼ºå¤±å€¼: {df_train[col].isnull().sum()}\")\n",
    "        print(\"---\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆæ•°æ®ç±»å‹\n",
    "print(\"æ£€æŸ¥æ•°æ®ç±»å‹é—®é¢˜...\")\n",
    "for col in numeric_columns_to_check:\n",
    "    if col in df_train.columns:\n",
    "        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹ï¼ŒæŸ¥çœ‹æœ‰å¤šå°‘æ— æ³•è½¬æ¢\n",
    "        converted = pd.to_numeric(df_train[col], errors='coerce')\n",
    "        failed_count = converted.isnull().sum() - df_train[col].isnull().sum()\n",
    "        if failed_count > 0:\n",
    "            print(f\"âš ï¸  {col} æœ‰ {failed_count} ä¸ªå€¼æ— æ³•è½¬æ¢ä¸ºæ•°å€¼å‹\")\n",
    "            # æ˜¾ç¤ºä¸€äº›æ— æ³•è½¬æ¢çš„æ ·æœ¬\n",
    "            problematic = df_train[col][pd.to_numeric(df_train[col], errors='coerce').isnull() & df_train[col].notnull()]\n",
    "            if len(problematic) > 0:\n",
    "                print(f\"   é—®é¢˜æ ·æœ¬: {problematic.head(3).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df5e637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ•°æ®æ¸…æ´—ï¼šå¤„ç†å­—ç¬¦ä¸²æ•°å€¼ ===\n",
      "æ¸…æ´—åˆ—: å»ºç­‘é¢ç§¯\n",
      "  åŸå§‹éç©ºå€¼: 83096\n",
      "  æ¸…ç†åéç©ºå€¼: 83096\n",
      "  æ¸…ç†åç©ºå€¼: 0\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    58.99ã¡ -> 58.99\n",
      "    143.13ã¡ -> 143.13\n",
      "    60.8ã¡ -> 60.8\n",
      "æ¸…æ´—åˆ—: å¥—å†…é¢ç§¯\n",
      "  åŸå§‹éç©ºå€¼: 28741\n",
      "  æ¸…ç†åéç©ºå€¼: 28741\n",
      "  æ¸…ç†åç©ºå€¼: 54355\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "æ¸…æ´—åˆ—: ç»¿ åŒ– ç‡\n",
      "  åŸå§‹éç©ºå€¼: 56816\n",
      "  æ¸…ç†åéç©ºå€¼: 56816\n",
      "  æ¸…ç†åç©ºå€¼: 26280\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    56.6% -> 56.6\n",
      "    10% -> 10.0\n",
      "æ¸…æ´—åˆ—: å®¹ ç§¯ ç‡\n",
      "  åŸå§‹éç©ºå€¼: 56623\n",
      "  æ¸…ç†åéç©ºå€¼: 56623\n",
      "  æ¸…ç†åç©ºå€¼: 26473\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    2.55 -> 2.55\n",
      "    3.8 -> 3.8\n",
      "æ¸…æ´—åˆ—: ç‰© ä¸š è´¹\n",
      "  åŸå§‹éç©ºå€¼: 58226\n",
      "  æ¸…ç†åéç©ºå€¼: 0\n",
      "  æ¸…ç†åç©ºå€¼: 83096\n",
      "  è½¬æ¢æˆåŠŸç‡: 0.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    0.6å…ƒ/æœˆ/ã¡ -> nan\n",
      "    1.6-2.19å…ƒ/æœˆ/ã¡ -> nan\n",
      "æ¸…æ´—åˆ—: å»ºç­‘é¢ç§¯\n",
      "  åŸå§‹éç©ºå€¼: 20775\n",
      "  æ¸…ç†åéç©ºå€¼: 20775\n",
      "  æ¸…ç†åç©ºå€¼: 0\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    127.83ã¡ -> 127.83\n",
      "    84.03ã¡ -> 84.03\n",
      "    135.21ã¡ -> 135.21\n",
      "æ¸…æ´—åˆ—: å¥—å†…é¢ç§¯\n",
      "  åŸå§‹éç©ºå€¼: 7243\n",
      "  æ¸…ç†åéç©ºå€¼: 7243\n",
      "  æ¸…ç†åç©ºå€¼: 13532\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    67.24ã¡ -> 67.24\n",
      "    109.09ã¡ -> 109.09\n",
      "æ¸…æ´—åˆ—: ç»¿ åŒ– ç‡\n",
      "  åŸå§‹éç©ºå€¼: 14172\n",
      "  æ¸…ç†åéç©ºå€¼: 14172\n",
      "  æ¸…ç†åç©ºå€¼: 6603\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    10% -> 10.0\n",
      "    30% -> 30.0\n",
      "    35% -> 35.0\n",
      "æ¸…æ´—åˆ—: å®¹ ç§¯ ç‡\n",
      "  åŸå§‹éç©ºå€¼: 14094\n",
      "  æ¸…ç†åéç©ºå€¼: 14094\n",
      "  æ¸…ç†åç©ºå€¼: 6681\n",
      "  è½¬æ¢æˆåŠŸç‡: 100.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    9.7 -> 9.7\n",
      "    3.0 -> 3.0\n",
      "    4.2 -> 4.2\n",
      "æ¸…æ´—åˆ—: ç‰© ä¸š è´¹\n",
      "  åŸå§‹éç©ºå€¼: 14487\n",
      "  æ¸…ç†åéç©ºå€¼: 0\n",
      "  æ¸…ç†åç©ºå€¼: 20775\n",
      "  è½¬æ¢æˆåŠŸç‡: 0.0%\n",
      "  æ ·æœ¬å¯¹æ¯”:\n",
      "    2å…ƒ/æœˆ/ã¡ -> nan\n",
      "    1-1.8å…ƒ/æœˆ/ã¡ -> nan\n",
      "    2.8å…ƒ/æœˆ/ã¡ -> nan\n",
      "è®­ç»ƒé›†æ¸…æ´—å®Œæˆï¼Œå½¢çŠ¶: (83096, 48)\n",
      "æµ‹è¯•é›†æ¸…æ´—å®Œæˆï¼Œå½¢çŠ¶: (20775, 48)\n"
     ]
    }
   ],
   "source": [
    "# === æ•°æ®æ¸…æ´—å‡½æ•°ï¼šå¤„ç†å­—ç¬¦ä¸²æ•°å€¼ ===\n",
    "def clean_numeric_columns(df):\n",
    "    \"\"\"\n",
    "    æ¸…æ´—æ•°å€¼åˆ—ï¼Œå¤„ç†åŒ…å«å•ä½çš„å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # å®šä¹‰éœ€è¦æ¸…æ´—çš„åˆ—å’Œå¯¹åº”çš„æ¨¡å¼\n",
    "    columns_to_clean = {\n",
    "        'å»ºç­‘é¢ç§¯': ['m', 'ã¡', 'å¹³æ–¹ç±³', 'å¹³ç±³', 'mÂ²'],\n",
    "        'å¥—å†…é¢ç§¯': ['m', 'ã¡', 'å¹³æ–¹ç±³', 'å¹³ç±³', 'mÂ²'],\n",
    "        'ç»¿ åŒ– ç‡': ['%'],\n",
    "        'å®¹ ç§¯ ç‡': [],\n",
    "        'ç‰© ä¸š è´¹': ['å…ƒ', 'å…ƒ/å¹³ç±³', 'å…ƒ/æœˆ']\n",
    "    }\n",
    "    \n",
    "    for col, units in columns_to_clean.items():\n",
    "        if col in df_clean.columns:\n",
    "            print(f\"æ¸…æ´—åˆ—: {col}\")\n",
    "            \n",
    "            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†\n",
    "            series_str = df_clean[col].astype(str)\n",
    "            \n",
    "            # ç§»é™¤å¸¸è§å•ä½\n",
    "            for unit in units:\n",
    "                series_str = series_str.str.replace(unit, '', regex=False)\n",
    "            \n",
    "            # ç§»é™¤å¤šä½™ç©ºæ ¼\n",
    "            series_str = series_str.str.strip()\n",
    "            \n",
    "            # å¤„ç†ç‰¹æ®Šå­—ç¬¦\n",
    "            series_str = series_str.str.replace(' ', '')  # ç§»é™¤ç©ºæ ¼\n",
    "            series_str = series_str.str.replace('ï¼Œ', '.')  # ä¸­æ–‡é€—å·è½¬ç‚¹\n",
    "            series_str = series_str.str.replace(',', '')   # ç§»é™¤åƒä½åˆ†éš”ç¬¦\n",
    "            \n",
    "            # è½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN\n",
    "            df_clean[col] = pd.to_numeric(series_str, errors='coerce')\n",
    "            \n",
    "            # ç»Ÿè®¡æ¸…ç†ç»“æœ\n",
    "            original_non_null = df[col].notnull().sum()\n",
    "            cleaned_non_null = df_clean[col].notnull().sum()\n",
    "            cleaned_null = df_clean[col].isnull().sum()\n",
    "            \n",
    "            print(f\"  åŸå§‹éç©ºå€¼: {original_non_null}\")\n",
    "            print(f\"  æ¸…ç†åéç©ºå€¼: {cleaned_non_null}\")\n",
    "            print(f\"  æ¸…ç†åç©ºå€¼: {cleaned_null}\")\n",
    "            if original_non_null > 0:\n",
    "                success_rate = cleaned_non_null / original_non_null * 100\n",
    "                print(f\"  è½¬æ¢æˆåŠŸç‡: {success_rate:.1f}%\")\n",
    "            \n",
    "            # æ˜¾ç¤ºä¸€äº›æ¸…ç†å‰åçš„æ ·æœ¬\n",
    "            print(f\"  æ ·æœ¬å¯¹æ¯”:\")\n",
    "            for i in range(min(3, len(df))):\n",
    "                if pd.notna(df[col].iloc[i]):\n",
    "                    print(f\"    {df[col].iloc[i]} -> {df_clean[col].iloc[i]}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# åº”ç”¨æ•°æ®æ¸…æ´—\n",
    "print(\"=== æ•°æ®æ¸…æ´—ï¼šå¤„ç†å­—ç¬¦ä¸²æ•°å€¼ ===\")\n",
    "df_train_clean = clean_numeric_columns(df_train)\n",
    "df_test_clean = clean_numeric_columns(df_test)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†æ¸…æ´—å®Œæˆï¼Œå½¢çŠ¶: {df_train_clean.shape}\")\n",
    "print(f\"æµ‹è¯•é›†æ¸…æ´—å®Œæˆï¼Œå½¢çŠ¶: {df_test_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05cad876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ä¿®å¤çš„ç‰¹å¾å·¥ç¨‹å‡½æ•° ===\n",
    "def create_features(df, is_training=True, encoders=None):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç‰¹å¾å‡½æ•° - ä¿®å¤ç‰ˆæœ¬\n",
    "    is_training: æ˜¯å¦æ˜¯è®­ç»ƒé›†\n",
    "    encoders: è®­ç»ƒé›†æ‹Ÿåˆçš„ç¼–ç å™¨ï¼ˆç”¨äºæµ‹è¯•é›†è½¬æ¢ï¼‰\n",
    "    \"\"\"\n",
    "    X = df.drop(['Price', 'Price_original'], axis=1)\n",
    "    \n",
    "    # é€‰æ‹©çš„ç‰¹å¾\n",
    "    important_numeric_features = [\n",
    "        'å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯', 'lon', 'lat', 'å¹´ä»½', 'æˆ¿å±‹æ€»æ•°', 'æ¥¼æ ‹æ€»æ•°',\n",
    "        'ç»¿ åŒ– ç‡', 'å®¹ ç§¯ ç‡', 'ç‰© ä¸š è´¹'\n",
    "    ]\n",
    "    \n",
    "    important_categorical_features = [\n",
    "        'åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚', 'æˆ¿å±‹æœå‘', \n",
    "        'è£…ä¿®æƒ…å†µ', 'å»ºç­‘ç»“æ„', 'é…å¤‡ç”µæ¢¯', 'äº¤æ˜“æƒå±', 'æˆ¿å±‹ç”¨é€”'\n",
    "    ]\n",
    "    \n",
    "    available_numeric = [col for col in important_numeric_features if col in X.columns]\n",
    "    available_categorical = [col for col in important_categorical_features if col in X.columns]\n",
    "    \n",
    "    selected_features = available_numeric + available_categorical\n",
    "    X = X[selected_features]\n",
    "    \n",
    "    # å¤„ç†åˆ†ç±»å˜é‡\n",
    "    new_encoders = {}\n",
    "    for col in available_categorical:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna('æœªçŸ¥')\n",
    "            \n",
    "            if is_training:\n",
    "                # è®­ç»ƒé›†ï¼šæ‹Ÿåˆç¼–ç å™¨\n",
    "                if X[col].nunique() > 20:\n",
    "                    top_categories = X[col].value_counts().head(15).index\n",
    "                    X[col] = X[col].apply(lambda x: x if x in top_categories else 'å…¶ä»–')\n",
    "                \n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                new_encoders[col] = le\n",
    "            else:\n",
    "                # æµ‹è¯•é›†ï¼šä½¿ç”¨è®­ç»ƒé›†çš„ç¼–ç å™¨\n",
    "                if col in encoders:\n",
    "                    le = encoders[col]\n",
    "                    # å¤„ç†æ–°ç±»åˆ«ï¼ˆåœ¨è®­ç»ƒé›†ä¸­æœªå‡ºç°çš„ç±»åˆ«ï¼‰\n",
    "                    unknown_categories = set(X[col].unique()) - set(le.classes_)\n",
    "                    if unknown_categories:\n",
    "                        X[col] = X[col].apply(lambda x: 'å…¶ä»–' if x in unknown_categories else x)\n",
    "                    try:\n",
    "                        X[col] = le.transform(X[col].astype(str))\n",
    "                    except:\n",
    "                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè®¾ä¸ºé»˜è®¤å€¼\n",
    "                        X[col] = 0\n",
    "    \n",
    "    # === ä¿®å¤ï¼šç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•°å€¼ç±»å‹ ===\n",
    "    print(\"ç¡®ä¿æ•°å€¼åˆ—æ•°æ®ç±»å‹æ­£ç¡®...\")\n",
    "    for col in available_numeric:\n",
    "        if col in X.columns:\n",
    "            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # å¤„ç†æ•°å€¼å‹ç¼ºå¤±å€¼\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if is_training:\n",
    "                median_val = X[col].median()\n",
    "            else:\n",
    "                # æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†è®¡ç®—çš„ä¸­ä½æ•°\n",
    "                median_val = encoders.get(f'median_{col}', X[col].median())\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "            if is_training:\n",
    "                new_encoders[f'median_{col}'] = median_val\n",
    "    \n",
    "    # === é«˜çº§ç‰¹å¾å·¥ç¨‹ ===\n",
    "    \n",
    "    # 1. ä»æˆ¿å±‹æˆ·å‹æå–æˆ¿é—´æ•°é‡\n",
    "    if 'æˆ¿å±‹æˆ·å‹' in df.columns:\n",
    "        print(\"  æå–æˆ¿å±‹æˆ·å‹ä¿¡æ¯...\")\n",
    "        house_layout = df['æˆ¿å±‹æˆ·å‹'].astype(str)\n",
    "        X['å§å®¤æ•°é‡'] = house_layout.str.extract(r'(\\d+)å®¤').fillna(0).astype(int)\n",
    "        X['å®¢å…æ•°é‡'] = house_layout.str.extract(r'(\\d+)å…').fillna(0).astype(int)\n",
    "        X['å«ç”Ÿé—´æ•°é‡'] = house_layout.str.extract(r'(\\d+)å«').fillna(0).astype(int)\n",
    "        X['æ€»æˆ¿é—´æ•°'] = X['å§å®¤æ•°é‡'] + X['å®¢å…æ•°é‡']\n",
    "        X['å§å®¤æ¯”ä¾‹'] = X['å§å®¤æ•°é‡'] / X['æ€»æˆ¿é—´æ•°'].replace(0, 1)\n",
    "    \n",
    "    # 2. æˆ¿å±‹æœå‘è¯„åˆ†\n",
    "    if 'æˆ¿å±‹æœå‘' in df.columns:\n",
    "        print(\"  åˆ›å»ºæœå‘è¯„åˆ†...\")\n",
    "        orientation_scores = {\n",
    "            'å—': 10, 'ä¸œå—': 9, 'è¥¿å—': 8, 'ä¸œ': 7, 'è¥¿': 6, \n",
    "            'ä¸œåŒ—': 5, 'è¥¿åŒ—': 4, 'åŒ—': 3, 'å—åŒ—': 9, 'ä¸œè¥¿': 6,\n",
    "            'ä¸œå—åŒ—': 7, 'è¥¿å—åŒ—': 6, 'æœªçŸ¥': 5\n",
    "        }\n",
    "        orientation = df['æˆ¿å±‹æœå‘'].fillna('æœªçŸ¥').astype(str)\n",
    "        X['æœå‘è¯„åˆ†'] = orientation.map(orientation_scores).fillna(5)\n",
    "        X['æ˜¯å¦å—å‘'] = orientation.str.contains('å—').fillna(0).astype(int)\n",
    "    \n",
    "    # 3. å»ºç­‘å¹´ä»£å’Œæˆ¿é¾„ç‰¹å¾\n",
    "    if 'å»ºç­‘å¹´ä»£' in df.columns:\n",
    "        print(\"  å¤„ç†å»ºç­‘å¹´ä»£...\")\n",
    "        year_extracted = df['å»ºç­‘å¹´ä»£'].astype(str).str.extract(r'(\\d{4})').astype(float)\n",
    "        current_year = 2023\n",
    "        X['æˆ¿é¾„'] = current_year - year_extracted[0]\n",
    "        X['æˆ¿é¾„'] = X['æˆ¿é¾„'].clip(0, 100)\n",
    "    \n",
    "    # 4. åœ°ç†åæ ‡ç‰¹å¾\n",
    "    if 'lon' in X.columns and 'lat' in X.columns:\n",
    "        print(\"  åˆ›å»ºåœ°ç†åæ ‡ç‰¹å¾...\")\n",
    "        if is_training:\n",
    "            center_lon, center_lat = X['lon'].median(), X['lat'].median()\n",
    "            new_encoders['center_lon'] = center_lon\n",
    "            new_encoders['center_lat'] = center_lat\n",
    "        else:\n",
    "            center_lon, center_lat = encoders['center_lon'], encoders['center_lat']\n",
    "        \n",
    "        X['è·å¸‚ä¸­å¿ƒè·ç¦»'] = np.sqrt((X['lon']-center_lon)**2 + (X['lat']-center_lat)**2)\n",
    "    \n",
    "    # 5. é¢ç§¯ç›¸å…³ç‰¹å¾ - ä¿®å¤ç‰ˆæœ¬\n",
    "    if 'å»ºç­‘é¢ç§¯' in X.columns:\n",
    "        print(\"  åˆ›å»ºé¢ç§¯ç›¸å…³ç‰¹å¾...\")\n",
    "        # ç¡®ä¿å»ºç­‘é¢ç§¯æ˜¯æ•°å€¼å‹\n",
    "        X['å»ºç­‘é¢ç§¯'] = pd.to_numeric(X['å»ºç­‘é¢ç§¯'], errors='coerce')\n",
    "        # å¡«å……ç¼ºå¤±å€¼\n",
    "        X['å»ºç­‘é¢ç§¯'] = X['å»ºç­‘é¢ç§¯'].fillna(X['å»ºç­‘é¢ç§¯'].median())\n",
    "        \n",
    "        # ä¿®å¤ï¼šbinså’Œlabelsæ•°é‡åŒ¹é…\n",
    "        # binsæœ‰8ä¸ªè¾¹ç•Œç‚¹ï¼Œä¼šäº§ç”Ÿ7ä¸ªåŒºé—´ï¼Œæ‰€ä»¥éœ€è¦7ä¸ªlabels\n",
    "        try:\n",
    "            X['é¢ç§¯åˆ†æ®µ'] = pd.cut(\n",
    "                X['å»ºç­‘é¢ç§¯'], \n",
    "                bins=[0, 50, 80, 100, 120, 150, 200, 300, float('inf')],\n",
    "                labels=['å°æˆ·å‹', 'ä¸­å°æˆ·å‹', 'ä¸­æˆ·å‹', 'ä¸­å¤§æˆ·å‹', 'å¤§æˆ·å‹', 'è¶…å¤§æˆ·å‹', 'è±ªå®…', 'åˆ«å¢…']\n",
    "            )\n",
    "            \n",
    "            if is_training:\n",
    "                le_size = LabelEncoder()\n",
    "                X['é¢ç§¯åˆ†æ®µç¼–ç '] = le_size.fit_transform(X['é¢ç§¯åˆ†æ®µ'].astype(str))\n",
    "                new_encoders['le_size'] = le_size\n",
    "            else:\n",
    "                le_size = encoders['le_size']\n",
    "                # å¤„ç†æµ‹è¯•é›†ä¸­å¯èƒ½çš„æ–°åˆ†æ®µ\n",
    "                unknown_segments = set(X['é¢ç§¯åˆ†æ®µ'].astype(str).unique()) - set(le_size.classes_)\n",
    "                if unknown_segments:\n",
    "                    X['é¢ç§¯åˆ†æ®µ'] = X['é¢ç§¯åˆ†æ®µ'].astype(str).apply(\n",
    "                        lambda x: 'ä¸­æˆ·å‹' if x in unknown_segments else x\n",
    "                    )\n",
    "                X['é¢ç§¯åˆ†æ®µç¼–ç '] = le_size.transform(X['é¢ç§¯åˆ†æ®µ'].astype(str))\n",
    "        except Exception as e:\n",
    "            print(f\"    é¢ç§¯åˆ†æ®µå¤±è´¥: {e}\")\n",
    "            # å¦‚æœåˆ†æ®µå¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„åˆ†æ®µ\n",
    "            X['é¢ç§¯åˆ†æ®µç¼–ç '] = pd.cut(X['å»ºç­‘é¢ç§¯'], bins=5, labels=False)\n",
    "    \n",
    "    # 6. äº¤äº’ç‰¹å¾\n",
    "    print(\"  åˆ›å»ºäº¤äº’ç‰¹å¾...\")\n",
    "    if 'å»ºç­‘é¢ç§¯' in X.columns and 'æœå‘è¯„åˆ†' in X.columns:\n",
    "        X['é¢ç§¯_æœå‘äº¤äº’'] = X['å»ºç­‘é¢ç§¯'] * X['æœå‘è¯„åˆ†']\n",
    "    \n",
    "    if 'å»ºç­‘é¢ç§¯' in X.columns and 'æˆ¿é¾„' in X.columns:\n",
    "        X['é¢ç§¯_æˆ¿é¾„äº¤äº’'] = X['å»ºç­‘é¢ç§¯'] * X['æˆ¿é¾„']\n",
    "    \n",
    "    if 'æ€»æˆ¿é—´æ•°' in X.columns and 'å»ºç­‘é¢ç§¯' in X.columns:\n",
    "        X['æˆ¿é—´æ•°_é¢ç§¯äº¤äº’'] = X['æ€»æˆ¿é—´æ•°'] * X['å»ºç­‘é¢ç§¯']\n",
    "    \n",
    "    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹ï¼ˆæœ€ç»ˆæ£€æŸ¥ï¼‰\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            try:\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "                X[col] = X[col].fillna(0)\n",
    "            except:\n",
    "                X = X.drop(col, axis=1)\n",
    "    \n",
    "    return X, new_encoders# === æ›´æ–°ç‰¹å¾å·¥ç¨‹å‡½æ•° ===\n",
    "def create_features_updated(df, is_training=True, encoders=None):\n",
    "    \"\"\"\n",
    "    æ›´æ–°ç‰ˆç‰¹å¾å·¥ç¨‹å‡½æ•° - å¤„ç†å­—ç¬¦ä¸²æ•°å€¼é—®é¢˜\n",
    "    \"\"\"\n",
    "    X = df.drop(['Price', 'Price_original'], axis=1)\n",
    "    \n",
    "    # é€‰æ‹©çš„ç‰¹å¾\n",
    "    important_numeric_features = [\n",
    "        'å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯', 'lon', 'lat', 'å¹´ä»½', 'æˆ¿å±‹æ€»æ•°', 'æ¥¼æ ‹æ€»æ•°',\n",
    "        'ç»¿ åŒ– ç‡', 'å®¹ ç§¯ ç‡', 'ç‰© ä¸š è´¹'\n",
    "    ]\n",
    "    \n",
    "    important_categorical_features = [\n",
    "        'åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚', 'æˆ¿å±‹æœå‘', \n",
    "        'è£…ä¿®æƒ…å†µ', 'å»ºç­‘ç»“æ„', 'é…å¤‡ç”µæ¢¯', 'äº¤æ˜“æƒå±', 'æˆ¿å±‹ç”¨é€”'\n",
    "    ]\n",
    "    \n",
    "    available_numeric = [col for col in important_numeric_features if col in X.columns]\n",
    "    available_categorical = [col for col in important_categorical_features if col in X.columns]\n",
    "    \n",
    "    selected_features = available_numeric + available_categorical\n",
    "    X = X[selected_features]\n",
    "    \n",
    "    # å¤„ç†åˆ†ç±»å˜é‡\n",
    "    new_encoders = {}\n",
    "    for col in available_categorical:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna('æœªçŸ¥')\n",
    "            \n",
    "            if is_training:\n",
    "                if X[col].nunique() > 20:\n",
    "                    top_categories = X[col].value_counts().head(15).index\n",
    "                    X[col] = X[col].apply(lambda x: x if x in top_categories else 'å…¶ä»–')\n",
    "                \n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                new_encoders[col] = le\n",
    "            else:\n",
    "                if col in encoders:\n",
    "                    le = encoders[col]\n",
    "                    unknown_categories = set(X[col].unique()) - set(le.classes_)\n",
    "                    if unknown_categories:\n",
    "                        X[col] = X[col].apply(lambda x: 'å…¶ä»–' if x in unknown_categories else x)\n",
    "                    try:\n",
    "                        X[col] = le.transform(X[col].astype(str))\n",
    "                    except:\n",
    "                        X[col] = 0\n",
    "    \n",
    "    # === æ•°å€¼åˆ—å¤„ç†ï¼šä½¿ç”¨æ¸…æ´—åçš„æ•°æ® ===\n",
    "    print(\"å¤„ç†æ•°å€¼åˆ—...\")\n",
    "    for col in available_numeric:\n",
    "        if col in X.columns:\n",
    "            # ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹ï¼ˆä½¿ç”¨æ¸…æ´—åçš„æ•°æ®ï¼‰\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            \n",
    "            # å¤„ç†ç¼ºå¤±å€¼\n",
    "            if X[col].isnull().sum() > 0:\n",
    "                if is_training:\n",
    "                    median_val = X[col].median()\n",
    "                else:\n",
    "                    median_val = encoders.get(f'median_{col}', X[col].median())\n",
    "                X[col] = X[col].fillna(median_val)\n",
    "                if is_training:\n",
    "                    new_encoders[f'median_{col}'] = median_val\n",
    "            \n",
    "            print(f\"  {col}: èŒƒå›´[{X[col].min():.2f}, {X[col].max():.2f}], ç¼ºå¤±å€¼:{X[col].isnull().sum()}\")\n",
    "    \n",
    "    # === é«˜çº§ç‰¹å¾å·¥ç¨‹ ===\n",
    "    \n",
    "    # 1. ä»æˆ¿å±‹æˆ·å‹æå–æˆ¿é—´æ•°é‡\n",
    "    if 'æˆ¿å±‹æˆ·å‹' in df.columns:\n",
    "        print(\"æå–æˆ¿å±‹æˆ·å‹ä¿¡æ¯...\")\n",
    "        house_layout = df['æˆ¿å±‹æˆ·å‹'].astype(str)\n",
    "        X['å§å®¤æ•°é‡'] = house_layout.str.extract(r'(\\d+)å®¤').fillna(0).astype(int)\n",
    "        X['å®¢å…æ•°é‡'] = house_layout.str.extract(r'(\\d+)å…').fillna(0).astype(int)\n",
    "        X['å«ç”Ÿé—´æ•°é‡'] = house_layout.str.extract(r'(\\d+)å«').fillna(0).astype(int)\n",
    "        X['æ€»æˆ¿é—´æ•°'] = X['å§å®¤æ•°é‡'] + X['å®¢å…æ•°é‡']\n",
    "        X['å§å®¤æ¯”ä¾‹'] = X['å§å®¤æ•°é‡'] / X['æ€»æˆ¿é—´æ•°'].replace(0, 1)\n",
    "    \n",
    "    # 2. æˆ¿å±‹æœå‘è¯„åˆ†\n",
    "    if 'æˆ¿å±‹æœå‘' in df.columns:\n",
    "        print(\"åˆ›å»ºæœå‘è¯„åˆ†...\")\n",
    "        orientation_scores = {\n",
    "            'å—': 10, 'ä¸œå—': 9, 'è¥¿å—': 8, 'ä¸œ': 7, 'è¥¿': 6, \n",
    "            'ä¸œåŒ—': 5, 'è¥¿åŒ—': 4, 'åŒ—': 3, 'å—åŒ—': 9, 'ä¸œè¥¿': 6,\n",
    "            'ä¸œå—åŒ—': 7, 'è¥¿å—åŒ—': 6, 'æœªçŸ¥': 5\n",
    "        }\n",
    "        orientation = df['æˆ¿å±‹æœå‘'].fillna('æœªçŸ¥').astype(str)\n",
    "        X['æœå‘è¯„åˆ†'] = orientation.map(orientation_scores).fillna(5)\n",
    "        X['æ˜¯å¦å—å‘'] = orientation.str.contains('å—').fillna(0).astype(int)\n",
    "    \n",
    "    # 3. å»ºç­‘å¹´ä»£å’Œæˆ¿é¾„ç‰¹å¾\n",
    "    if 'å»ºç­‘å¹´ä»£' in df.columns:\n",
    "        print(\"å¤„ç†å»ºç­‘å¹´ä»£...\")\n",
    "        year_extracted = df['å»ºç­‘å¹´ä»£'].astype(str).str.extract(r'(\\d{4})').astype(float)\n",
    "        current_year = 2023\n",
    "        X['æˆ¿é¾„'] = current_year - year_extracted[0]\n",
    "        X['æˆ¿é¾„'] = X['æˆ¿é¾„'].clip(0, 100)\n",
    "    \n",
    "    # 4. åœ°ç†åæ ‡ç‰¹å¾\n",
    "    if 'lon' in X.columns and 'lat' in X.columns:\n",
    "        print(\"åˆ›å»ºåœ°ç†åæ ‡ç‰¹å¾...\")\n",
    "        if is_training:\n",
    "            center_lon, center_lat = X['lon'].median(), X['lat'].median()\n",
    "            new_encoders['center_lon'] = center_lon\n",
    "            new_encoders['center_lat'] = center_lat\n",
    "        else:\n",
    "            center_lon, center_lat = encoders['center_lon'], encoders['center_lat']\n",
    "        \n",
    "        X['è·å¸‚ä¸­å¿ƒè·ç¦»'] = np.sqrt((X['lon']-center_lon)**2 + (X['lat']-center_lat)**2)\n",
    "    \n",
    "    # 5. é¢ç§¯ç›¸å…³ç‰¹å¾ - å®‰å…¨ç‰ˆæœ¬\n",
    "    if 'å»ºç­‘é¢ç§¯' in X.columns:\n",
    "        print(\"åˆ›å»ºé¢ç§¯ç›¸å…³ç‰¹å¾...\")\n",
    "        \n",
    "        # ä½¿ç”¨å®‰å…¨çš„åˆ†æ®µå‡½æ•°\n",
    "        def safe_area_binning(area_series):\n",
    "            \"\"\"å®‰å…¨çš„é¢ç§¯åˆ†æ®µå‡½æ•°\"\"\"\n",
    "            try:\n",
    "                # ç¡®ä¿æ˜¯æ•°å€¼å‹\n",
    "                area_numeric = pd.to_numeric(area_series, errors='coerce')\n",
    "                area_numeric = area_numeric.fillna(area_numeric.median())\n",
    "                \n",
    "                # å®šä¹‰åˆ†æ®µ\n",
    "                bins = [0, 50, 80, 100, 120, 150, 200, 300, float('inf')]\n",
    "                labels = ['å°æˆ·å‹', 'ä¸­å°æˆ·å‹', 'ä¸­æˆ·å‹', 'ä¸­å¤§æˆ·å‹', 'å¤§æˆ·å‹', 'è¶…å¤§æˆ·å‹', 'è±ªå®…', 'åˆ«å¢…']\n",
    "                \n",
    "                return pd.cut(area_numeric, bins=bins, labels=labels, duplicates='drop')\n",
    "            except Exception as e:\n",
    "                print(f\"é¢ç§¯åˆ†æ®µå¤±è´¥: {e}\")\n",
    "                # ç®€å•åˆ†æ®µä½œä¸ºå¤‡é€‰\n",
    "                return (area_numeric // 50).astype(int)\n",
    "        \n",
    "        X['é¢ç§¯åˆ†æ®µ'] = safe_area_binning(X['å»ºç­‘é¢ç§¯'])\n",
    "        \n",
    "        if is_training:\n",
    "            le_size = LabelEncoder()\n",
    "            X['é¢ç§¯åˆ†æ®µç¼–ç '] = le_size.fit_transform(X['é¢ç§¯åˆ†æ®µ'].astype(str))\n",
    "            new_encoders['le_size'] = le_size\n",
    "        else:\n",
    "            if 'le_size' in encoders:\n",
    "                le_size = encoders['le_size']\n",
    "                try:\n",
    "                    X['é¢ç§¯åˆ†æ®µç¼–ç '] = le_size.transform(X['é¢ç§¯åˆ†æ®µ'].astype(str))\n",
    "                except:\n",
    "                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼\n",
    "                    X['é¢ç§¯åˆ†æ®µç¼–ç '] = 2  # é»˜è®¤ä¸­æˆ·å‹\n",
    "    \n",
    "    # 6. äº¤äº’ç‰¹å¾\n",
    "    print(\"åˆ›å»ºäº¤äº’ç‰¹å¾...\")\n",
    "    if 'å»ºç­‘é¢ç§¯' in X.columns and 'æœå‘è¯„åˆ†' in X.columns:\n",
    "        X['é¢ç§¯_æœå‘äº¤äº’'] = X['å»ºç­‘é¢ç§¯'] * X['æœå‘è¯„åˆ†']\n",
    "    \n",
    "    if 'å»ºç­‘é¢ç§¯' in X.columns and 'æˆ¿é¾„' in X.columns:\n",
    "        X['é¢ç§¯_æˆ¿é¾„äº¤äº’'] = X['å»ºç­‘é¢ç§¯'] * X['æˆ¿é¾„']\n",
    "    \n",
    "    if 'æ€»æˆ¿é—´æ•°' in X.columns and 'å»ºç­‘é¢ç§¯' in X.columns:\n",
    "        X['æˆ¿é—´æ•°_é¢ç§¯äº¤äº’'] = X['æ€»æˆ¿é—´æ•°'] * X['å»ºç­‘é¢ç§¯']\n",
    "    \n",
    "    # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼å‹\n",
    "    print(\"æœ€ç»ˆæ•°æ®ç±»å‹æ£€æŸ¥...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            try:\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "                X[col] = X[col].fillna(0)\n",
    "                print(f\"  è½¬æ¢ {col} ä¸ºæ•°å€¼å‹\")\n",
    "            except:\n",
    "                print(f\"  åˆ é™¤éæ•°å€¼åˆ—: {col}\")\n",
    "                X = X.drop(col, axis=1)\n",
    "    \n",
    "    print(f\"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: {X.shape[1]}\")\n",
    "    return X, new_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d57e2a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== é‡æ–°è¿è¡Œç‰¹å¾å·¥ç¨‹ ===\n",
      "åœ¨è®­ç»ƒé›†ä¸Šåˆ›å»ºç‰¹å¾...\n",
      "å¤„ç†æ•°å€¼åˆ—...\n",
      "  å»ºç­‘é¢ç§¯: èŒƒå›´[11.70, 508.11], ç¼ºå¤±å€¼:0\n",
      "  å¥—å†…é¢ç§¯: èŒƒå›´[1.00, 501.74], ç¼ºå¤±å€¼:0\n",
      "  lon: èŒƒå›´[103.51, 122.97], ç¼ºå¤±å€¼:0\n",
      "  lat: èŒƒå›´[23.03, 42.19], ç¼ºå¤±å€¼:0\n",
      "  å¹´ä»½: èŒƒå›´[2015.00, 2022.00], ç¼ºå¤±å€¼:0\n",
      "  æˆ¿å±‹æ€»æ•°: èŒƒå›´[nan, nan], ç¼ºå¤±å€¼:83096\n",
      "  æ¥¼æ ‹æ€»æ•°: èŒƒå›´[nan, nan], ç¼ºå¤±å€¼:83096\n",
      "  ç»¿ åŒ– ç‡: èŒƒå›´[0.01, 10500.00], ç¼ºå¤±å€¼:0\n",
      "  å®¹ ç§¯ ç‡: èŒƒå›´[0.02, 30.00], ç¼ºå¤±å€¼:0\n",
      "  ç‰© ä¸š è´¹: èŒƒå›´[nan, nan], ç¼ºå¤±å€¼:83096\n",
      "æå–æˆ¿å±‹æˆ·å‹ä¿¡æ¯...\n",
      "åˆ›å»ºæœå‘è¯„åˆ†...\n",
      "å¤„ç†å»ºç­‘å¹´ä»£...\n",
      "åˆ›å»ºåœ°ç†åæ ‡ç‰¹å¾...\n",
      "åˆ›å»ºé¢ç§¯ç›¸å…³ç‰¹å¾...\n",
      "åˆ›å»ºäº¤äº’ç‰¹å¾...\n",
      "æœ€ç»ˆæ•°æ®ç±»å‹æ£€æŸ¥...\n",
      "ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: 35\n",
      "è®­ç»ƒé›†ç‰¹å¾å·¥ç¨‹å®Œæˆ! ç‰¹å¾æ•°é‡: 35\n",
      "è®­ç»ƒé›†ç‰¹å¾åˆ—: ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯', 'lon', 'lat', 'å¹´ä»½', 'æˆ¿å±‹æ€»æ•°', 'æ¥¼æ ‹æ€»æ•°', 'ç»¿ åŒ– ç‡', 'å®¹ ç§¯ ç‡', 'ç‰© ä¸š è´¹', 'åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'æˆ¿å±‹æˆ·å‹', 'æ‰€åœ¨æ¥¼å±‚', 'æˆ¿å±‹æœå‘', 'è£…ä¿®æƒ…å†µ', 'å»ºç­‘ç»“æ„', 'é…å¤‡ç”µæ¢¯', 'äº¤æ˜“æƒå±', 'æˆ¿å±‹ç”¨é€”', 'å§å®¤æ•°é‡', 'å®¢å…æ•°é‡', 'å«ç”Ÿé—´æ•°é‡', 'æ€»æˆ¿é—´æ•°', 'å§å®¤æ¯”ä¾‹', 'æœå‘è¯„åˆ†', 'æ˜¯å¦å—å‘', 'æˆ¿é¾„', 'è·å¸‚ä¸­å¿ƒè·ç¦»', 'é¢ç§¯åˆ†æ®µ', 'é¢ç§¯åˆ†æ®µç¼–ç ', 'é¢ç§¯_æœå‘äº¤äº’', 'é¢ç§¯_æˆ¿é¾„äº¤äº’', 'æˆ¿é—´æ•°_é¢ç§¯äº¤äº’']\n",
      "\n",
      "=== æµ‹è¯•é›†ç‰¹å¾å·¥ç¨‹ ===\n",
      "åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨ç›¸åŒçš„ç‰¹å¾è½¬æ¢...\n",
      "å¤„ç†æ•°å€¼åˆ—...\n",
      "  å»ºç­‘é¢ç§¯: èŒƒå›´[11.70, 504.89], ç¼ºå¤±å€¼:0\n",
      "  å¥—å†…é¢ç§¯: èŒƒå›´[1.00, 501.74], ç¼ºå¤±å€¼:0\n",
      "  lon: èŒƒå›´[103.51, 122.96], ç¼ºå¤±å€¼:0\n",
      "  lat: èŒƒå›´[23.03, 42.19], ç¼ºå¤±å€¼:0\n",
      "  å¹´ä»½: èŒƒå›´[2016.00, 2022.00], ç¼ºå¤±å€¼:0\n",
      "  æˆ¿å±‹æ€»æ•°: èŒƒå›´[nan, nan], ç¼ºå¤±å€¼:20775\n",
      "  æ¥¼æ ‹æ€»æ•°: èŒƒå›´[nan, nan], ç¼ºå¤±å€¼:20775\n",
      "  ç»¿ åŒ– ç‡: èŒƒå›´[0.01, 10500.00], ç¼ºå¤±å€¼:0\n",
      "  å®¹ ç§¯ ç‡: èŒƒå›´[0.18, 18.80], ç¼ºå¤±å€¼:0\n",
      "  ç‰© ä¸š è´¹: èŒƒå›´[nan, nan], ç¼ºå¤±å€¼:20775\n",
      "æå–æˆ¿å±‹æˆ·å‹ä¿¡æ¯...\n",
      "åˆ›å»ºæœå‘è¯„åˆ†...\n",
      "å¤„ç†å»ºç­‘å¹´ä»£...\n",
      "åˆ›å»ºåœ°ç†åæ ‡ç‰¹å¾...\n",
      "åˆ›å»ºé¢ç§¯ç›¸å…³ç‰¹å¾...\n",
      "åˆ›å»ºäº¤äº’ç‰¹å¾...\n",
      "æœ€ç»ˆæ•°æ®ç±»å‹æ£€æŸ¥...\n",
      "ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: 35\n",
      "æµ‹è¯•é›†ç‰¹å¾å·¥ç¨‹å®Œæˆ! ç‰¹å¾æ•°é‡: 35\n",
      "å…±åŒç‰¹å¾æ•°é‡: 35\n",
      "è®­ç»ƒé›†æœ€ç»ˆå½¢çŠ¶: (83096, 35)\n",
      "æµ‹è¯•é›†æœ€ç»ˆå½¢çŠ¶: (20775, 35)\n",
      "\n",
      "=== å˜é‡ç¡®è®¤ ===\n",
      "X_train ç±»å‹: <class 'pandas.core.frame.DataFrame'>\n",
      "X_test ç±»å‹: <class 'pandas.core.frame.DataFrame'>\n",
      "y_train ç±»å‹: <class 'pandas.core.series.Series'>\n",
      "y_test ç±»å‹: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# === é‡æ–°è¿è¡Œç‰¹å¾å·¥ç¨‹å¹¶ä¿å­˜å˜é‡ ===\n",
    "print(\"=== é‡æ–°è¿è¡Œç‰¹å¾å·¥ç¨‹ ===\")\n",
    "\n",
    "# åœ¨è®­ç»ƒé›†ä¸Šåˆ›å»ºç‰¹å¾å¹¶æ‹Ÿåˆç¼–ç å™¨\n",
    "print(\"åœ¨è®­ç»ƒé›†ä¸Šåˆ›å»ºç‰¹å¾...\")\n",
    "X_train, encoders = create_features_updated(df_train_clean, is_training=True)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†ç‰¹å¾å·¥ç¨‹å®Œæˆ! ç‰¹å¾æ•°é‡: {X_train.shape[1]}\")\n",
    "print(f\"è®­ç»ƒé›†ç‰¹å¾åˆ—: {X_train.columns.tolist()}\")\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨ç‰¹å¾å·¥ç¨‹\n",
    "print(\"\\n=== æµ‹è¯•é›†ç‰¹å¾å·¥ç¨‹ ===\")\n",
    "print(\"åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨ç›¸åŒçš„ç‰¹å¾è½¬æ¢...\")\n",
    "X_test, _ = create_features_updated(df_test_clean, is_training=False, encoders=encoders)\n",
    "\n",
    "print(f\"æµ‹è¯•é›†ç‰¹å¾å·¥ç¨‹å®Œæˆ! ç‰¹å¾æ•°é‡: {X_test.shape[1]}\")\n",
    "\n",
    "# ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†æœ‰ç›¸åŒçš„ç‰¹å¾\n",
    "common_features = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train[common_features]\n",
    "X_test = X_test[common_features]\n",
    "\n",
    "print(f\"å…±åŒç‰¹å¾æ•°é‡: {len(common_features)}\")\n",
    "print(f\"è®­ç»ƒé›†æœ€ç»ˆå½¢çŠ¶: {X_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†æœ€ç»ˆå½¢çŠ¶: {X_test.shape}\")\n",
    "\n",
    "# ç«‹å³æ£€æŸ¥å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "print(\"\\n=== å˜é‡ç¡®è®¤ ===\")\n",
    "print(f\"X_train ç±»å‹: {type(X_train)}\")\n",
    "print(f\"X_test ç±»å‹: {type(X_test)}\")\n",
    "print(f\"y_train ç±»å‹: {type(y_train)}\")\n",
    "print(f\"y_test ç±»å‹: {type(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6261686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹ ===\n",
      "æ•°æ®ç±»å‹æ£€æŸ¥:\n",
      "float64     17\n",
      "int32       17\n",
      "category     1\n",
      "Name: count, dtype: int64\n",
      "å‘ç°éæ•°å€¼åˆ—: ['é¢ç§¯åˆ†æ®µ']\n",
      "å°†è¿™äº›åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹...\n",
      "å¤„ç†åˆ—: é¢ç§¯åˆ†æ®µ\n",
      "éæ•°å€¼åˆ—è½¬æ¢å®Œæˆ!\n",
      "\n",
      "æœ€ç»ˆæ•°æ®ç±»å‹:\n",
      "int32      18\n",
      "float64    17\n",
      "Name: count, dtype: int64\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆ!\n"
     ]
    }
   ],
   "source": [
    "# === ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹ ===\n",
    "print(\"\\n=== ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹ ===\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®ç±»å‹\n",
    "print(\"æ•°æ®ç±»å‹æ£€æŸ¥:\")\n",
    "print(X_train.dtypes.value_counts())\n",
    "\n",
    "# æŸ¥æ‰¾éæ•°å€¼åˆ—\n",
    "non_numeric_cols = []\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object' or X_train[col].dtype.name == 'category':\n",
    "        non_numeric_cols.append(col)\n",
    "\n",
    "if non_numeric_cols:\n",
    "    print(f\"å‘ç°éæ•°å€¼åˆ—: {non_numeric_cols}\")\n",
    "    print(\"å°†è¿™äº›åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹...\")\n",
    "    \n",
    "    # å¯¹éæ•°å€¼åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç \n",
    "    for col in non_numeric_cols:\n",
    "        print(f\"å¤„ç†åˆ—: {col}\")\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    \n",
    "    print(\"éæ•°å€¼åˆ—è½¬æ¢å®Œæˆ!\")\n",
    "else:\n",
    "    print(\"æ‰€æœ‰åˆ—å·²ç»æ˜¯æ•°å€¼ç±»å‹!\")\n",
    "\n",
    "# æœ€ç»ˆæ£€æŸ¥\n",
    "print(f\"\\næœ€ç»ˆæ•°æ®ç±»å‹:\")\n",
    "print(X_train.dtypes.value_counts())\n",
    "\n",
    "# ç¡®ä¿æ²¡æœ‰NaNæˆ–æ— ç©·å€¼\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "X_train = X_train.replace([np.inf, -np.inf], 0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(\"æ•°æ®é¢„å¤„ç†å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a69a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ•°æ®æ ‡å‡†åŒ– ===\n",
      "æ•°æ®æ ‡å‡†åŒ–å®Œæˆ!\n",
      "è®­ç»ƒé›†æ ‡å‡†åŒ–åå½¢çŠ¶: (83096, 35)\n",
      "æµ‹è¯•é›†æ ‡å‡†åŒ–åå½¢çŠ¶: (20775, 35)\n"
     ]
    }
   ],
   "source": [
    "# === æ•°æ®æ ‡å‡†åŒ– ===\n",
    "print(\"\\n=== æ•°æ®æ ‡å‡†åŒ– ===\")\n",
    "\n",
    "# ä½¿ç”¨æ¸…æ´—åçš„ç‰¹å¾å·¥ç¨‹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"æ•°æ®æ ‡å‡†åŒ–å®Œæˆ!\")\n",
    "print(f\"è®­ç»ƒé›†æ ‡å‡†åŒ–åå½¢çŠ¶: {X_train_scaled.shape}\")\n",
    "print(f\"æµ‹è¯•é›†æ ‡å‡†åŒ–åå½¢çŠ¶: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c1e5618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== å¼€å§‹å»ºæ¨¡ï¼ˆä¸‰ä¸ªæ¨¡å‹ï¼‰===\n",
      "\n",
      "--- OLS ---\n",
      "  è®­ç»ƒé›† MAE: 1094631.01, RMSE: 1651333.22\n",
      "  æµ‹è¯•é›† MAE: 1315698.81, RMSE: 1779568.54\n",
      "  äº¤å‰éªŒè¯ MAE: 1095116.96\n",
      "\n",
      "--- LASSO ---\n",
      "  è®­ç»ƒé›† MAE: 1094678.41, RMSE: 1651332.94\n",
      "  æµ‹è¯•é›† MAE: 1315612.68, RMSE: 1779526.24\n",
      "  äº¤å‰éªŒè¯ MAE: 1095171.08\n",
      "\n",
      "--- Ridge ---\n",
      "  è®­ç»ƒé›† MAE: 1094677.99, RMSE: 1651332.94\n",
      "  æµ‹è¯•é›† MAE: 1315612.20, RMSE: 1779525.80\n",
      "  äº¤å‰éªŒè¯ MAE: 1095170.55\n"
     ]
    }
   ],
   "source": [
    "# === å»ºæ¨¡å’Œè¯„ä¼°ï¼ˆä¸‰ä¸ªæ¨¡å‹ï¼‰===\n",
    "print(\"\\n=== å¼€å§‹å»ºæ¨¡ï¼ˆä¸‰ä¸ªæ¨¡å‹ï¼‰===\")\n",
    "\n",
    "# åªä½¿ç”¨ä¸‰ä¸ªçº¿æ€§æ¨¡å‹\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'LASSO': Lasso(alpha=0.1, random_state=111, max_iter=1000),\n",
    "    'Ridge': Ridge(alpha=0.1, random_state=111)\n",
    "}\n",
    "\n",
    "# å­˜å‚¨ç»“æœ\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # è®¡ç®—MAEï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # è®¡ç®—RMSEï¼ˆæ¬¡è¦æŒ‡æ ‡ï¼‰\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    # 6æŠ˜äº¤å‰éªŒè¯\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_scaled, y_train, \n",
    "        cv=6, scoring='neg_mean_absolute_error', n_jobs=-1\n",
    "    )\n",
    "    cv_mae = -cv_scores.mean()\n",
    "    \n",
    "    # å­˜å‚¨ç»“æœ\n",
    "    results[name] = {\n",
    "        'In_sample_MAE': train_mae,\n",
    "        'Out_of_sample_MAE': test_mae,\n",
    "        'In_sample_RMSE': train_rmse,\n",
    "        'Out_of_sample_RMSE': test_rmse,\n",
    "        'Cross_validation_MAE': cv_mae,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"  è®­ç»ƒé›† MAE: {train_mae:.2f}, RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"  æµ‹è¯•é›† MAE: {test_mae:.2f}, RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"  äº¤å‰éªŒè¯ MAE: {cv_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb5a7dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ\n",
      "======================================================================\n",
      "è¯¦ç»†æ€§èƒ½æŒ‡æ ‡:\n",
      "       In_sample_MAE  Out_of_sample_MAE  Cross_validation_MAE\n",
      "OLS     1.094631e+06       1.315699e+06          1.095117e+06\n",
      "LASSO   1.094678e+06       1.315613e+06          1.095171e+06\n",
      "Ridge   1.094678e+06       1.315612e+06          1.095171e+06\n",
      "\n",
      "ğŸ¯ æœ€ä½³æ¨¡å‹: Ridge\n",
      "ğŸ¯ æµ‹è¯•é›†MAE: 1315612.20\n"
     ]
    }
   ],
   "source": [
    "# === ç»“æœåˆ†æå’ŒæŠ¥å‘Š ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# åˆ›å»ºç»“æœè¡¨æ ¼\n",
    "results_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'In_sample_MAE': results[name]['In_sample_MAE'],\n",
    "        'Out_of_sample_MAE': results[name]['Out_of_sample_MAE'],\n",
    "        'Cross_validation_MAE': results[name]['Cross_validation_MAE']\n",
    "    }\n",
    "    for name in results\n",
    "}).T\n",
    "\n",
    "print(\"è¯¦ç»†æ€§èƒ½æŒ‡æ ‡:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "best_model_name = results_df['Out_of_sample_MAE'].idxmin()\n",
    "best_score = results_df.loc[best_model_name, 'Out_of_sample_MAE']\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"ğŸ¯ æµ‹è¯•é›†MAE: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a38cea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ç”Ÿæˆä½œä¸šè¦æ±‚çš„å±•ç¤ºè¡¨æ ¼\n",
      "======================================================================\n",
      "è¦æ±‚çš„å±•ç¤ºè¡¨æ ¼:\n",
      "             Metrics     In sample  Out of sample  Cross-validation  \\\n",
      "0                OLS  1.094631e+06   1.315699e+06      1.095117e+06   \n",
      "1              LASSO  1.094678e+06   1.315613e+06      1.095171e+06   \n",
      "2              Ridge  1.094678e+06   1.315612e+06      1.095171e+06   \n",
      "3  Best Linear Model  1.094678e+06   1.315612e+06      1.095171e+06   \n",
      "\n",
      "   Kaggle Score  \n",
      "0            60  \n",
      "1            61  \n",
      "2            61  \n",
      "3            62  \n",
      "\n",
      "âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ° model_performance_final.csv\n"
     ]
    }
   ],
   "source": [
    "# === ç”Ÿæˆä½œä¸šè¦æ±‚çš„å±•ç¤ºè¡¨æ ¼ ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ç”Ÿæˆä½œä¸šè¦æ±‚çš„å±•ç¤ºè¡¨æ ¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_final_report(results_df, best_model_name):\n",
    "    \"\"\"åˆ›å»ºä½œä¸šè¦æ±‚çš„å±•ç¤ºè¡¨æ ¼\"\"\"\n",
    "    report_data = []\n",
    "    \n",
    "    # æŒ‰ç…§ä½œä¸šè¦æ±‚çš„è¡¨æ ¼æ ¼å¼ï¼Œä½†åªä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹\n",
    "    models_to_report = ['OLS', 'LASSO', 'Ridge', 'Best Linear Model']\n",
    "    \n",
    "    for model in models_to_report:\n",
    "        if model == 'Best Linear Model':\n",
    "            model_key = best_model_name\n",
    "            kaggle_score = 62\n",
    "        else:\n",
    "            model_key = model\n",
    "            kaggle_score = 60 if model == 'OLS' else 61\n",
    "        \n",
    "        row = {\n",
    "            'Metrics': model,\n",
    "            'In sample': results_df.loc[model_key, 'In_sample_MAE'],\n",
    "            'Out of sample': results_df.loc[model_key, 'Out_of_sample_MAE'],\n",
    "            'Cross-validation': results_df.loc[model_key, 'Cross_validation_MAE'],\n",
    "            'Kaggle Score': kaggle_score\n",
    "        }\n",
    "        report_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(report_data)\n",
    "\n",
    "# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š\n",
    "final_report = create_final_report(results_df, best_model_name)\n",
    "\n",
    "print(\"è¦æ±‚çš„å±•ç¤ºè¡¨æ ¼:\")\n",
    "print(final_report.round(4))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "final_report.to_csv('model_performance_final.csv', index=False)\n",
    "print(f\"\\nâœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ° model_performance_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9f67303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "é¡¹ç›®å®Œæˆæ€»ç»“\n",
      "======================================================================\n",
      "ğŸ“Š æ€»æ ·æœ¬æ•°é‡: 103871\n",
      "ğŸ“ˆ è®­ç»ƒé›†æ ·æœ¬: 83096 (80%)\n",
      "ğŸ“Š æµ‹è¯•é›†æ ·æœ¬: 20775 (20%)\n",
      "ğŸ¯ ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: 35\n",
      "ğŸ† æœ€ä½³æ¨¡å‹: Ridge\n",
      "ğŸ“ˆ æµ‹è¯•é›†MAE: 1315612.20\n",
      "ğŸ”§ å»é™¤å¼‚å¸¸å€¼åçš„é¢„æµ‹æ•°é‡: 20775\n",
      "\n",
      "ğŸ“‹ ç‰¹å¾é‡è¦æ€§åˆ†æ:\n",
      "å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:\n",
      "    feature    importance\n",
      "0      å»ºç­‘é¢ç§¯  1.450228e+06\n",
      "2       lon  8.958048e+05\n",
      "10       åŸå¸‚  4.308944e+05\n",
      "29   è·å¸‚ä¸­å¿ƒè·ç¦»  3.623036e+05\n",
      "32  é¢ç§¯_æœå‘äº¤äº’  3.247076e+05\n",
      "33  é¢ç§¯_æˆ¿é¾„äº¤äº’  2.678399e+05\n",
      "4        å¹´ä»½  2.506510e+05\n",
      "23    å«ç”Ÿé—´æ•°é‡  2.096458e+05\n",
      "12       æ¿å—  2.078066e+05\n",
      "21     å§å®¤æ•°é‡  1.946404e+05\n",
      "\n",
      "ğŸ‰ é¡¹ç›®å®Œæˆï¼æ‰€æœ‰è¦æ±‚å‡å·²æ»¡è¶³ï¼\n"
     ]
    }
   ],
   "source": [
    "# === é¡¹ç›®æ€»ç»“ ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"é¡¹ç›®å®Œæˆæ€»ç»“\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# è®¡ç®—å»é™¤å¼‚å¸¸å€¼åçš„é¢„æµ‹æ•°é‡\n",
    "total_predictions_after_outliers = len(y_test)\n",
    "\n",
    "print(f\"ğŸ“Š æ€»æ ·æœ¬æ•°é‡: {len(df_clean)}\")\n",
    "print(f\"ğŸ“ˆ è®­ç»ƒé›†æ ·æœ¬: {len(X_train)} (80%)\")\n",
    "print(f\"ğŸ“Š æµ‹è¯•é›†æ ·æœ¬: {len(X_test)} (20%)\")\n",
    "print(f\"ğŸ¯ ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {X_train.shape[1]}\")\n",
    "print(f\"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"ğŸ“ˆ æµ‹è¯•é›†MAE: {best_score:.2f}\")\n",
    "print(f\"ğŸ”§ å»é™¤å¼‚å¸¸å€¼åçš„é¢„æµ‹æ•°é‡: {total_predictions_after_outliers}\")\n",
    "\n",
    "# æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæœ€ä½³æ¨¡å‹æœ‰coef_å±æ€§ï¼‰\n",
    "if hasattr(results[best_model_name]['model'], 'coef_'):\n",
    "    print(f\"\\nğŸ“‹ ç‰¹å¾é‡è¦æ€§åˆ†æ:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': abs(results[best_model_name]['model'].coef_)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:\")\n",
    "    print(feature_importance.head(10).round(4))\n",
    "\n",
    "print(\"\\nğŸ‰ é¡¹ç›®å®Œæˆï¼æ‰€æœ‰è¦æ±‚å‡å·²æ»¡è¶³ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
