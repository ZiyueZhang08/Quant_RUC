{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d646a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6de7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chrome browser...\n"
     ]
    }
   ],
   "source": [
    "# Create a WebDriver instance\n",
    "print(\"Initializing Chrome browser...\")\n",
    "service = Service(\"C:/Tools/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd28ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded\n"
     ]
    }
   ],
   "source": [
    "# Navigate to the webpage\n",
    "url = 'https://cq.esf.fang.com/house-a058-b04846/'\n",
    "driver.get(url)\n",
    "print(\"Page loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d10c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Set implicit wait to globally wait for elements to load\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Wait for the main content of the page to load\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.TAG_NAME, 'dl'))\n",
    "    )\n",
    "    print(\"Page loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error waiting for page to load: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57183c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Function to scrape data from the current page\n",
    "def scrape_page():\n",
    "    try:\n",
    "        # Locate rows\n",
    "        listings_container = driver.find_element(By.CLASS_NAME, 'shop_list')\n",
    "        rows = listings_container.find_elements(By.TAG_NAME, 'dl')\n",
    "        \n",
    "        # Define selectors\n",
    "        selectors = {\n",
    "            'title': (By.CSS_SELECTOR, '.tit_shop'),\n",
    "            'area': (By.CSS_SELECTOR, '.tel_shop'),\n",
    "            'location': (By.CSS_SELECTOR, '.add_shop'),\n",
    "            'subway': (By.CSS_SELECTOR, '.bg_none.icon_dt'),\n",
    "            'price': (By.CLASS_NAME, 'red')\n",
    "        }\n",
    "        \n",
    "        print(f\"Extracting {len(rows)} listings...\")\n",
    "        success_count = 0\n",
    "        \n",
    "        # Process each listing\n",
    "        for i, row in enumerate(rows, 1):\n",
    "            try:\n",
    "                # Extract sale information\n",
    "                title = row.find_element(*selectors['title']).text.strip()\n",
    "                area_info = row.find_element(*selectors['area']).text.strip()\n",
    "                location = row.find_element(*selectors['location']).text.strip() \n",
    "                subInfor = row.find_element(*selectors['subway']).text.strip()\n",
    "                price_info = row.find_element(*selectors['price']).text.strip()\n",
    "                \n",
    "                # Extract price with decimal points\n",
    "                price = ''.join(c for c in price_info if c.isdigit() or c == '.')\n",
    "                \n",
    "                # Add data to list if title exists or at least some information is available\n",
    "                if title or any([area_info, location, subInfor, price_info]):\n",
    "                    data.append([title, area_info, location, subInfor, price])\n",
    "                    success_count += 1\n",
    "                \n",
    "                # Print progress every 10 listings\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Processed {i}/{len(rows)} listings\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data from row {i}: {e}\")\n",
    "        \n",
    "        print(f\"Successfully extracted {success_count}/{len(rows)} valid listings\")\n",
    "        return success_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from page: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d99ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape 20 pages...\n",
      "\n",
      "Scraping page 1/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 1 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 2/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 2 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 3/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 3 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 4/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 4 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 5/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 5 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 6/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 6 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 7/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 7 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 8/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 8 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 9/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 9 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 10/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 10 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 11/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 11 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 12/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 12 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 13/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 13 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 14/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 14 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 15/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 15 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 16/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 16 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 17/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 17 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 18/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 18 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 19/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 19 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping page 20/20\n",
      "Extracting 60 listings...\n",
      "Processed 10/60 listings\n",
      "Processed 20/60 listings\n",
      "Processed 30/60 listings\n",
      "Processed 40/60 listings\n",
      "Processed 50/60 listings\n",
      "Processed 60/60 listings\n",
      "Successfully extracted 60/60 valid listings\n",
      "Page 20 completed. Successfully extracted 60 listings.\n",
      "\n",
      "Scraping completed!\n",
      "Successfully scraped 1200 listings from 20 pages\n",
      "Data saved to Longtousi_sale_data_20pages.parquet\n",
      "\n",
      "Closing browser...\n"
     ]
    }
   ],
   "source": [
    "# Main scraping loop with pagination\n",
    "try:\n",
    "    total_pages_to_scrape = 20\n",
    "    current_page = 1\n",
    "    \n",
    "    # Record the total number of listings extracted\n",
    "    total_listings = 0\n",
    "    \n",
    "    print(f\"Starting to scrape {total_pages_to_scrape} pages...\")\n",
    "    \n",
    "    while current_page <= total_pages_to_scrape:\n",
    "        print(f\"\\nScraping page {current_page}/{total_pages_to_scrape}\")\n",
    "        \n",
    "        # Extract data from the current page\n",
    "        listings_extracted = scrape_page()\n",
    "        \n",
    "        print(f\"Page {current_page} completed. Successfully extracted {listings_extracted} listings.\")\n",
    "        \n",
    "        # Check if there is a next page to navigate to\n",
    "        if current_page < total_pages_to_scrape:\n",
    "            try:\n",
    "                # Locate and click the next page button\n",
    "                next_page_button = WebDriverWait(driver, 8).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//div[@class=\"page_al\"]//a[contains(text(), \"下一页\")]'))\n",
    "                )\n",
    "                # Scroll to the button and click\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(false);\", next_page_button)\n",
    "                next_page_button.click()\n",
    "                current_page += 1\n",
    "                \n",
    "                time.sleep(3)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error navigating to next page: {e}\")\n",
    "                print(\"Exiting pagination.\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Save the data to a Parquet file\n",
    "    if data:\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data, columns=['Title', 'Area Info', 'Location', 'Subway Info', 'Price'])\n",
    "        \n",
    "        # Process area extraction\n",
    "        def extract_area(area_info):\n",
    "            try:\n",
    "                if not area_info or str(area_info).strip() == \"N/A\":\n",
    "                    return \"N/A\"\n",
    "                # Ensure area_info is of string type\n",
    "                area_info = str(area_info).strip()\n",
    "                parts = area_info.split('|')\n",
    "                for part in parts:\n",
    "                    if '㎡' in part:\n",
    "                        # Extract the numeric part containing decimal points\n",
    "                        extracted = ''.join(c for c in part if c.isdigit() or c == '.')\n",
    "                        return extracted if extracted else \"N/A\"\n",
    "            except Exception as e:\n",
    "                return \"N/A\"\n",
    "        \n",
    "        # Apply area extraction to all rows\n",
    "        df['Area'] = df['Area Info'].apply(extract_area)\n",
    "        \n",
    "        # Save the DataFrame to a Parquet file\n",
    "        df.to_parquet('Longtousi_sale_data_20pages.parquet', index=False)\n",
    "        \n",
    "        print(f\"\\nScraping completed!\")\n",
    "        print(f\"Successfully scraped {len(df)} listings from {current_page} pages\")\n",
    "        print(f\"Data saved to Longtousi_sale_data_20pages.parquet\")\n",
    "    else:\n",
    "        print(\"No data was scraped from the website\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during scraping: {e}\")\n",
    "finally:\n",
    "    # Close the browser session\n",
    "    print(\"\\nClosing browser...\")\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
