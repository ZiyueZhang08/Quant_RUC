{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff16843-958d-433c-8768-ac583b3446b1",
   "metadata": {},
   "source": [
    "# University Applications Letters Generator \n",
    "\n",
    "- Purpose: when we try to apply graduate schools, the statement of purpose will take us tremendous time. If we turn to the application agencies, they will charge tens of thousands of CNY. Also, the agencies always make ***mistakes***.\n",
    "\n",
    "- Work:  use python to generate 90 copies of state of purpose efficiently\n",
    "\n",
    "- Details:\n",
    "    - step1 : find a application letter template by using online\n",
    "    - step2 : replace the personal information with your own personal information to get a `MS Word template` (one pager)\n",
    "    - step3 : create a `excel list` of 30 universities from econ department ranking by and only keep the university names\n",
    "    (https://ideas.repec.org/top/top.econdept.html, 10 from top 30, 10 from top 60, and 10 from top 90)\n",
    "    - step4: create a `excel list` of 3 interested research areas from (https://www.scmor.com/view/10554) (economics, management, finance, information management or etc.)\n",
    "    - step5 : add the `excel list` from step4 of top journals for each research areas you selected above (3 for each area)\n",
    "    - step6 : add the `excel list` from step4 of the skills you search from (Glassdoor Job)[https://www.glassdoor.com.hk/Job/index.htm]\n",
    "    - step7 : Loop over the two lists aforementioned to fill the template.\n",
    "    - step8 : use docxtpl to generate the MS Word document\n",
    "    \n",
    "    - step9 : use docx2pdf to generate the PDF document (only for the windows users)\n",
    "    - step10 : create a subdir named \"HW_School_Application\" under your home dir and upload your `your codes, excel list, Word template and only 1 copy of PDF or WORD` to ***GitHub***  by next class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a646a",
   "metadata": {},
   "source": [
    "Example:\n",
    "    \n",
    "Dear Admission Committee,\n",
    "\n",
    "My name is `Lei Ge`, and I am pleased to apply for the `Master of Finance program` at `Renmin University of China`.\n",
    "\n",
    "In my free time, I enjoy reading top-tier academic research to stay updated with the latest advancements in `finance`. I occasionally study articles from leading ABS 4+ rated journals such as the `Review of Financial Studies (RES), Journal of Finance (JF), and Management Science (MS)`, among others. This habit not only deepens my understanding of theoretical and empirical approaches in finance but also sharpens my ability to critically analyze complex economic phenomena.\n",
    "\n",
    "I want to be a `quant researcher`. To achive my dream, I have practical skills such as `Python, SQL, Math, PowerBI, Tableau and etc`. \n",
    "\n",
    "I am particularly drawn to Renmin University of China due to its strong academic environment and research-oriented approach. \n",
    "\n",
    "Thank you for considering my application. I am eager to contribute to and benefit from the rigorous academic culture at `Renmin University of China`.\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "Lei Ge\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- TOOLS: function, loop, docxtpl, docx2pdf\n",
    "- Results: you get a 90 copies of statement of purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9765167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 完成（docxtpl 模板），模板已保存在当前目录: /Users/luok/Desktop/2023200211/sop_template_docxtpl.docx\n"
     ]
    }
   ],
   "source": [
    "# Step1: 使用 docxtpl 的 SOP 模板（含日期与联系方式）\n",
    "import os\n",
    "from docx import Document\n",
    "\n",
    "cwd = os.getcwd()\n",
    "TARGET_PATH = os.path.join(cwd, \"sop_template_docxtpl.docx\")\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "# 开头\n",
    "doc.add_paragraph(\"Dear Admission Committee,\")\n",
    "\n",
    "doc.add_paragraph(\n",
    "    \"My name is {{ applicant_name }}, and I am pleased to apply for the {{ program_name }} at {{ university_name }}.\"\n",
    ")\n",
    "\n",
    "doc.add_paragraph(\n",
    "    \"In my free time, I enjoy reading top-tier academic research to stay updated with the latest advancements in \"\n",
    "    \"{{ research_area }}. I occasionally study articles from leading journals such as the {{ top_journals }}, among others. \"\n",
    "    \"This habit not only deepens my understanding of theoretical and empirical approaches but also sharpens my ability \"\n",
    "    \"to critically analyze complex economic phenomena.\"\n",
    ")\n",
    "\n",
    "doc.add_paragraph(\n",
    "    \"I want to be a {{ career_goal }}. To achieve my dream, I have practical skills such as {{ skills }}.\"\n",
    ")\n",
    "\n",
    "doc.add_paragraph(\n",
    "    \"I am particularly drawn to {{ university_name }} due to its strong academic environment and research-oriented approach.\"\n",
    ")\n",
    "\n",
    "doc.add_paragraph(\n",
    "    \"Thank you for considering my application. I am eager to contribute to and benefit from the rigorous academic culture at \"\n",
    "    \"{{ university_name }}.\"\n",
    ")\n",
    "\n",
    "# 结束语与签名\n",
    "doc.add_paragraph(\"Sincerely,\")\n",
    "doc.add_paragraph(\"{{ applicant_name }}\")\n",
    "\n",
    "# 日期与联系方式\n",
    "doc.add_paragraph(\"Date: {{ date }}\")\n",
    "doc.add_paragraph(\"Contact: {{ contact }}\")\n",
    "\n",
    "# 保存\n",
    "doc.save(TARGET_PATH)\n",
    "print(\"Step1 完成（docxtpl 模板），模板已保存在当前目录:\", TARGET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93f03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step2 (docxtpl) 完成，已在当前目录生成： /Users/luok/Desktop/2023200211/sop_docxtpl_filled.docx\n"
     ]
    }
   ],
   "source": [
    "# Step2 (docxtpl): 使用 sop_template_docxtpl.docx 渲染并生成成品（含日期与联系方式）\n",
    "import os, sys, subprocess\n",
    "from datetime import date\n",
    "\n",
    "# 确保 docxtpl 可用\n",
    "try:\n",
    "    from docxtpl import DocxTemplate  # type: ignore[reportMissingImports]\n",
    "except Exception:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"docxtpl\"], check=False)\n",
    "    from docxtpl import DocxTemplate  # type: ignore[reportMissingImports]\n",
    "\n",
    "cwd = os.getcwd()\n",
    "TEMPLATE_PATH = os.path.join(cwd, \"sop_template_docxtpl.docx\")\n",
    "OUTPUT_DOCX = os.path.join(cwd, \"sop_docxtpl_filled.docx\")\n",
    "\n",
    "if not os.path.exists(TEMPLATE_PATH):\n",
    "    raise FileNotFoundError(f\"未找到模板: {TEMPLATE_PATH}，请先运行Step1生成模板\")\n",
    "\n",
    "# 个人信息（可按需修改）\n",
    "context = {\n",
    "    \"applicant_name\": \"Qimin Lin\",\n",
    "    \"program_name\": \"Master of Finance program\",\n",
    "    \"university_name\": \"Renmin University of China\",\n",
    "    \"research_area\": \"finance\",\n",
    "    \"top_journals\": \"Review of Financial Studies (RFS), Journal of Finance (JF), Management Science (MS)\",\n",
    "    \"career_goal\": \"quant researcher\",\n",
    "    \"skills\": \"Python, SQL, Math,solid grasp of economic and financial knowledge,proficient application of AI tools for academic and analytical scenarios\",\n",
    "    \"date\": date.today().isoformat(),\n",
    "    \"contact\": \"+86-139-0000-0000 | linqimin@qq.com\",\n",
    "}\n",
    "\n",
    "# 渲染\n",
    "tpl = DocxTemplate(TEMPLATE_PATH)\n",
    "tpl.render(context)\n",
    "tpl.save(OUTPUT_DOCX)\n",
    "print(\"Step2 (docxtpl) 完成，已在当前目录生成：\", OUTPUT_DOCX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40336e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step3 完成：已保存 30 所大学名称到 Excel：/Users/luok/Desktop/2023200211/step3_universities.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step3: 爬取 RePEc 经济系排名并生成仅含大学名称的 Excel（30 所）\n",
    "# 需求：从 https://ideas.repec.org/top/top.econdept.html 爬取“经济系”排名，\n",
    "#       选取 10 所来自 Top30、10 所来自 Top60（31-60 区间）、10 所来自 Top90（61-90 区间），\n",
    "#       仅保留大学/机构名称，保存为 Excel 文件。\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# 确保依赖存在\n",
    "for pkg in [\"requests\", \"beautifulsoup4\", \"pandas\", \"lxml\"]:\n",
    "    try:\n",
    "        __import__(pkg if pkg != \"beautifulsoup4\" else \"bs4\")\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "\n",
    "REPEC_URL = \"https://ideas.repec.org/top/top.econdept.html\"\n",
    "OUTPUT_XLSX = os.path.join(os.getcwd(), \"step3_universities.xlsx\")\n",
    "\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,zh-CN,zh;q=0.8\",\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def locate_ranking_table(soup: BeautifulSoup):\n",
    "    \"\"\"在页面中定位包含列 Rank / Institution 的表格。\"\"\"\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        # 获取表头\n",
    "        headers = []\n",
    "        thead = table.find(\"thead\")\n",
    "        if thead:\n",
    "            ths = thead.find_all(\"th\")\n",
    "            headers = [th.get_text(strip=True) for th in ths]\n",
    "        else:\n",
    "            # 可能没有 thead，则尝试第一行作为表头\n",
    "            first_tr = table.find(\"tr\")\n",
    "            if first_tr:\n",
    "                headers = [th.get_text(strip=True) for th in first_tr.find_all([\"th\", \"td\"])]\n",
    "        # 判断是否为目标表\n",
    "        if not headers:\n",
    "            continue\n",
    "        normalized = [h.lower() for h in headers]\n",
    "        if \"rank\" in normalized and \"institution\" in normalized:\n",
    "            return table\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_departments(html: str) -> List[Tuple[int, str]]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = locate_ranking_table(soup)\n",
    "    if table is None:\n",
    "        raise RuntimeError(\"未找到包含 Rank/Institution 的排名表格，请检查页面结构是否变化\")\n",
    "\n",
    "    rows = []\n",
    "    # 排除表头行\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) < 2:\n",
    "            continue\n",
    "        # 解析 Rank\n",
    "        rank_text = tds[0].get_text(strip=True)\n",
    "        try:\n",
    "            rank = int(rank_text.split()[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        # 解析 Institution 名称（优先取 <a> 的文本）\n",
    "        inst_cell = tds[1]\n",
    "        a = inst_cell.find(\"a\")\n",
    "        if a and a.get_text(strip=True):\n",
    "            name = a.get_text(strip=True)\n",
    "        else:\n",
    "            # 退化：直接取单元格文本，并尽量去掉地理位置尾巴\n",
    "            name = inst_cell.get_text(\" \", strip=True)\n",
    "        rows.append((rank, name))\n",
    "\n",
    "    # 去重并按 rank 排序\n",
    "    dedup = {}\n",
    "    for r, n in rows:\n",
    "        if r not in dedup:\n",
    "            dedup[r] = n\n",
    "    items = sorted(dedup.items(), key=lambda x: x[0])\n",
    "    return items\n",
    "\n",
    "\n",
    "def select_top_30(items: List[Tuple[int, str]]) -> List[str]:\n",
    "    \"\"\"从 1-30、31-60、61-90 各取前 10 所，共 30 所。只返回名称列表。\"\"\"\n",
    "    seg1 = [n for r, n in items if 1 <= r <= 30][:10]\n",
    "    seg2 = [n for r, n in items if 31 <= r <= 60][:10]\n",
    "    seg3 = [n for r, n in items if 61 <= r <= 90][:10]\n",
    "    combined = seg1 + seg2 + seg3\n",
    "    if len(seg1) < 10 or len(seg2) < 10 or len(seg3) < 10:\n",
    "        print(\"警告：某些区间不足 10 所，已按可用数量返回。\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def save_to_excel(names: List[str], path: str) -> None:\n",
    "    df = pd.DataFrame({\"university_name\": names})\n",
    "    # 仅一列，保证“只保留大学名称”\n",
    "    df.to_excel(path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html = fetch_html(REPEC_URL)\n",
    "    items = parse_departments(html)\n",
    "    names = select_top_30(items)\n",
    "    save_to_excel(names, OUTPUT_XLSX)\n",
    "    print(f\"Step3 完成：已保存 30 所大学名称到 Excel：{OUTPUT_XLSX}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8e2dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step3 更新完成：已仅保留大学名，保存到： /Users/luok/Desktop/2023200211/step3_universities.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step3-更新：仅保留大学名（如含逗号则取最后一段）\n",
    "import os, sys, subprocess\n",
    "\n",
    "for pkg in [\"requests\", \"beautifulsoup4\", \"pandas\", \"lxml\"]:\n",
    "    try:\n",
    "        __import__(pkg if pkg != \"beautifulsoup4\" else \"bs4\")\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://ideas.repec.org/top/top.econdept.html\"\n",
    "OUTPUT_XLSX = os.path.join(os.getcwd(), \"step3_universities.xlsx\")\n",
    "\n",
    "\n",
    "def fetch(url: str) -> str:\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def locate_table(soup: BeautifulSoup):\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        headers = []\n",
    "        thead = table.find(\"thead\")\n",
    "        if thead:\n",
    "            headers = [th.get_text(strip=True).lower() for th in thead.find_all(\"th\")]\n",
    "        else:\n",
    "            tr = table.find(\"tr\")\n",
    "            if tr:\n",
    "                headers = [td.get_text(strip=True).lower() for td in tr.find_all([\"th\",\"td\"])]\n",
    "        if \"rank\" in headers and \"institution\" in headers:\n",
    "            return table\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_university_name(name: str) -> str:\n",
    "    # 如果包含逗号，取最后一段（通常是大学名）\n",
    "    parts = [p.strip() for p in name.split(\",\") if p.strip()]\n",
    "    if parts:\n",
    "        return parts[-1]\n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "def parse_items(html: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = locate_table(soup)\n",
    "    if table is None:\n",
    "        raise RuntimeError(\"未找到排名表格\")\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            rank = int(tds[0].get_text(strip=True).split()[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "        a = tds[1].find(\"a\")\n",
    "        name = a.get_text(strip=True) if a and a.get_text(strip=True) else tds[1].get_text(\" \", strip=True)\n",
    "        rows.append((rank, name))\n",
    "    rows = sorted({r:n for r,n in rows}.items(), key=lambda x:x[0])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def select_top_30(items):\n",
    "    seg1 = [n for r,n in items if 1<=r<=30][:10]\n",
    "    seg2 = [n for r,n in items if 31<=r<=60][:10]\n",
    "    seg3 = [n for r,n in items if 61<=r<=90][:10]\n",
    "    names = seg1+seg2+seg3\n",
    "    # 清洗：仅保留大学名\n",
    "    names = [extract_university_name(n) for n in names]\n",
    "    return names\n",
    "\n",
    "html = fetch(URL)\n",
    "items = parse_items(html)\n",
    "names = select_top_30(items)\n",
    "pd.DataFrame({\"university_name\": names}).to_excel(OUTPUT_XLSX, index=False)\n",
    "print(\"Step3 更新完成：已仅保留大学名，保存到：\", OUTPUT_XLSX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f0b9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step4 完成： /Users/luok/Desktop/2023200211/step4_research_areas.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step4: 生成感兴趣研究领域 Excel（来自 SCMOR 页面的领域编码）\n",
    "# 目标领域：ECON, FINANCE, INFO MAN\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "areas = [\n",
    "    {\"area_code\": \"ECON\", \"area_name\": \"economics\"},\n",
    "    {\"area_code\": \"FINANCE\", \"area_name\": \"finance\"},\n",
    "    {\"area_code\": \"INFO MAN\", \"area_name\": \"information management\"},\n",
    "]\n",
    "\n",
    "STEP4_XLSX = os.path.join(os.getcwd(), \"step4_research_areas.xlsx\")\n",
    "\n",
    "pd.DataFrame(areas).to_excel(STEP4_XLSX, index=False)\n",
    "print(\"Step4 完成：\", STEP4_XLSX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aa904b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step5 完成： /Users/luok/Desktop/2023200211/step5_area_top_journals.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step5: 从 SCMOR 页面爬取每领域最先出现的3本期刊，导出 Excel\n",
    "# 页面： https://www.scmor.com/view/10554\n",
    "import os, sys, subprocess\n",
    "\n",
    "for pkg in [\"requests\", \"beautifulsoup4\", \"pandas\", \"lxml\"]:\n",
    "    try:\n",
    "        __import__(pkg if pkg != \"beautifulsoup4\" else \"bs4\")\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "\n",
    "URL = \"https://www.scmor.com/view/10554\"\n",
    "STEP5_XLSX = os.path.join(os.getcwd(), \"step5_area_top_journals.xlsx\")\n",
    "\n",
    "TARGET_CODES = [\"ECON\", \"FINANCE\", \"INFO MAN\"]\n",
    "\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,zh-CN,zh;q=0.8\",\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def tables_from_page(html: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return soup.find_all(\"table\")\n",
    "\n",
    "\n",
    "def normalize_header(text: str) -> str:\n",
    "    return text.strip().lower().replace(\" \", \"\").replace(\"*\", \"\")\n",
    "\n",
    "\n",
    "def parse_table(table) -> pd.DataFrame | None:\n",
    "    # 尝试解析为 DataFrame，要求包含“领域/field”和“期刊/journal”两列\n",
    "    # 容忍列名变化：领域、field、领域代码；期刊、期刊名称、journal\n",
    "    headers = []\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "    else:\n",
    "        first_tr = table.find(\"tr\")\n",
    "        if first_tr:\n",
    "            headers = [td.get_text(strip=True) for td in first_tr.find_all([\"th\", \"td\"])]\n",
    "    if not headers:\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    body_trs = table.find_all(\"tr\")\n",
    "    # 跳过第一行表头\n",
    "    for tr in body_trs[1:]:\n",
    "        tds = tr.find_all([\"td\", \"th\"])  # 有些表用 th 做第一列\n",
    "        if len(tds) < len(headers):\n",
    "            continue\n",
    "        row = [td.get_text(\" \", strip=True) for td in tds[:len(headers)]]\n",
    "        rows.append(row)\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # 统一列名\n",
    "    col_map = {}\n",
    "    for col in df.columns:\n",
    "        key = normalize_header(col)\n",
    "        if key in (\"领域\", \"field\", \"领域field\", \"领域/field\", \"领域代码\", \"领域domain\", \"domain\", \"领域列\"):\n",
    "            col_map[col] = \"field\"\n",
    "        elif key in (\"期刊\", \"期刊名称\", \"journal\", \"期刊名\"):\n",
    "            col_map[col] = \"journal\"\n",
    "        else:\n",
    "            # 保留原列\n",
    "            col_map[col] = col\n",
    "    df = df.rename(columns=col_map)\n",
    "\n",
    "    if \"field\" not in df.columns or \"journal\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    # 清洗 field 与 journal\n",
    "    df[\"field\"] = df[\"field\"].astype(str).str.strip()\n",
    "    df[\"journal\"] = df[\"journal\"].astype(str).str.strip()\n",
    "    return df[[\"field\", \"journal\"]]\n",
    "\n",
    "\n",
    "html = fetch_html(URL)\n",
    "all_tables = tables_from_page(html)\n",
    "frames = []\n",
    "for tbl in all_tables:\n",
    "    d = parse_table(tbl)\n",
    "    if d is not None and not d.empty:\n",
    "        frames.append(d)\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"未能在页面中解析出包含 领域/期刊 的表格，请检查页面结构\")\n",
    "\n",
    "merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# 查找每个领域最先出现的3本期刊（按页面顺序）\n",
    "records = []\n",
    "for code in TARGET_CODES:\n",
    "    sub = merged[merged[\"field\"].str.upper() == code]\n",
    "    top3 = sub.head(3)[\"journal\"].tolist()\n",
    "    for idx, jn in enumerate(top3, start=1):\n",
    "        records.append({\"area_code\": code, \"journal\": jn, \"rank_order\": idx})\n",
    "\n",
    "result = pd.DataFrame(records)\n",
    "result.to_excel(STEP5_XLSX, index=False)\n",
    "print(\"Step5 完成：\", STEP5_XLSX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ea03681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step6 完成：整合领域-期刊-技能，输出文件： /Users/luok/Desktop/2023200211/step6_area_journals_skills.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step6: 整合研究领域、顶刊与技能，并导出 Excel\n",
    "import os, sys, subprocess\n",
    "\n",
    "# 确保 pandas 可用\n",
    "try:\n",
    "    import pandas as pd  # type: ignore\n",
    "except Exception:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\", \"openpyxl\"], check=False)\n",
    "    import pandas as pd  # type: ignore\n",
    "\n",
    "CWD = os.getcwd()\n",
    "STEP4_XLSX = os.path.join(CWD, \"step4_research_areas.xlsx\")\n",
    "STEP5_XLSX = os.path.join(CWD, \"step5_area_top_journals.xlsx\")\n",
    "STEP6_XLSX = os.path.join(CWD, \"step6_area_journals_skills.xlsx\")\n",
    "\n",
    "# 用户提供的技能（按题意来自 Step4 的技能收集，此处直接使用给定字符串）\n",
    "skills_str = \"Python, SQL, Math, solid grasp of economic and financial knowledge, proficient application of AI tools for academic and analytical scenarios\"\n",
    "\n",
    "# 读取 Step4/Step5\n",
    "if not os.path.exists(STEP4_XLSX):\n",
    "    raise FileNotFoundError(f\"未找到 Step4 文件: {STEP4_XLSX}\")\n",
    "if not os.path.exists(STEP5_XLSX):\n",
    "    raise FileNotFoundError(f\"未找到 Step5 文件: {STEP5_XLSX}\")\n",
    "\n",
    "areas_df = pd.read_excel(STEP4_XLSX)\n",
    "journals_df = pd.read_excel(STEP5_XLSX)\n",
    "\n",
    "# 规范列名\n",
    "areas_df = areas_df.rename(columns={\"area_code\": \"area_code\", \"area_name\": \"area_name\"})\n",
    "journals_df = journals_df.rename(columns={\"area_code\": \"area_code\", \"journal\": \"journal\", \"rank_order\": \"rank_order\"})\n",
    "\n",
    "# 校验必要列\n",
    "for req in [\"area_code\", \"area_name\"]:\n",
    "    if req not in areas_df.columns:\n",
    "        raise KeyError(f\"Step4 缺少必要列: {req}\")\n",
    "for req in [\"area_code\", \"journal\"]:\n",
    "    if req not in journals_df.columns:\n",
    "        raise KeyError(f\"Step5 缺少必要列: {req}\")\n",
    "\n",
    "# 合并（左连接保留领域列表）\n",
    "merged = areas_df.merge(journals_df[[\"area_code\", \"journal\", \"rank_order\"]], on=\"area_code\", how=\"left\")\n",
    "\n",
    "# 将每个领域的 journal 聚合为逗号分隔\n",
    "agg = (merged\n",
    "    .sort_values([\"area_code\", \"rank_order\"], na_position=\"last\")\n",
    "    .groupby([\"area_code\", \"area_name\"], as_index=False)\n",
    "    .agg({\"journal\": lambda s: \", \".join([x for x in s.dropna().astype(str) if x])})\n",
    ")\n",
    "agg = agg.rename(columns={\"journal\": \"top_journals\"})\n",
    "\n",
    "# 添加技能列（同一串技能，便于后续模板渲染）\n",
    "agg[\"skills\"] = skills_str\n",
    "\n",
    "# 输出 Excel\n",
    "agg.to_excel(STEP6_XLSX, index=False)\n",
    "print(\"Step6 完成：整合领域-期刊-技能，输出文件：\", STEP6_XLSX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step7: 循环生成 30 所大学 × 3 个研究领域的 SOP（docxtpl）\n",
    "import os, sys, subprocess\n",
    "from datetime import date\n",
    "\n",
    "# 确保依赖\n",
    "try:\n",
    "    from docxtpl import DocxTemplate  # type: ignore\n",
    "except Exception:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"docxtpl\"], check=False)\n",
    "    from docxtpl import DocxTemplate  # type: ignore\n",
    "\n",
    "try:\n",
    "    import pandas as pd  # type: ignore\n",
    "except Exception:\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pandas\", \"openpyxl\"], check=False)\n",
    "    import pandas as pd  # type: ignore\n",
    "\n",
    "CWD = os.getcwd()\n",
    "TEMPLATE_PATH = os.path.join(CWD, \"sop_template_docxtpl.docx\")\n",
    "UNIV_XLSX = os.path.join(CWD, \"step3_universities.xlsx\")\n",
    "AREA_SKILL_XLSX = os.path.join(CWD, \"step6_area_journals_skills.xlsx\")\n",
    "OUTPUT_DIR = os.path.join(CWD, \"step7_outputs\")\n",
    "\n",
    "# 固定信息（可按需修改）\n",
    "APPLICANT_NAME = \"Qimin Lin\"\n",
    "PROGRAM_NAME = \"Master of Finance program\"\n",
    "CAREER_GOAL = \"quant researcher\"\n",
    "CONTACT = \"+86-139-0000-0000 | linqimin@qq.com\"\n",
    "\n",
    "# 校验资源\n",
    "if not os.path.exists(TEMPLATE_PATH):\n",
    "    raise FileNotFoundError(f\"未找到模板：{TEMPLATE_PATH}，请先运行 Step1 生成模板\")\n",
    "if not os.path.exists(UNIV_XLSX):\n",
    "    raise FileNotFoundError(f\"未找到 Step3 大学列表：{UNIV_XLSX}\")\n",
    "if not os.path.exists(AREA_SKILL_XLSX):\n",
    "    raise FileNotFoundError(f\"未找到 Step6 领域-期刊-技能表：{AREA_SKILL_XLSX}\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 读取数据\n",
    "univs = pd.read_excel(UNIV_XLSX)\n",
    "areas = pd.read_excel(AREA_SKILL_XLSX)\n",
    "\n",
    "# 列检查\n",
    "if \"university_name\" not in univs.columns:\n",
    "    raise KeyError(\"Step3 缺少必要列：university_name\")\n",
    "for col in [\"area_code\", \"area_name\", \"top_journals\", \"skills\"]:\n",
    "    if col not in areas.columns:\n",
    "        raise KeyError(f\"Step6 缺少必要列：{col}\")\n",
    "\n",
    "# 安全文件名\n",
    "def safe_filename(name: str) -> str:\n",
    "    keep = \"-_.() []{}\"\n",
    "    return \"\".join(c if c.isalnum() or c in keep else \"_\" for c in str(name)).strip(\" ._\")\n",
    "\n",
    "# 生成\n",
    "count = 0\n",
    "for _, urow in univs.iterrows():\n",
    "    uni = str(urow[\"university_name\"]).strip()\n",
    "    if not uni:\n",
    "        continue\n",
    "    for _, arow in areas.iterrows():\n",
    "        area_code = str(arow[\"area_code\"]).strip()\n",
    "        area_name = str(arow[\"area_name\"]).strip()\n",
    "        top_journals = str(arow.get(\"top_journals\", \"\")).strip()\n",
    "        skills = str(arow.get(\"skills\", \"\")).strip()\n",
    "\n",
    "        context = {\n",
    "            \"applicant_name\": APPLICANT_NAME,\n",
    "            \"program_name\": PROGRAM_NAME,\n",
    "            \"university_name\": uni,\n",
    "            \"research_area\": area_name or area_code,\n",
    "            \"top_journals\": top_journals,\n",
    "            \"career_goal\": CAREER_GOAL,\n",
    "            \"skills\": skills,\n",
    "            \"date\": date.today().isoformat(),\n",
    "            \"contact\": CONTACT,\n",
    "        }\n",
    "\n",
    "        tpl = DocxTemplate(TEMPLATE_PATH)\n",
    "        tpl.render(context)\n",
    "        fname = f\"SOP_{safe_filename(uni)}_{safe_filename(area_code or area_name)}.docx\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, fname)\n",
    "        tpl.save(out_path)\n",
    "        count += 1\n",
    "\n",
    "print(f\"Step7 完成：已生成 {count} 份 SOP 到目录：{OUTPUT_DIR}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
