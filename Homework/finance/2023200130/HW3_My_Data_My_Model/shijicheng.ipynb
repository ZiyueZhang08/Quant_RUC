{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e04001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Collecting trio<1.0,>=0.30.0 (from selenium)\n",
      "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Collecting websocket-client<2.0,>=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Collecting attrs>=23.2.0 (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio<1.0,>=0.30.0->selenium)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.6 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.5/9.6 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.5/9.6 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 6.3/9.6 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 8.9/9.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 1.9 MB/s eta 0:00:00\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, python-dotenv, pysocks, pycparser, h11, attrs, wsproto, webdriver-manager, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-25.4.0 cffi-2.0.0 h11-0.16.0 outcome-1.3.0.post0 pycparser-2.23 pysocks-1.7.1 python-dotenv-1.1.1 selenium-4.36.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.31.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 websocket-client-1.9.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager beautifulsoup4 lxml requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c825100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始抓取 海淀-世纪城 二手房数据（来源：https://bj.esf.fang.com/）...\n",
      "第1页：抓取到 61 条房源\n",
      "第2页：抓取到 61 条房源\n",
      "第3页：抓取到 60 条房源\n",
      "第4页：抓取到 60 条房源\n",
      "第5页：抓取到 60 条房源\n",
      "第6页：抓取到 60 条房源\n",
      "第7页：抓取到 60 条房源\n",
      "第8页：抓取到 60 条房源\n",
      "第9页：抓取到 60 条房源\n",
      "第10页：抓取到 60 条房源\n",
      "第11页：抓取到 60 条房源\n",
      "第12页：抓取到 60 条房源\n",
      "第13页：抓取到 60 条房源\n",
      "第14页：抓取到 60 条房源\n",
      "第15页：抓取到 60 条房源\n",
      "第16页：抓取到 60 条房源\n",
      "第17页：抓取到 60 条房源\n",
      "第18页：抓取到 60 条房源\n",
      "第19页：抓取到 60 条房源\n",
      "第20页：抓取到 60 条房源\n",
      "\n",
      "抓取完成！共获取 1202 条二手房数据，已保存至 shijicheng_esf.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "try:\n",
    "    from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "    WEBDRIVER_MANAGER_AVAILABLE = True\n",
    "except Exception:\n",
    "    WEBDRIVER_MANAGER_AVAILABLE = False\n",
    "\n",
    "\n",
    "# 仅保留二手房相关配置\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "ESF_BASE = \"https://bj.esf.fang.com/\"  # 二手房基础域名\n",
    "ESF_COMMUNITY_PATH = \"house-a015277-b03115/\"  # 海淀-世纪城二手房社区页路径\n",
    "HEADERS = {\n",
    "    \"User-Agent\": USER_AGENT,\n",
    "    \"Referer\": ESF_BASE,\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "}\n",
    "\n",
    "\n",
    "# 仅保留二手房数据类\n",
    "@dataclass\n",
    "class SaleItem:\n",
    "    area_sqm: float  # 面积（平方米）\n",
    "    total_price_wan: float  # 总价（万元）\n",
    "    unit_price: float  # 单价（元/平方米）\n",
    "\n",
    "\n",
    "def ensure_domain(url: str, allowed_domains: Iterable[str]) -> bool:\n",
    "    \"\"\"验证URL是否属于允许的二手房域名\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    hostname = parsed.hostname or \"\"\n",
    "    return any(hostname == d or hostname.endswith(\".\" + d) for d in allowed_domains)\n",
    "\n",
    "\n",
    "def get_soup(url: str, session: Optional[requests.Session] = None) -> BeautifulSoup:\n",
    "    \"\"\"获取页面HTML并解析为BeautifulSoup对象（requests版）\"\"\"\n",
    "    if not ensure_domain(url, [\"esf.fang.com\"]):\n",
    "        raise ValueError(\"URL 不在允许的二手房域名中，拒绝请求：\" + url)\n",
    "    s = session or requests.Session()\n",
    "    resp = s.get(url, headers=HEADERS, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "\n",
    "def create_driver() -> webdriver.Edge:\n",
    "    \"\"\"创建Edge无头浏览器驱动（用于动态页面加载）\"\"\"\n",
    "    options = EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    options.add_argument(\"--headless=new\")  # 无头模式\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(f\"--user-agent={USER_AGENT}\")\n",
    "    options.add_argument(\"--lang=zh-CN\")\n",
    "    \n",
    "    # 固定Edge驱动路径（需根据实际情况调整）\n",
    "    driver_dir = r\"C:\\Users\\lenovo\\Desktop\\2025\\KE\\Quent\\edgedriver_win64\"\n",
    "    driver_exe = os.path.join(driver_dir, \"msedgedriver.exe\")\n",
    "    if not os.path.exists(driver_exe):\n",
    "        raise FileNotFoundError(\n",
    "            f\"未找到 Edge 驱动：{driver_exe}\\n\"\n",
    "            \"请确认已解压并提供正确路径（需要 msedgedriver.exe）\"\n",
    "        )\n",
    "    \n",
    "    service = EdgeService(executable_path=driver_exe)\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_soup_selenium(url: str, driver: webdriver.Edge) -> BeautifulSoup:\n",
    "    \"\"\"通过Selenium获取动态页面并解析（处理懒加载）\"\"\"\n",
    "    if not ensure_domain(url, [\"esf.fang.com\"]):\n",
    "        raise ValueError(\"URL 不在允许的二手房域名中，拒绝请求：\" + url)\n",
    "    \n",
    "    driver.get(url)\n",
    "    # 等待页面加载完成\n",
    "    try:\n",
    "        WebDriverWait(driver, 8).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    \n",
    "    # 等待房源列表元素出现\n",
    "    selectors = [\n",
    "        (By.CSS_SELECTOR, \"div.shop_list_4 dl\"),\n",
    "        (By.CSS_SELECTOR, \"div.shop_list\"),\n",
    "        (By.CSS_SELECTOR, \"div#houseList dl\"),\n",
    "        (By.CSS_SELECTOR, \"div.houseList dl\"),\n",
    "        (By.CSS_SELECTOR, \"ul.listCon li\"),\n",
    "        (By.CSS_SELECTOR, \"div#listBox dl\"),\n",
    "        (By.CSS_SELECTOR, \"ul#houseList li\"),\n",
    "    ]\n",
    "    for by, sel in selectors:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((by, sel)))\n",
    "            break\n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    \n",
    "    # 滚动页面触发懒加载\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, Math.min(800, document.body.scrollHeight));\")\n",
    "        time.sleep(0.6)\n",
    "        driver.execute_script(\"window.scrollTo(0, Math.min(2000, document.body.scrollHeight));\")\n",
    "        time.sleep(0.6)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.6)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # 首次运行保存首页HTML用于调试\n",
    "    if not hasattr(driver, \"_saved_first_page\"):\n",
    "        with open(\"debug_esf_first_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        setattr(driver, \"_saved_first_page\", True)\n",
    "    \n",
    "    return BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "\n",
    "def extract_numbers(text: str, pattern: str) -> Optional[float]:\n",
    "    \"\"\"从文本中提取数字（用于解析面积、价格）\"\"\"\n",
    "    m = re.search(pattern, text.replace(\"\\xa0\", \" \"))\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_sale_list(soup: BeautifulSoup) -> Tuple[List[SaleItem], Optional[str]]:\n",
    "    \"\"\"解析二手房列表页面，提取房源数据和下一页链接\"\"\"\n",
    "    items: List[SaleItem] = []\n",
    "    \n",
    "    # 匹配二手房列表容器\n",
    "    containers = soup.select(\n",
    "        \"div.shop_list, ul.shop_list, div#houseList, div.houseList, ul.list, div#list, div.listBox, ul#houseList\"\n",
    "    ) or [soup]\n",
    "    \n",
    "    for container in containers:\n",
    "        # 匹配单个房源卡片\n",
    "        cards = container.select(\"dl, dd, div.list, div.item, li, section\") or []\n",
    "        for card in cards:\n",
    "            text = card.get_text(\" \", strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # 提取核心数据\n",
    "            area = extract_numbers(text, r\"(\\d+(?:\\.\\d+)?)\\s*㎡\")  # 面积\n",
    "            total_price = extract_numbers(text, r\"(\\d+(?:\\.\\d+)?)\\s*万\")  # 总价（万）\n",
    "            unit_price = extract_numbers(text, r\"(\\d+(?:\\.\\d+)?)\\s*元/㎡\")  # 单价\n",
    "            \n",
    "            # 数据补全（缺一个时通过另一个计算）\n",
    "            if area and (total_price or unit_price):\n",
    "                if total_price is None:\n",
    "                    total_price = round((unit_price * area) / 10000.0, 2)\n",
    "                if unit_price is None:\n",
    "                    unit_price = round((total_price * 10000.0) / area, 2)\n",
    "                items.append(SaleItem(area, total_price, unit_price))\n",
    "    \n",
    "    # 提取下一页链接\n",
    "    next_href = None\n",
    "    next_link = soup.select_one(\"a[rel=next], a.pageNext, a.next, a.next-page, a#PageControl1_hlk_next, a:-soup-contains('下一页')\")\n",
    "    if next_link and next_link.get(\"href\"):\n",
    "        next_href = next_link[\"href\"]\n",
    "    \n",
    "    return items, next_href\n",
    "\n",
    "\n",
    "def crawl_sales_pages(keyword: str, district: str, max_pages: int = 20, delay_sec: float = 1.0) -> Iterable[Tuple[int, List[SaleItem]]]:\n",
    "    \"\"\"抓取二手房多页数据，返回（页码，该页房源列表）\"\"\"\n",
    "    driver = create_driver()\n",
    "    try:\n",
    "        for page_no in range(1, max_pages + 1):\n",
    "            # 构造分页URL（第1页无分页参数，>=2页加i3{页码}/）\n",
    "            page_path = ESF_COMMUNITY_PATH + (f\"i3{page_no}/\" if page_no > 1 else \"\")\n",
    "            url = urljoin(ESF_BASE, page_path)\n",
    "            \n",
    "            # 解析页面并提取数据\n",
    "            soup = get_soup_selenium(url, driver)\n",
    "            page_items, _ = parse_sale_list(soup)\n",
    "            \n",
    "            yield page_no, page_items\n",
    "            time.sleep(delay_sec)  # 爬取间隔，避免反爬\n",
    "    finally:\n",
    "        driver.quit()  # 确保驱动关闭\n",
    "\n",
    "\n",
    "def write_sales_to_csv(items: List[SaleItem], filepath: str) -> None:\n",
    "    \"\"\"将二手房数据写入CSV文件\"\"\"\n",
    "    with open(filepath, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        # CSV表头\n",
    "        writer.writerow([\"area_sqm（平方米）\", \"total_price_wan（万元）\", \"unit_price（元/平方米）\"])\n",
    "        # 写入数据（总价转换为元，保留整数）\n",
    "        for it in items:\n",
    "            writer.writerow([it.area_sqm, int(round(it.total_price_wan * 10000)), it.unit_price])\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"主函数：执行二手房抓取并保存到CSV\"\"\"\n",
    "    keyword = \"世纪城\"\n",
    "    district = \"海淀\"\n",
    "    max_pages = 20  # 最大抓取页数\n",
    "    output_csv = \"shijicheng_esf.csv\"  # 输出CSV文件名\n",
    "\n",
    "    print(f\"开始抓取 {district}-{keyword} 二手房数据（来源：{ESF_BASE}）...\")\n",
    "    sales_all: List[SaleItem] = []\n",
    "    \n",
    "    # 批量抓取多页数据\n",
    "    for page_no, items in crawl_sales_pages(keyword, district, max_pages=max_pages):\n",
    "        print(f\"第{page_no}页：抓取到 {len(items)} 条房源\")\n",
    "        sales_all.extend(items)\n",
    "    \n",
    "    # 保存到CSV\n",
    "    write_sales_to_csv(sales_all, output_csv)\n",
    "    print(f\"\\n抓取完成！共获取 {len(sales_all)} 条二手房数据，已保存至 {output_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea6d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在采集第 01 页 …\n",
      "正在采集第 02 页 …\n",
      "正在采集第 03 页 …\n",
      "正在采集第 04 页 …\n",
      "正在采集第 05 页 …\n",
      "正在采集第 06 页 …\n",
      "正在采集第 07 页 …\n",
      "正在采集第 08 页 …\n",
      "正在采集第 09 页 …\n",
      "正在采集第 10 页 …\n",
      "正在采集第 11 页 …\n",
      "正在采集第 12 页 …\n",
      "正在采集第 13 页 …\n",
      "正在采集第 14 页 …\n",
      "正在采集第 15 页 …\n",
      "正在采集第 16 页 …\n",
      "正在采集第 17 页 …\n",
      "正在采集第 18 页 …\n",
      "正在采集第 19 页 …\n",
      "正在采集第 20 页 …\n",
      "全部完成，共采集 1200 条，已写入 c:\\Users\\lenovo\\Desktop\\2025\\KE\\Quent\\Quant_RUC-main\\Homework\\hlg_fang_rent.csv\n",
      "生成的文件包含以下字段：面积(㎡)、月租金(元)\n"
     ]
    }
   ],
   "source": [
    "import re, time, csv, random, os\n",
    "from pathlib import Path\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# ----------------- 参数区 -----------------\n",
    "START_URL   = \"https://zu.fang.com/house-a015277-b03115/\"\n",
    "MAX_PAGE    = 20\n",
    "CSV_FILE    = \"hlg_fang_rent.csv\"\n",
    "# 使用与第一段代码相同的Edge驱动路径\n",
    "DRIVER_DIR = r\"C:\\Users\\lenovo\\Desktop\\2025\\KE\\Quent\\edgedriver_win64\"\n",
    "DRIVER_PATH = os.path.join(DRIVER_DIR, \"msedgedriver.exe\")\n",
    "# -------------------------------------------\n",
    "\n",
    "# -------------- 浏览器初始化 --------------\n",
    "def init_driver():\n",
    "    # 使用Edge浏览器配置\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.use_chromium = True\n",
    "    # 反反爬设置\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    # 可根据需要注释/取消注释无头模式\n",
    "    # options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    # 验证驱动是否存在\n",
    "    if not os.path.exists(DRIVER_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"未找到 Edge 驱动：{DRIVER_PATH}\\n\"\n",
    "            \"请确认已解压并提供正确路径（需要 msedgedriver.exe）\"\n",
    "        )\n",
    "    \n",
    "    # 使用Edge驱动服务\n",
    "    service = EdgeService(executable_path=DRIVER_PATH)\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    \n",
    "    # 保持反检测设置\n",
    "    driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "        \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
    "    })\n",
    "    \n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "# -------------- 单页解析（XPath 版）--------------\n",
    "def parse_one_page(driver):\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    # 1. 等待列表容器出现\n",
    "    wait.until(EC.presence_of_element_located(\n",
    "        (By.XPATH, '//*[@id=\"listBox\"]//div[contains(@class,\"houseList\")]')))\n",
    "\n",
    "    # 2. 取所有房源行\n",
    "    rows = driver.find_elements(By.XPATH, '//*[@id=\"listBox\"]//dd')\n",
    "    data = []\n",
    "    for dl in rows:\n",
    "        try:\n",
    "            # 月租金\n",
    "            price_txt = dl.find_element(By.XPATH, './/span[contains(@class,\"price\")]').text\n",
    "            price = int(re.search(r'(\\d+)', price_txt.replace(',', '')).group(1))\n",
    "\n",
    "            # 面积\n",
    "            info_txt = dl.find_element(By.XPATH, './/p[contains(@class,\"font15\") or contains(@class,\"room\")]').text\n",
    "            m = re.search(r'([\\d.]+)\\s*㎡', info_txt)\n",
    "            area = float(m.group(1)) if m else None  # 容错处理\n",
    "\n",
    "            # 只保留面积和月租两个字段\n",
    "            data.append({\"面积(㎡)\": area, \"月租金(元)\": price})\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] 丢弃一行：\", e)\n",
    "    return data\n",
    "\n",
    "# -------------- 主流程 --------------\n",
    "def main():\n",
    "    driver = init_driver()\n",
    "    driver.get(START_URL)\n",
    "    all_records = []\n",
    "\n",
    "    for page in range(1, MAX_PAGE + 1):\n",
    "        print(f\"正在采集第 {page:02d} 页 …\")\n",
    "        try:\n",
    "            all_records += parse_one_page(driver)\n",
    "            if page == MAX_PAGE:\n",
    "                break\n",
    "            # 点击“下一页”\n",
    "            next_btn = driver.find_element(By.XPATH, '//a[contains(text(),\"下一页\")]')\n",
    "            next_btn.click()\n",
    "            time.sleep(random.uniform(1.2, 2.4))  # 随机延迟避免反爬\n",
    "        except TimeoutException:\n",
    "            print(\"[ERROR] 加载下一页超时，提前结束\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] 未知异常：\", e)\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    # 保存 csv\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(all_records).to_csv(CSV_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"全部完成，共采集 {len(all_records)} 条，已写入 {Path(CSV_FILE).absolute()}\")\n",
    "    print(f\"生成的文件包含以下字段：面积(㎡)、月租金(元)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
