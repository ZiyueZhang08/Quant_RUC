{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.base import clone \n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "from geopy.distance import geodesic # 用于计算地理距离\n",
    "from sklearn.cluster import KMeans # 用于地理聚类\n",
    "from sklearn.preprocessing import KBinsDiscretizer # 用于特征分箱\n",
    "import cn2an\n",
    "import warnings\n",
    "\n",
    "# 设置环境\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义处理price各列的辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 辅助函数：特征工程 ---\n",
    "\n",
    "def handle_region(df, rare_threshold=0.03):\n",
    "    \"\"\"\n",
    "    处理 '区域' 列:\n",
    "    将出现频率低于 threshold 的类别合并为 '其他区域'\n",
    "    \"\"\"\n",
    "    print(\"处理 [区域]...\")\n",
    "    value_counts = df['区域'].value_counts(normalize=True)\n",
    "    rare_regions = value_counts[value_counts <= rare_threshold].index\n",
    "    \n",
    "    # 替换稀有类别\n",
    "    df['区域'] = df['区域'].replace(rare_regions, '其他区域')\n",
    "    print(f\"合并了 {len(rare_regions)} 个稀有区域为 '其他区域'\")\n",
    "    return df\n",
    "\n",
    "def handle_ring_road(df):\n",
    "    \"\"\"\n",
    "    处理 '环线' 列:\n",
    "    1. 填充NaN为 '未知'\n",
    "    2. 进行有序编码\n",
    "    \"\"\"\n",
    "    print(\"处理 [环线]...\")\n",
    "    df['环线'] = df['环线'].fillna('未知')\n",
    "    \n",
    "    # 定义环线的有序映射\n",
    "\n",
    "    ring_map = {\n",
    "        '二环内': 1,\n",
    "        '二至三环': 1,\n",
    "        '三至四环': 2,\n",
    "        '四至五环': 2, \n",
    "        '五至六环': 3,\n",
    "        '六环外': 4,\n",
    "        '内环内':1,\n",
    "        '内环至中环':2,\n",
    "        '中环至外环':3,\n",
    "        '内环至外环':3,\n",
    "        '外环外':4,\n",
    "        '未知': 4\n",
    "    }\n",
    "    \n",
    "    # 使用 .map() 进行映射，未在map中出现的值用 5 填充\n",
    "    df['环线_ordinal'] = df['环线'].map(ring_map).fillna(5).astype(int)\n",
    "    \n",
    "    # 删除原始列\n",
    "    df = df.drop('环线', axis=1)\n",
    "    return df\n",
    "# --- 户型解析 ---\n",
    "def handle_house_type(df, col_name, n_train): # 添加 n_train 参数\n",
    "    \"\"\"\n",
    "    解析'房屋户型'列，提取'室', '厅', '卫'的数量。 (来自 demo.ipynb)\n",
    "    使用训练集的中位数填充。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(f\"  处理 [{col_name}]...\")\n",
    "\n",
    "    df[col_name] = df[col_name].astype(str).str.replace('房间', '室')\n",
    "    layout_info = df[col_name].str.extract(r'(\\d+)[室](?:(\\d+)厅)?(?:.*?(\\d+)卫)?', expand=True)\n",
    "    layout_info.columns = ['室', '厅', '卫']\n",
    "    df = pd.concat([df, layout_info], axis=1)\n",
    "\n",
    "    for col in ['室', '厅', '卫']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    print(f\"    正在计算 '室', '厅', '卫' 的中位数 (基于前 {n_train} 行)...\")\n",
    "    train_part = df.iloc[:n_train]\n",
    "    medians = {}\n",
    "    for col in ['室', '厅', '卫']:\n",
    "        if col in df.columns:\n",
    "            median_val = train_part[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                if col == '室': median_val = 2.0\n",
    "                elif col == '厅': median_val = 1.0\n",
    "                else: median_val = 1.0\n",
    "            print(f\"      '{col}' 中位数: {median_val}\")\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            medians[col] = median_val\n",
    "\n",
    "    for col in ['室', '厅', '卫']:\n",
    "        df[col] = df[col].fillna(medians[col])\n",
    "        # 0厅是合理的，0室0卫通常不合理，用中位数填充\n",
    "        if col != '厅':\n",
    "             df.loc[df[col] <= 0, col] = medians[col]\n",
    "\n",
    "    df = df.drop(columns=[col_name], errors='ignore')\n",
    "    print(f\"    已提取 '室', '厅', '卫' 特征并填充缺失值。\")\n",
    "    return df\n",
    "\n",
    "# --- 楼层处理 ---\n",
    "def handle_floor(df, n_train, col='所在楼层'):\n",
    "    \"\"\"\n",
    "    解析楼层列，提取'总楼层'和'楼层类别'。 (来自 demo.ipynb)\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    print(f\"  处理 [{col}]...\")\n",
    "\n",
    "    if col not in df_processed.columns:\n",
    "        print(f\"    错误：列 '{col}' 不在DataFrame中，无法处理楼层。\")\n",
    "        return df\n",
    "\n",
    "    print(\"    提取总楼层和当前楼层...\")\n",
    "    total_floors1 = df_processed[col].astype(str).str.extract(r'\\(共(\\d+)层\\)', expand=False)\n",
    "    total_floors2 = df_processed[col].astype(str).str.extract(r'/(\\d+)(?:层|\\))?$', expand=False)\n",
    "    current_floor_ext = df_processed[col].astype(str).str.extract(r'^(\\d+)/', expand=False)\n",
    "    df_processed['总楼层'] = pd.to_numeric(total_floors1.fillna(total_floors2), errors='coerce')\n",
    "    df_processed['当前楼层_temp'] = pd.to_numeric(current_floor_ext, errors='coerce')\n",
    "\n",
    "    print(\"    提取明确的楼层类别...\")\n",
    "    df_processed['楼层类别_explicit'] = np.nan\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('高楼层', na=False), '楼层类别_explicit'] = '高楼层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('中楼层', na=False), '楼层类别_explicit'] = '中楼层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('低楼层', na=False), '楼层类别_explicit'] = '低楼层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('顶层', na=False), '楼层类别_explicit'] = '顶层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('底层', na=False), '楼层类别_explicit'] = '底层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('地下', na=False), '楼层类别_explicit'] = '地下室'\n",
    "\n",
    "    print(\"    推断楼层类别...\")\n",
    "    df_processed['楼层类别_inferred'] = np.nan\n",
    "    infer_mask = (df_processed['楼层类别_explicit'].isna()) & (df_processed['当前楼层_temp'].notna()) & (df_processed['总楼层'].notna()) & (df_processed['总楼层'] > 0)\n",
    "    if infer_mask.any():\n",
    "        current = df_processed.loc[infer_mask, '当前楼层_temp']\n",
    "        total = df_processed.loc[infer_mask, '总楼层']\n",
    "        ratio = current / total\n",
    "        df_processed.loc[infer_mask, '楼层类别_inferred'] = '中楼层'\n",
    "        df_processed.loc[infer_mask & (ratio <= 1/3), '楼层类别_inferred'] = '低楼层'\n",
    "        df_processed.loc[infer_mask & (ratio >= 2/3), '楼层类别_inferred'] = '高楼层'\n",
    "        df_processed.loc[infer_mask & (current == total), '楼层类别_inferred'] = '顶层'\n",
    "        df_processed.loc[infer_mask & (current == 1), '楼层类别_inferred'] = '底层'\n",
    "\n",
    "    print(\"    合并类别并填充缺失值...\")\n",
    "    df_processed['楼层类别'] = df_processed['楼层类别_explicit'].fillna(df_processed['楼层类别_inferred'])\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    median_total_floors_train = train_part['总楼层'].median()\n",
    "    if pd.isna(median_total_floors_train): median_total_floors_train = 18.0\n",
    "    mode_floor_category_train = train_part['楼层类别'].mode()\n",
    "    fill_category_train = mode_floor_category_train[0] if not mode_floor_category_train.empty else '中楼层'\n",
    "\n",
    "    original_na_total_floors = df_processed['总楼层'].isnull().sum()\n",
    "    if original_na_total_floors > 0:\n",
    "        df_processed['总楼层'].fillna(median_total_floors_train, inplace=True)\n",
    "        print(f\"      填充了 '总楼层' 列的 {original_na_total_floors} 个缺失值 (使用训练集中位数 {median_total_floors_train:.0f})。\")\n",
    "    original_na_category = df_processed['楼层类别'].isnull().sum()\n",
    "    if original_na_category > 0:\n",
    "        df_processed['楼层类别'].fillna(fill_category_train, inplace=True)\n",
    "        print(f\"      填充了 '楼层类别' 列的 {original_na_category} 个缺失值 (使用训练集众数 '{fill_category_train}')。\")\n",
    "    df_processed['总楼层'] = pd.to_numeric(df_processed['总楼层'], errors='coerce').fillna(median_total_floors_train)\n",
    "\n",
    "    print(\"    进行独热编码并清理临时列...\")\n",
    "    cols_to_drop = [col, '当前楼层_temp', '楼层类别_explicit', '楼层类别_inferred']\n",
    "    df_processed = df_processed.drop(columns=cols_to_drop, errors='ignore')\n",
    "    df_processed = pd.get_dummies(df_processed, columns=['楼层类别'], prefix='楼层', drop_first=False, dummy_na=False)\n",
    "    print(\"    楼层信息处理完成。\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_building_structure(df):\n",
    "    \"\"\"\n",
    "    处理 '建筑结构' 列:\n",
    "    \"\"\"\n",
    "    print(\"处理 [建筑结构]...\")\n",
    "\n",
    "    structure_col = df['建筑结构'].copy()\n",
    "\n",
    "    structure_col = structure_col.fillna('未知结构')\n",
    "    structure_col = structure_col.replace('', '未知结构')\n",
    "    \n",
    "    # 2. 基于数据分布进行智能分组\n",
    "    def group_building_structure(struct):\n",
    "        if pd.isna(struct) or struct in ['', '未知结构', '（空白）']:\n",
    "            return '未知结构'\n",
    "        elif struct == '钢混结构':\n",
    "            return '钢混结构'  # 最多的一类，单独保留\n",
    "        elif struct in ['混合结构', '砖混结构']:\n",
    "            return '混合砖混类'  # 中等数量的两类合并\n",
    "        elif struct in ['框架结构', '钢结构']:\n",
    "            return '框架钢构类' \n",
    "        elif struct == '砖木结构':\n",
    "            return '其他稀有结构'  # 数量极少，归为其他\n",
    "        else:\n",
    "            return '其他结构'  \n",
    "    \n",
    "    df['建筑结构分组'] = structure_col.apply(group_building_structure)\n",
    "    \n",
    "    # 3. 创建结构稳定性特征,基于工程知识, 数值越高表示结构越稳定/现代\n",
    "    structure_stability_map = {\n",
    "        '钢混结构': 5,      # 最稳定现代\n",
    "        '框架钢构类': 4,     # 现代结构\n",
    "        '混合砖混类': 3,     # 中等稳定性\n",
    "        '未知结构': 2,       # 设为中性\n",
    "        '其他结构': 2,       # 设为中性\n",
    "        '其他稀有结构': 1    # 传统/老旧结构\n",
    "    }\n",
    "    \n",
    "    df['结构稳定性评分'] = df['建筑结构分组'].map(structure_stability_map)\n",
    "    \n",
    "    # 创建结构安全性特征（基于建筑规范）数值越高表示安全性越好\n",
    "    structure_safety_map = {\n",
    "        '钢混结构': 3,        # 安全性好\n",
    "        '框架钢构类': 3,       # 安全性好\n",
    "        '混合砖混类': 2,       # 安全性中等\n",
    "        '未知结构': 1,         # 设为保守值\n",
    "        '其他结构': 1,         # 设为保守值\n",
    "        '其他稀有结构': 1      # 传统结构安全性相对较低\n",
    "    }\n",
    "    \n",
    "    df['结构安全性评分'] = df['建筑结构分组'].map(structure_safety_map)\n",
    "    df['建筑结构'] = structure_col\n",
    "    \n",
    "    print(f\"建筑结构分组分布: {df['建筑结构分组'].value_counts().to_dict()}\")\n",
    "    return df\n",
    "\n",
    "def handle_area(df, n_train):\n",
    "    \"\"\"\n",
    "    专门处理建筑面积和套内面积列，并创建得房率特征\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    print(f\"  处理 [建筑面积] 和 [套内面积]...\")\n",
    "\n",
    "    print(\"    清理面积单位并转为数值...\")\n",
    "    for col in ['建筑面积', '套内面积']:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col].astype(str).str.replace('㎡', '').str.strip(), errors='coerce')\n",
    "            df_processed.loc[df_processed[col] < 5, col] = np.nan # 小于5平米视为异常\n",
    "\n",
    "    print(f\"    计算训练集 (前 {n_train} 行) 平均得房率...\")\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    valid_train_data = train_part[\n",
    "        (train_part['套内面积'].notna()) & (train_part['套内面积'] > 0) &\n",
    "        (train_part['建筑面积'].notna()) & (train_part['建筑面积'] > 0) &\n",
    "        (train_part['套内面积'] <= train_part['建筑面积'])\n",
    "    ].copy()\n",
    "\n",
    "    efficiency_rate_train = 0.8\n",
    "    if not valid_train_data.empty and valid_train_data['建筑面积'].sum() > 0:\n",
    "        efficiency_rate_train = valid_train_data['套内面积'].sum() / valid_train_data['建筑面积'].sum()\n",
    "        efficiency_rate_train = np.clip(efficiency_rate_train, 0.6, 1.0)\n",
    "    print(f\"      计算得到的训练集平均得房率: {efficiency_rate_train:.4f}\")\n",
    "\n",
    "    print(\"    估算/修正 '套内面积'...\")\n",
    "    impute_mask = (\n",
    "        (df_processed['套内面积'].isna()) |\n",
    "        (df_processed['套内面积'] / df_processed['建筑面积'] < 0.5) |\n",
    "        (df_processed['套内面积'] / df_processed['建筑面积'] > 1.0)\n",
    "    ) & df_processed['建筑面积'].notna() & (df_processed['建筑面积'] > 0)\n",
    "    rows_to_impute = impute_mask.sum()\n",
    "    if rows_to_impute > 0:\n",
    "        df_processed.loc[impute_mask, '套内面积'] = df_processed.loc[impute_mask, '建筑面积'] * efficiency_rate_train\n",
    "        print(f\"      使用得房率估算了 {rows_to_impute} 行的 '套内面积'\")\n",
    "\n",
    "    print(\"    使用中位数填充 '建筑面积' 缺失值...\")\n",
    "    median_building_area = train_part['建筑面积'].median()\n",
    "    if pd.isna(median_building_area): median_building_area = 90.0\n",
    "    df_processed['建筑面积'].fillna(median_building_area, inplace=True)\n",
    "    print(f\"      建筑面积中位数: {median_building_area:.2f}\")\n",
    "\n",
    "    print(\"    再次检查并填充 '套内面积' 缺失值...\")\n",
    "    final_impute_mask = df_processed['套内面积'].isna() & df_processed['建筑面积'].notna() & (df_processed['建筑面积'] > 0)\n",
    "    rows_to_impute_final = final_impute_mask.sum()\n",
    "    if rows_to_impute_final > 0:\n",
    "         df_processed.loc[final_impute_mask, '套内面积'] = df_processed.loc[final_impute_mask, '建筑面积'] * efficiency_rate_train\n",
    "         print(f\"      补充估算了 {rows_to_impute_final} 行的 '套内面积'\")\n",
    "\n",
    "    print(\"    创建 '得房率' 特征...\")\n",
    "    df_processed['得房率'] = np.where(\n",
    "        df_processed['建筑面积'] > 0,\n",
    "        df_processed['套内面积'] / df_processed['建筑面积'],\n",
    "        np.nan\n",
    "    )\n",
    "    median_efficiency = df_processed.iloc[:n_train]['得房率'].median()\n",
    "    if pd.isna(median_efficiency) or median_efficiency <= 0.5 or median_efficiency > 1.0:\n",
    "        median_efficiency = efficiency_rate_train\n",
    "    df_processed['得房率'].fillna(median_efficiency, inplace=True)\n",
    "    df_processed['得房率'] = df_processed['得房率'].clip(0.5, 1.0)\n",
    "    print(f\"      最终 '得房率' 使用的中位数/填充值: {median_efficiency:.4f}\")\n",
    "\n",
    "    # 保留套内面积，因为可能有用\n",
    "    # df_processed = df_processed.drop(columns=['套内面积'], errors='ignore')\n",
    "    return df_processed\n",
    "\n",
    "def handle_orientation(df):\n",
    "    \"\"\"\n",
    "    处理 '房屋朝向' 列:\n",
    "    进行 Multi-Hot 编码\n",
    "    \"\"\"\n",
    "    print(\"处理 [房屋朝向]...\")\n",
    "    df['房屋朝向'] = df['房屋朝向'].fillna('未知')\n",
    "\n",
    "    df['is_朝南'] = df['房屋朝向'].str.contains('南').astype(int)\n",
    "    df['is_朝东'] = df['房屋朝向'].str.contains('东').astype(int)\n",
    "    df['is_朝西'] = df['房屋朝向'].str.contains('西').astype(int)\n",
    "    df['is_朝北'] = df['房屋朝向'].str.contains('北').astype(int)\n",
    "    \n",
    "    df = df.drop('房屋朝向', axis=1)\n",
    "    return df\n",
    "\n",
    "def process_structure(df, structure_col, fillna_value, prefix):\n",
    "    \"\"\"\n",
    "    处理列，填充缺失值并将其转换为虚拟变量。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    print(f\"  处理 [{structure_col}] (转为 dummies)...\")\n",
    "    if structure_col not in df_processed.columns:\n",
    "        print(f\"    警告: 列 '{structure_col}' 不存在，跳过。\")\n",
    "        return df_processed\n",
    "\n",
    "    df_processed[structure_col].fillna(fillna_value, inplace=True)\n",
    "    structure_dummies = pd.get_dummies(df_processed[structure_col], prefix=prefix, dummy_na=False)\n",
    "    df_processed = pd.concat([df_processed, structure_dummies], axis=1)\n",
    "    df_processed = df_processed.drop(columns=[structure_col], errors='ignore')\n",
    "    print(f\"    已将 '{structure_col}' 转换为 dummies，前缀为 '{prefix}'。\")\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_decoration(df):\n",
    "    \"\"\"\n",
    "    处理 '装修情况' 列 :\n",
    "    填充 NaN 值为 '未知'\n",
    "    \"\"\"\n",
    "    print(\"处理 [装修情况]...\")\n",
    "    if '装修情况' not in df.columns:\n",
    "        print(\"警告: 列 '装修情况' 不存在。\")\n",
    "        return df\n",
    "        \n",
    "    # 填充 NaN \n",
    "    df['装修情况'] = df['装修情况'].fillna('未知')    \n",
    "    # 打印唯一值以供检查\n",
    "    print(f\"  处理后 '装修情况' 的唯一值: {df['装修情况'].unique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_elevator_ratio(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '梯户比例' 列:\n",
    "    ... (其他不变) ...\n",
    "    \"\"\"\n",
    "    print(\"处理 [梯户比例]...\")\n",
    "    \n",
    "    if '梯户比例' not in df.columns:\n",
    "        print(\"警告: 列 '梯户比例' 不存在。\")\n",
    "        return df\n",
    "        \n",
    "    df['梯户比例_clean'] = df['梯户比例'].fillna('')\n",
    "    df['梯户比例_clean'] = df['梯户比例_clean'].replace(r'^\\\\\\\\s*、\\\\\\\\s*$', '', regex=True)\n",
    "    \n",
    "    def extract_ratio(ratio_str):\n",
    "        try:\n",
    "            match = re.search(r'(\\\\S+?)梯\\\\s*(\\\\S+?)户', ratio_str)\n",
    "            if match:\n",
    "                ele_str, hh_str = match.groups()\n",
    "                elevator = cn2an.cn2an(ele_str.strip(), \"smart\")\n",
    "                household = cn2an.cn2an(hh_str.strip(), \"smart\")\n",
    "                return float(elevator), float(household)\n",
    "        except:\n",
    "            pass\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    ratios = df['梯户比例_clean'].apply(extract_ratio)\n",
    "    df['电梯数'] = ratios.apply(lambda x: x[0])\n",
    "    df['每层户数'] = ratios.apply(lambda x: x[1])\n",
    "    \n",
    "    df['户梯比'] = df['每层户数'] / df['电梯数']\n",
    "    \n",
    "    df['户梯比'] = df['户梯比'].replace([np.inf, -np.inf, 0], np.nan)\n",
    "    \n",
    "    # 仅使用训练集计算中位数\n",
    "    train_part = df.iloc[:n_train]\n",
    "    valid_ratios_train = train_part['户梯比'].dropna()\n",
    "    median_ratio = valid_ratios_train.median() if not valid_ratios_train.empty else 2.0\n",
    "    median_elevator_train = train_part['电梯数'].median()\n",
    "    median_household_train = train_part['每层户数'].median()\n",
    "\n",
    "    df['户梯比'] = df['户梯比'].fillna(median_ratio)\n",
    "    df['电梯数'] = df['电梯数'].fillna(median_elevator_train).replace(0, 1)\n",
    "    df['每层户数'] = df['每层户数'].fillna(median_household_train)\n",
    "    \n",
    "    df['高密度标志'] = (df['户梯比'] > 5).astype(int)\n",
    "    df['低密度标志'] = (df['户梯比'] < 2).astype(int)\n",
    "    \n",
    "    df = df.drop(['梯户比例', '梯户比例_clean'], axis=1, errors='ignore')\n",
    "    \n",
    "    print(f\"户梯比中位数 (基于训练集): {median_ratio:.2f}\")\n",
    "    return df\n",
    "\n",
    "def handle_villa_type(df):\n",
    "    \"\"\"\n",
    "    处理 '别墅类型' 列\n",
    "    绝大多数为NaN\n",
    "    \"\"\"\n",
    "    col = '别墅类型'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "    # 填充 NaN\n",
    "    df[col] = df[col].fillna('非别墅')\n",
    "    \n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_property_use(df, rare_threshold=0.005): # 0.5% 阈值\n",
    "    \"\"\"\n",
    "    处理 '房屋用途' 列 \n",
    "    1. 替换 NaN 为 '未知'\n",
    "    2. 合并同义词/相似类别\n",
    "    3. 合并低于 threshold 的稀有类别为 '其他'\n",
    "    \"\"\"\n",
    "    col = '房屋用途'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    # 1. 填充 NaN \n",
    "    df[col] = df[col].fillna('未知')\n",
    "\n",
    "    # 2. 合并同义词 \n",
    "    # 2a. 合并所有 \"公寓\" (公寓, 公寓(住宅), 公寓/公寓, 酒店式公寓, 商务公寓等)\n",
    "    # 使用正则表达式 r'.*公寓.*' 匹配所有包含 '公寓' 的条目\n",
    "    df[col] = df[col].replace(r'.*公寓.*', '公寓', regex=True)\n",
    "    \n",
    "    # 2b. 合并所有 \"商用\"\n",
    "    commercial_list = [\n",
    "        '商业办公类', \n",
    "        '商住两用', \n",
    "        '商业', \n",
    "        '写字楼',\n",
    "        '底商' # 底商也归为商用\n",
    "    ]\n",
    "    df[col] = df[col].replace(commercial_list, '商用')\n",
    "    \n",
    "    # 3. 合并稀有类别\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    rare_values = value_counts[value_counts < rare_threshold].index\n",
    "    rare_values = rare_values.drop('其他', errors='ignore') \n",
    "    \n",
    "    if len(rare_values) > 0:\n",
    "        print(f\"  在 '{col}' 中, 合并 {len(rare_values)} 个稀有类别为 '其他': {rare_values.tolist()}\")\n",
    "        df[col] = df[col].replace(rare_values, '其他')\n",
    "    \n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "def handle_transaction_ownership(df, rare_threshold=0.005): # 0.5% 阈值\n",
    "    \"\"\"\n",
    "    处理 '交易权属' 列 \n",
    "    1. 替换 NaN为 '未知'\n",
    "    2. 合并同义词/相似类别\n",
    "    3. 合并低于 threshold 的稀有类别为 '其他'\n",
    "    \"\"\"\n",
    "    col = '交易权属'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    # 1. 填充 NaN \n",
    "    df[col] = df[col].fillna('未知')\n",
    "    # 2. 合并同义词\n",
    "    # 合并 \"安置房\"\n",
    "    df[col] = df[col].replace(['动迁安置房', '拆迁还建房', '回迁房', '定向安置房'], '安置房')\n",
    "    # 合并 \"经济适用房\"\n",
    "    df[col] = df[col].replace(['二类经济适用房', '经济适用房', '限价商品房', '一类经济适用房'], '经济适用房')\n",
    "    # 合并 \"政策房\"\n",
    "    df[col] = df[col].replace(['已购公房', '房改房', '售后公房', '央产房'], '政策房')\n",
    "    \n",
    "    # 3. 合并稀有类别\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    rare_values = value_counts[value_counts < rare_threshold].index\n",
    "    \n",
    "    if len(rare_values) > 0:\n",
    "        print(f\"  在 '{col}' 中, 合并 {len(rare_values)} 个稀有类别为 '其他'\")\n",
    "        df[col] = df[col].replace(rare_values, '其他')\n",
    "        \n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_house_advantages(df):\n",
    "    \"\"\"\n",
    "    处理 '房屋优势' 列:\n",
    "    多标签文本字段。\n",
    "    1. 清洗 (NaN, '、') 为空字符串\n",
    "    2. 提取 'is_Adv_地铁' (二元特征)。\n",
    "    3. 提取 'is_Adv_装修' (二元特征)。\n",
    "    4. 提取 'Adv_Tenure_Ordinal' (有序特征)，用于后续合并：\n",
    "       - 3: 房本满五年\n",
    "       - 2: 房本满两年\n",
    "       - 0: 未提及或未知\n",
    "    \"\"\"\n",
    "    col = '房屋优势'\n",
    "    col_clean = '房屋优势_clean' \n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    # 1. 清洗 \n",
    "    df[col_clean] = df[col].fillna('')\n",
    "    df[col_clean] = df[col_clean].replace(r'^\\s*、\\s*$', '', regex=True)\n",
    "    \n",
    "    # 2. 提取 'is_Adv_地铁'\n",
    "    df['is_Adv_地铁'] = df[col_clean].str.contains('地铁').astype(int)\n",
    "    \n",
    "    # 3. 提取 'is_Adv_装修'\n",
    "    df['is_Adv_装修'] = df[col_clean].str.contains('装修').astype(int)\n",
    "    \n",
    "    # 4. 提取 'Adv_Tenure_Ordinal' \n",
    "    conditions = [\n",
    "        df[col_clean].str.contains('房本满五年'),\n",
    "        df[col_clean].str.contains('房本满两年')\n",
    "    ]\n",
    "    choices = [\n",
    "        3,  # 满五年\n",
    "        2   # 满两年\n",
    "    ]\n",
    "    # 默认值为 0 \n",
    "    df['Adv_Tenure_Ordinal'] = np.select(conditions, choices, default=0)\n",
    "    \n",
    "    print(f\"  提取了 'is_Adv_地铁', 'is_Adv_装修', 'Adv_Tenure_Ordinal'\")\n",
    "    df = df.drop([col, col_clean], axis=1, errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_developer(df):\n",
    "    \"\"\"\n",
    "    处理 '开发商' 列:\n",
    "    将此列转换为一个二元特征 'has_Developer' (1=有, 0=无)。\n",
    "    \"\"\"\n",
    "    col = '开发商'\n",
    "    new_col = 'has_Developer'\n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    # 1. 定义所有代表 \"无开发商\" 的字符串\n",
    "    no_developer_list = [\n",
    "        '无', \n",
    "        '无开发公司', \n",
    "        '无开发商', \n",
    "        '暂无信息', \n",
    "        '暂无资料'\n",
    "    ]\n",
    "\n",
    "    # 2. 填充 NaN\n",
    "    df[col] = df[col].fillna('无')\n",
    "\n",
    "    df[new_col] = (~df[col].isin(no_developer_list)).astype(int)\n",
    "\n",
    "    df = df.drop(col, axis=1)\n",
    "    print(f\"  创建了新特征 '{new_col}'\")\n",
    "    return df\n",
    "\n",
    "def handle_property_management(df):\n",
    "    \"\"\"\n",
    "    处理 '物业公司' 列:\n",
    "    将此列转换为一个二元特征 'has_PropertyMgmt' (1=有, 0=无)。\n",
    "    \"\"\"\n",
    "    col = '物业公司'\n",
    "    new_col = 'has_PropertyMgmt'\n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    # 1. 定义所有代表 \"无物业\" 的字符串\n",
    "    no_management_list = [\n",
    "        '无', \n",
    "        '无物业', \n",
    "        '无物业管理', \n",
    "        '无物业管理服务', \n",
    "        '暂时无物业公司'\n",
    "    ]\n",
    "\n",
    "    # 2. 填充 NaN。\n",
    "    df[col] = df[col].fillna('无物业')\n",
    "    df[new_col] = (~df[col].isin(no_management_list)).astype(int)\n",
    "\n",
    "    df = df.drop(col, axis=1)\n",
    "    print(f\"  创建了新特征 '{new_col}'\")\n",
    "    return df\n",
    "\n",
    "def handle_district(df):\n",
    "    \"\"\"\n",
    "    处理 '区县' 列 数字型分类特征。\n",
    "    1. 填充 7% 的缺失值 (NaN) 为 '未知' 类别。\n",
    "    2. 将整列转换为 'string' 类型，以防止模型将其误认为连续数值。\n",
    "    后面使用目标编码 (Target Encoding)\n",
    "    \"\"\"\n",
    "    col = '区县'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    df[col] = df[col].fillna('未知')\n",
    "    df[col] = df[col].astype(str)\n",
    "    \n",
    "    print(f\"  已将 '{col}' 填充缺失值并转换为 'string' 类型。\")\n",
    "    print(f\"  处理后 '{col}' 的唯一值 (示例): {df[col].unique()[:10]}\")\n",
    "    return df\n",
    "\n",
    "def clean_numeric_str(s):\n",
    "    \"\"\"\n",
    "    辅助函数：从 '1317户' 或 '19栋' 这样的字符串中提取数字。\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    match = re.search(r'(\\d+)', str(s))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def handle_community_stats(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '房屋总数' 和 '楼栋总数':\n",
    "    \"\"\"\n",
    "    houses_col = '房屋总数'\n",
    "    buildings_col = '楼栋总数'\n",
    "    interaction_col = 'avg_units_per_building' \n",
    "    \n",
    "    print(f\"处理 [{houses_col}] 和 [{buildings_col}]...\")\n",
    "    \n",
    "    if houses_col not in df.columns or buildings_col not in df.columns:\n",
    "        print(f\"警告: 缺少 '{houses_col}' 或 '{buildings_col}'。\")\n",
    "        return df\n",
    "\n",
    "    df[houses_col] = df[houses_col].apply(clean_numeric_str)\n",
    "    df[buildings_col] = df[buildings_col].apply(clean_numeric_str)\n",
    "    \n",
    "    temp_buildings = df[buildings_col].replace(0, np.nan)\n",
    "    df[interaction_col] = df[houses_col] / temp_buildings\n",
    "    \n",
    "    # 仅使用训练集计算中位数\n",
    "    train_part = df.iloc[:n_train]\n",
    "    houses_median = train_part[houses_col].median()\n",
    "    buildings_median = train_part[buildings_col].median()\n",
    "    interaction_median = train_part[interaction_col].median() \n",
    "\n",
    "\n",
    "    df[houses_col] = df[houses_col].fillna(houses_median)\n",
    "    df[buildings_col] = df[buildings_col].fillna(buildings_median)\n",
    "    df[interaction_col] = df[interaction_col].fillna(interaction_median)\n",
    "\n",
    "    print(f\"  清洗了 '{houses_col}' (中位数: {houses_median}) 和 '{buildings_col}' (中位数: {buildings_median})。\")\n",
    "    print(f\"  创建了新的交互特征 '{interaction_col}' (中位数: {interaction_median})。\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def parse_building_year(s):\n",
    "    \"\"\"\n",
    "    辅助函数：从字符串中解析年份\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan    \n",
    "\n",
    "    s_str = str(s)\n",
    "    nums = re.findall(r'(\\d{4})', s_str)\n",
    "    \n",
    "    if len(nums) == 0:\n",
    "        return np.nan\n",
    "    elif len(nums) == 1:\n",
    "        return float(nums[0])\n",
    "    else:\n",
    "        return (float(nums[0]) + float(nums[1])) / 2\n",
    "\n",
    "def handle_building_age(df, n_train, col='建筑年代', trans_year_col='交易年份',\n",
    "                        group_col_l1='板块', group_col_l2='区域'):\n",
    "    \"\"\"\n",
    "    处理 '建筑年代' 列，计算 '房龄' 并使用多级中位数填充缺失值。\n",
    "        col (str): 包含原始建筑年代信息的列名。\n",
    "        trans_year_col (str): 包含交易年份的列名。\n",
    "        group_col_l1 (str): 第一级分组列名 ('板块')。\n",
    "        group_col_l2 (str): 第二级分组列名 ('区域')。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 处理后的数据框，包含 '房龄' 列。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy() \n",
    "    age_col = '房龄'         \n",
    "    parsed_year_col = col + '_parsed' # 临时解析年份列\n",
    "\n",
    "    print(f\"处理 [{col}]，计算 {age_col}...\")\n",
    "\n",
    "    #  检查依赖列是否存在 \n",
    "    if col not in df_processed.columns:\n",
    "        print(f\"    警告：未找到 '{col}' 列。无法计算房龄，将填充默认值 20。\")\n",
    "        df_processed[age_col] = 20\n",
    "        return df_processed\n",
    "    if trans_year_col not in df_processed.columns:\n",
    "        print(f\"    警告：未找到 '{trans_year_col}' 列，无法计算房龄，将填充默认值 20。\")\n",
    "        df_processed[age_col] = 20\n",
    "        df_processed = df_processed.drop(columns=[col], errors='ignore') # 删除原始列\n",
    "        return df_processed\n",
    "    if group_col_l1 not in df_processed.columns:\n",
    "        print(f\"    警告：未找到一级分组列 '{group_col_l1}'，L1填充将无效。\")\n",
    "        group_col_l1 = None # 设为 None 以跳过 L1 填充\n",
    "    if group_col_l2 not in df_processed.columns:\n",
    "        print(f\"    警告：未找到二级分组列 '{group_col_l2}'，L2填充将无效。\")\n",
    "        group_col_l2 = None # 设为 None 以跳过 L2 填充\n",
    "\n",
    "    #  1. 解析年份 \n",
    "    df_processed[parsed_year_col] = df_processed[col].apply(parse_building_year)\n",
    "\n",
    "    #  2. 计算房龄 \n",
    "    df_processed[age_col] = df_processed[trans_year_col] - df_processed[parsed_year_col]\n",
    "    nan_count_initial = df_processed[age_col].isnull().sum()\n",
    "    print(f\"    初步计算后 '{age_col}' 缺失值数量: {nan_count_initial}\")\n",
    "\n",
    "    #  3. 填充缺失值 (L1/L2/L3 中位数，仅基于训练集计算) \n",
    "    if nan_count_initial > 0:\n",
    "        print(f\"    使用分组中位数填充 '{age_col}' 缺失值 (基于前 {n_train} 行)...\")\n",
    "        train_part = df_processed.iloc[:n_train]\n",
    "        # 只使用房龄 >= 0 的训练集数据计算中位数\n",
    "        valid_train_age = train_part.loc[train_part[age_col] >= 0].copy()\n",
    "\n",
    "        # 计算 L1, L2, L3 填充值\n",
    "        median_map_l1 = pd.Series(dtype=float)\n",
    "        if group_col_l1 and not valid_train_age.empty:\n",
    "             try:\n",
    "                 median_map_l1 = valid_train_age.groupby(group_col_l1)[age_col].median()\n",
    "             except Exception as e:\n",
    "                 print(f\"      警告: 计算 L1 中位数图时出错: {e}\")\n",
    "\n",
    "        median_map_l2 = pd.Series(dtype=float)\n",
    "        if group_col_l2 and not valid_train_age.empty:\n",
    "             try:\n",
    "                 median_map_l2 = valid_train_age.groupby(group_col_l2)[age_col].median()\n",
    "             except Exception as e:\n",
    "                 print(f\"      警告: 计算 L2 中位数图时出错: {e}\")\n",
    "\n",
    "        global_median = valid_train_age[age_col].median() if not valid_train_age.empty else np.nan\n",
    "        fill_global = global_median if pd.notna(global_median) else 20.0 # 备用值，例如20年\n",
    "\n",
    "        # 应用 L1 填充\n",
    "        nan_mask_l1 = df_processed[age_col].isnull()\n",
    "        if nan_mask_l1.any() and group_col_l1 and not median_map_l1.empty:\n",
    "            fill_values_l1 = df_processed.loc[nan_mask_l1, group_col_l1].map(median_map_l1)\n",
    "            df_processed[age_col].fillna(fill_values_l1, inplace=True)\n",
    "\n",
    "        # 应用 L2 填充\n",
    "        nan_mask_l2 = df_processed[age_col].isnull()\n",
    "        if nan_mask_l2.any() and group_col_l2 and not median_map_l2.empty:\n",
    "            fill_values_l2 = df_processed.loc[nan_mask_l2, group_col_l2].map(median_map_l2)\n",
    "            df_processed[age_col].fillna(fill_values_l2, inplace=True)\n",
    "\n",
    "        # 应用 L3 (Global) 填充\n",
    "        nan_mask_l3 = df_processed[age_col].isnull()\n",
    "        if nan_mask_l3.any():\n",
    "            df_processed[age_col].fillna(fill_global, inplace=True)\n",
    "            print(f\" 使用全局中位数/备用值 ({fill_global:.1f}) 填充了 {nan_mask_l3.sum()} 个剩余缺失值。\")\n",
    "        print(f\"'{age_col}' 缺失值填充完成。\")\n",
    "    else:\n",
    "        print(f\"'{age_col}' 无需填充。\")\n",
    "\n",
    "    #  4. 清理异常值 (负房龄) \n",
    "    negative_age_count = (df_processed[age_col] < 0).sum()\n",
    "    if negative_age_count > 0:\n",
    "        print(f\"发现 {negative_age_count} 行负房龄，将其修正为 0。\")\n",
    "        df_processed[age_col] = np.maximum(0, df_processed[age_col])\n",
    "\n",
    "    #  5. 删除原始列和临时列 \n",
    "    df_processed = df_processed.drop(columns=[col, parsed_year_col], errors='ignore')\n",
    "    print(f\"'{age_col}' 特征处理完成。最终缺失值: {df_processed[age_col].isnull().sum()}\")\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "def _parse_fee(s):\n",
    "    \"\"\"\n",
    "    如果为范围，则取平均值。\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    # 转换为字符串并移除非数字字符，保留小数点和破折号\n",
    "    s_str = str(s).replace(' ', '') # 移除空格\n",
    "    if '空白' in s_str:\n",
    "        return np.nan    \n",
    "    # 查找所有数字 (包括小数)\n",
    "    nums = re.findall(r'(\\d+\\.?\\d*)', s_str)\n",
    "    \n",
    "    if len(nums) == 0:\n",
    "        return np.nan\n",
    "    elif len(nums) == 1:\n",
    "        return float(nums[0])\n",
    "    else:\n",
    "        return (float(nums[0]) + float(nums[1])) / 2\n",
    "\n",
    "def handle_greenery_rate(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '绿化率' 列:\n",
    "    \"\"\"\n",
    "    col = '绿化率'\n",
    "    new_col = 'GreeneryRate' \n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    s = df[col].astype(str).str.replace(r'[\\\\s%]', '', regex=True)\n",
    "    \n",
    "    # 在原始列上操作\n",
    "    df[col] = df[col].replace('10500', np.nan)  \n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce') \n",
    "    df[col] = df[col].replace(10500, np.nan)\n",
    "    \n",
    "    # 重新基于清理后的列计算 s_numeric\n",
    "    s_numeric = pd.to_numeric(s, errors='coerce')\n",
    "    s_numeric = s_numeric.replace(10500, np.nan) # 确保 s_numeric 也清除了\n",
    "\n",
    "    # 仅使用训练集计算中位数\n",
    "    median_val = s_numeric.iloc[:n_train].median()\n",
    "\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val)\n",
    "    \n",
    "    print(f\"  创建了新特征 '{new_col}' (中位数: {median_val:.2f})。\")\n",
    "\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_plot_ratio(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '容积率' 列\n",
    "    \"\"\"\n",
    "    col = '容积率' \n",
    "    new_col = 'PlotRatio' \n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    s_numeric = pd.to_numeric(df[col], errors='coerce') \n",
    "    \n",
    "\n",
    "    # 仅使用训练集计算中位数\n",
    "    median_val = s_numeric.iloc[:n_train].median()    \n",
    "    \n",
    "    df[new_col] = s_numeric.fillna(median_val)\n",
    "    \n",
    "    print(f\"  创建了新特征 '{new_col}' (中位数: {median_val:.2f})。\")\n",
    "    \n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_property_fee(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '物业费' 列 \n",
    "    \"\"\"\n",
    "    col = '物业费'\n",
    "    new_col = 'PropertyFee'\n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    s_numeric = df[col].apply(_parse_fee) \n",
    "    \n",
    "    # 仅使用训练集计算中位数\n",
    "    median_val = s_numeric.iloc[:n_train].median()\n",
    "    \n",
    "    df[new_col] = s_numeric.fillna(median_val)\n",
    "    \n",
    "    print(f\"  创建了新特征 '{new_col}' (中位数: {median_val:.2f})。\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_water_supply(df):\n",
    "    \"\"\"\n",
    "    处理 '供水' 列\n",
    "    1. 填充 NaN\n",
    "    2. 创建 is_Water_Civil (民水) 和 is_Water_Commercial (商水) 两个二元特征\n",
    "    \"\"\"\n",
    "    col = '供水'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        # 如果列不存在，创建默认值以避免下游错误\n",
    "        df['is_Water_Civil'] = 0\n",
    "        df['is_Water_Commercial'] = 0\n",
    "        return df\n",
    "\n",
    "    # 1. 填充NaN为空字符串''，这样 .str.contains 才不会对NaN报错，直接将填充后的 Pandas Series 赋值给 s。\n",
    "    s = df[col].fillna('') \n",
    "    \n",
    "    # 2. 创建二元特征\n",
    "    df['is_Water_Civil'] = s.str.contains('民水').astype(int)\n",
    "    df['is_Water_Commercial'] = s.str.contains('商水').astype(int)\n",
    "    \n",
    "    print(\"  创建了 'is_Water_Civil' 和 'is_Water_Commercial' 特征。\")\n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    # 3. 删除原始列\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_heating(df):\n",
    "    \"\"\"\n",
    "    创建 is_Heating_Central (集中), is_Heating_Self (自采暖), is_Heating_None (无) 三个二元特征\n",
    "    \"\"\"\n",
    "    col = '供暖'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        df['is_Heating_Central'] = 0\n",
    "        df['is_Heating_Self'] = 0\n",
    "        df['is_Heating_None'] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('')\n",
    "    \n",
    "    #  创建二元特征\n",
    "    df['is_Heating_Central'] = s.str.contains('集中供暖').astype(int)\n",
    "    df['is_Heating_Self'] = s.str.contains('自采暖').astype(int)\n",
    "    df['is_Heating_None'] = s.str.contains('无供暖').astype(int)\n",
    "    \n",
    "    print(\"  创建了 'is_Heating_Central', 'is_Heating_Self', 'is_Heating_None' 特征。\")\n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    # 删除原始列\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_electricity(df):\n",
    "    \"\"\"\n",
    "    处理 '供电' 列 \n",
    "    创建 is_Electricity_Civil (民电) 和 is_Electricity_Commercial (商电) 两个二元特征\n",
    "    \"\"\"\n",
    "    col = '供电'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        df['is_Electricity_Civil'] = 0\n",
    "        df['is_Electricity_Commercial'] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('')\n",
    "       \n",
    "    # 创建二元特征\n",
    "    df['is_Electricity_Civil'] = s.str.contains('民电').astype(int)\n",
    "    df['is_Electricity_Commercial'] = s.str.contains('商电').astype(int)\n",
    "    \n",
    "    print(\"  创建了 'is_Electricity_Civil' 和 'is_Electricity_Commercial' 特征。\")\n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    #  删除原始列\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "    \n",
    "def handle_gas_fee(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '燃气费' 列\n",
    "    \"\"\"\n",
    "    col = '燃气费'\n",
    "    new_col = 'GasFee' \n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    s_numeric = df[col].apply(_parse_fee) \n",
    "    \n",
    "    median_val = s_numeric.iloc[:n_train].median()\n",
    "    \n",
    "    df[new_col] = s_numeric.fillna(median_val)\n",
    "    \n",
    "    print(f\"  创建了新特征 '{new_col}' (中位数: {median_val:.2f})。\")\n",
    "    \n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_heating_fee(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '供热费' 列 \n",
    "    \"\"\"\n",
    "    col = '供热费'\n",
    "    new_col = 'HeatingFee'\n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    s_numeric = df[col].apply(_parse_fee) \n",
    "    \n",
    "    median_val = s_numeric.iloc[:n_train].median()\n",
    "\n",
    "    \n",
    "    df[new_col] = s_numeric.fillna(median_val)\n",
    "    \n",
    "    print(f\"  创建了新特征 '{new_col}' (中位数: {median_val:.2f})。\")\n",
    "    \n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_parking_spots(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '停车位' 列 用中位数填充\n",
    "    \"\"\"\n",
    "    col = '停车位'\n",
    "    new_col = 'ParkingSpots' \n",
    "    \n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        df[new_col] = 0 \n",
    "        return df\n",
    "\n",
    "    s_numeric = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    median_val = s_numeric.iloc[:n_train].median()\n",
    " \n",
    "    \n",
    "    if pd.isna(median_val):\n",
    "        median_val = 0 \n",
    "        print(f\"  警告: '{col}' 的训练集中位数计算为 NaN，使用 0 作为填充值。\")\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val)\n",
    "    \n",
    "    print(f\"  创建了新特征 '{new_col}' (中位数: {median_val:.2f})。\")\n",
    "    \n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_elevator_equipped(df):\n",
    "    \"\"\"\n",
    "    处理 '配备电梯' 列 \n",
    "    \"\"\"\n",
    "    print(\"处理 [配备电梯]...\")\n",
    "    if '配备电梯' not in df.columns:\n",
    "        print(\"警告: 列 '配备电梯' 不存在。\")\n",
    "        return df\n",
    "\n",
    "    # 填充 NaN\n",
    "    df['配备电梯'] = df['配备电梯'].fillna('未知')\n",
    "    \n",
    "    # 打印唯一值以供检查\n",
    "    print(f\"  处理后 '配备电梯' 的唯一值: {df['配备电梯'].unique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_housing_tenure(df):\n",
    "    \"\"\"\n",
    "    处理 '房屋年限' 列 \n",
    "    \"\"\"\n",
    "    col = '房屋年限'\n",
    "    print(f\"处理 [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"警告: 列 '{col}' 不存在。\")\n",
    "        return df\n",
    "    df[col] = df[col].fillna('未知')\n",
    "    print(f\"  处理后 '{col}' 的唯一值: {df[col].unique()}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 1: 数据加载 (仅 Price) ---\n",
      "Price 训练数据加载成功: (103871, 55)\n",
      "Price 测试数据加载成功: (34017, 55)\n",
      "\\n--- 步骤 2: 存储原始信息与分离 ---\n",
      "Price 训练集原始行数: 103871\n",
      "已分离 Price 目标变量 (y_train_price, y_train_ln_price)，长度: 103871\n",
      "已分离 Price 测试集 ID (test_ids_price)，长度: 34017\n",
      "\\n--- 步骤 3: 合并 Price 训练集与测试集 ---\n",
      "Price 数据集合并完成。合并后 df_price 形状: (137888, 55)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2709"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  步骤 1: 数据加载 (Price) \n",
    "print(\"--- 步骤 1: 数据加载 (仅 Price) ---\")\n",
    "try:\n",
    "    df_train_price_raw = pd.read_csv('./data/ruc_Class25Q2_train_price.csv')\n",
    "    df_test_price_raw = pd.read_csv('./data/ruc_Class25Q2_test_price.csv')\n",
    "    print(f\"Price 训练数据加载成功: {df_train_price_raw.shape}\")\n",
    "    print(f\"Price 测试数据加载成功: {df_test_price_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"错误：未找到 Price 数据文件。请确保文件在 './data/' 目录下。\")\n",
    "\n",
    "# --- 步骤 2: 存储原始信息与分离 ---\n",
    "print(\"\\\\n--- 步骤 2: 存储原始信息与分离 ---\")\n",
    "n_train_price = df_train_price_raw.shape[0]\n",
    "print(f\"Price 训练集原始行数: {n_train_price}\")\n",
    "\n",
    "# 分离目标变量 (Price)\n",
    "if 'Price' in df_train_price_raw.columns:\n",
    "    y_train_price = df_train_price_raw['Price'].copy()\n",
    "    # 对目标变量进行对数变换 \n",
    "    y_train_ln_price = np.log1p(y_train_price)\n",
    "    print(f\"已分离 Price 目标变量 (y_train_price, y_train_ln_price)，长度: {len(y_train_price)}\")\n",
    "else:\n",
    "    print(\"警告: 'Price' 列在训练集中未找到。\")\n",
    "    y_train_price = None\n",
    "    y_train_ln_price = None\n",
    "\n",
    "# 分离测试集 ID\n",
    "if 'ID' in df_test_price_raw.columns:\n",
    "    test_ids_price = df_test_price_raw['ID'].copy()\n",
    "    print(f\"已分离 Price 测试集 ID (test_ids_price)，长度: {len(test_ids_price)}\")\n",
    "else:\n",
    "    print(\"警告: 'ID' 列在测试集中未找到。\")\n",
    "    test_ids_price = None\n",
    "\n",
    "# --- 步骤 3: 合并数据集以便统一处理 ---\n",
    "print(\"\\\\n--- 步骤 3: 合并 Price 训练集与测试集 ---\")\n",
    "# 从训练集中移除 Price 列，从测试集中移除 ID 列\n",
    "df_train_to_concat = df_train_price_raw.drop(columns=['Price'], errors='ignore')\n",
    "df_test_to_concat = df_test_price_raw.drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "# 添加来源标识\n",
    "df_train_to_concat['source'] = 'train'\n",
    "df_test_to_concat['source'] = 'test'\n",
    "\n",
    "# 合并\n",
    "df_price = pd.concat([df_train_to_concat, df_test_to_concat], ignore_index=True)\n",
    "print(f\"Price 数据集合并完成。合并后 df_price 形状: {df_price.shape}\")\n",
    "\n",
    "# 清理原始数据框以释放内存\n",
    "del df_train_price_raw, df_test_price_raw, df_train_to_concat, df_test_to_concat\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- 步骤 4: 初步列删除 ---\n",
      "准备删除以下 18 列: ['抵押信息', '环线位置', '上次交易', '产权所属', '物业办公电话', '产权描述', '核心卖点', '户型介绍', '周边配套', '交通出行', '客户反馈', '板块_comm', '建筑结构_comm', 'coord_x', 'coord_y', '停车费用', '物业类别', 'source']\n",
      "初步列删除后 df_price 形状: (137888, 37)\n",
      "\\n初步清理后的数据信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137888 entries, 0 to 137887\n",
      "Data columns (total 37 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   城市      137888 non-null  int64  \n",
      " 1   区域      137888 non-null  float64\n",
      " 2   板块      137888 non-null  float64\n",
      " 3   环线      56096 non-null   object \n",
      " 4   房屋户型    137294 non-null  object \n",
      " 5   所在楼层    137888 non-null  object \n",
      " 6   建筑面积    137888 non-null  object \n",
      " 7   套内面积    45899 non-null   object \n",
      " 8   房屋朝向    137887 non-null  object \n",
      " 9   建筑结构    137294 non-null  object \n",
      " 10  装修情况    137294 non-null  object \n",
      " 11  梯户比例    134634 non-null  object \n",
      " 12  配备电梯    121445 non-null  object \n",
      " 13  别墅类型    1597 non-null    object \n",
      " 14  交易时间    137888 non-null  object \n",
      " 15  交易权属    137888 non-null  object \n",
      " 16  房屋用途    137887 non-null  object \n",
      " 17  房屋年限    82082 non-null   object \n",
      " 18  房屋优势    110273 non-null  object \n",
      " 19  lon     137888 non-null  float64\n",
      " 20  lat     137888 non-null  float64\n",
      " 21  年份      137888 non-null  float64\n",
      " 22  区县      126915 non-null  float64\n",
      " 23  建筑年代    93381 non-null   object \n",
      " 24  开发商     91155 non-null   object \n",
      " 25  房屋总数    127042 non-null  object \n",
      " 26  楼栋总数    127042 non-null  object \n",
      " 27  物业公司    89038 non-null   object \n",
      " 28  绿化率     95825 non-null   object \n",
      " 29  容积率     95431 non-null   float64\n",
      " 30  物业费     98241 non-null   object \n",
      " 31  供水      99395 non-null   object \n",
      " 32  供暖      46360 non-null   object \n",
      " 33  供电      99419 non-null   object \n",
      " 34  燃气费     96030 non-null   object \n",
      " 35  供热费     42372 non-null   object \n",
      " 36  停车位     93749 non-null   float64\n",
      "dtypes: float64(8), int64(1), object(28)\n",
      "memory usage: 38.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# --- 步骤 4: 初步列删除 ---\n",
    "print(\"\\\\n--- 步骤 4: 初步列删除 ---\")\n",
    "\n",
    "columns_to_drop_initial = [\n",
    "    '抵押信息',       # 完全为空\n",
    "    '环线位置',       # 与 '环线' 重复\n",
    "    '上次交易',       # 时间信息将在后续处理\n",
    "    '产权所属',       # 类别较少且信息量可能不大\n",
    "    '物业办公电话',   # 信息价值低\n",
    "    '产权描述',       # 文本，暂不处理\n",
    "    '核心卖点',       # 文本，暂不处理\n",
    "    '户型介绍',       # 文本，暂不处理\n",
    "    '周边配套',       # 文本，暂不处理\n",
    "    '交通出行',       # 文本，暂不处理\n",
    "    '客户反馈',       # 文本，暂不处理\n",
    "    '板块_comm',      # 与 '板块' 信息重叠\n",
    "    '建筑结构_comm',  # 与 '建筑结构' 信息重叠\n",
    "    'coord_x',        # 与 lon/lat 重复\n",
    "    'coord_y',        # 与 lon/lat 重复\n",
    "    '停车费用',      \n",
    "    '物业类别',\n",
    "    'source'         \n",
    "]\n",
    "\n",
    "existing_cols_to_drop = [col for col in columns_to_drop_initial if col in df_price.columns]\n",
    "\n",
    "print(f\"准备删除以下 {len(existing_cols_to_drop)} 列: {existing_cols_to_drop}\")\n",
    "\n",
    "# 执行删除\n",
    "df_price = df_price.drop(columns=existing_cols_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"初步列删除后 df_price 形状: {df_price.shape}\")\n",
    "\n",
    "# 再次检查信息，看剩余列和大致类型\n",
    "print(\"\\\\n初步清理后的数据信息:\")\n",
    "df_price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始执行常规数据处理(n_train=103871) ---\n",
      "处理 [区域]...\n",
      "合并了 116 个稀有区域为 '其他区域'\n",
      "处理 [环线]...\n",
      "  处理 [房屋户型]...\n",
      "    正在计算 '室', '厅', '卫' 的中位数 (基于前 103871 行)...\n",
      "      '室' 中位数: 3.0\n",
      "      '厅' 中位数: 2.0\n",
      "      '卫' 中位数: 1.0\n",
      "    已提取 '室', '厅', '卫' 特征并填充缺失值。\n",
      "  处理 [所在楼层]...\n",
      "    提取总楼层和当前楼层...\n",
      "    提取明确的楼层类别...\n",
      "    推断楼层类别...\n",
      "    合并类别并填充缺失值...\n",
      "    进行独热编码并清理临时列...\n",
      "    楼层信息处理完成。\n",
      "处理 [建筑结构]...\n",
      "建筑结构分组分布: {'钢混结构': 107584, '混合砖混类': 19347, '未知结构': 7248, '框架钢构类': 3623, '其他稀有结构': 86}\n",
      "  处理 [建筑面积] 和 [套内面积]...\n",
      "    清理面积单位并转为数值...\n",
      "    计算训练集 (前 103871 行) 平均得房率...\n",
      "      计算得到的训练集平均得房率: 0.8253\n",
      "    估算/修正 '套内面积'...\n",
      "      使用得房率估算了 92231 行的 '套内面积'\n",
      "    使用中位数填充 '建筑面积' 缺失值...\n",
      "      建筑面积中位数: 91.11\n",
      "    再次检查并填充 '套内面积' 缺失值...\n",
      "    创建 '得房率' 特征...\n",
      "      最终 '得房率' 使用的中位数/填充值: 0.8253\n",
      "处理 [房屋朝向]...\n",
      "处理 [装修情况]...\n",
      "  处理后 '装修情况' 的唯一值: ['精装' '简装' '未知' '毛坯' '其他']\n",
      "处理 [梯户比例]...\n",
      "户梯比中位数 (基于训练集): 2.00\n",
      "处理 [别墅类型]...\n",
      "  处理后 '别墅类型' 的唯一值: ['非别墅' '独栋' '联排' '叠拼' '双拼']\n",
      "处理 [房屋用途]...\n",
      "  在 '房屋用途' 中, 合并 6 个稀有类别为 '其他': ['公寓', '车库', '新式里弄', '四合院', '花园洋房', '未知']\n",
      "  处理后 '房屋用途' 的唯一值: ['普通住宅' '别墅' '商用' '其他']\n",
      "处理 [交易权属]...\n",
      "  在 '交易权属' 中, 合并 3 个稀有类别为 '其他'\n",
      "  处理后 '交易权属' 的唯一值: ['商品房' '政策房' '经济适用房' '其他' '安置房']\n",
      "处理 [房屋优势]...\n",
      "  提取了 'is_Adv_地铁', 'is_Adv_装修', 'Adv_Tenure_Ordinal'\n",
      "处理 [开发商]...\n",
      "  创建了新特征 'has_Developer'\n",
      "处理 [物业公司]...\n",
      "  创建了新特征 'has_PropertyMgmt'\n",
      "处理 [区县]...\n",
      "  已将 '区县' 填充缺失值并转换为 'string' 类型。\n",
      "  处理后 '区县' 的唯一值 (示例): ['109.0' '65.0' '62.0' '123.0' '81.0' '112.0' '28.0' '68.0' '7.0' '5.0']\n",
      "处理 [房屋总数] 和 [楼栋总数]...\n",
      "  清洗了 '房屋总数' (中位数: 1372.0) 和 '楼栋总数' (中位数: 15.0)。\n",
      "  创建了新的交互特征 'avg_units_per_building' (中位数: 95.81818181818181)。\n",
      "处理 [建筑年代]，计算 房龄...\n",
      "    初步计算后 '房龄' 缺失值数量: 44507\n",
      "    使用分组中位数填充 '房龄' 缺失值 (基于前 103871 行)...\n",
      " 使用全局中位数/备用值 (15.0) 填充了 8033 个剩余缺失值。\n",
      "'房龄' 缺失值填充完成。\n",
      "'房龄' 特征处理完成。最终缺失值: 0\n",
      "处理 [绿化率]...\n",
      "  创建了新特征 'GreeneryRate' (中位数: 34.00)。\n",
      "处理 [容积率]...\n",
      "  创建了新特征 'PlotRatio' (中位数: 2.50)。\n",
      "处理 [物业费]...\n",
      "  创建了新特征 'PropertyFee' (中位数: 1.90)。\n",
      "处理 [供水]...\n",
      "  创建了 'is_Water_Civil' 和 'is_Water_Commercial' 特征。\n",
      "  处理后 '供水' 的唯一值: ['民水' '商水/民水' '商水' nan]\n",
      "处理 [供暖]...\n",
      "  创建了 'is_Heating_Central', 'is_Heating_Self', 'is_Heating_None' 特征。\n",
      "  处理后 '供暖' 的唯一值: ['集中供暖' '自采暖' '集中供暖/自采暖' nan '自采暖/无供暖' '集中供暖/自采暖/无供暖' '无供暖']\n",
      "处理 [供电]...\n",
      "  创建了 'is_Electricity_Civil' 和 'is_Electricity_Commercial' 特征。\n",
      "  处理后 '供电' 的唯一值: ['民电' '商电/民电' '商电' nan]\n",
      "处理 [燃气费]...\n",
      "  创建了新特征 'GasFee' (中位数: 2.61)。\n",
      "处理 [供热费]...\n",
      "  创建了新特征 'HeatingFee' (中位数: 25.00)。\n",
      "处理 [停车位]...\n",
      "  创建了新特征 'ParkingSpots' (中位数: 734.00)。\n",
      "处理 [配备电梯]...\n",
      "  处理后 '配备电梯' 的唯一值: ['无' '有' '未知']\n",
      "处理 [房屋年限]...\n",
      "  处理后 '房屋年限' 的唯一值: ['满五年' '满两年' '未满两年' '未知']\n",
      "--- 完成 apply_common_preprocessing ---\n",
      "\\n--- 初步预处理完成 ---\n",
      "处理后的 df_price_processed 形状: (137888, 63)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137888 entries, 0 to 137887\n",
      "Data columns (total 63 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   城市                         137888 non-null  int64  \n",
      " 1   区域                         137888 non-null  object \n",
      " 2   板块                         137888 non-null  float64\n",
      " 3   建筑面积                       137888 non-null  float64\n",
      " 4   套内面积                       137888 non-null  float64\n",
      " 5   建筑结构                       137888 non-null  object \n",
      " 6   装修情况                       137888 non-null  object \n",
      " 7   配备电梯                       137888 non-null  object \n",
      " 8   别墅类型                       137888 non-null  object \n",
      " 9   交易权属                       137888 non-null  object \n",
      " 10  房屋用途                       137888 non-null  object \n",
      " 11  房屋年限                       137888 non-null  object \n",
      " 12  lon                        137888 non-null  float64\n",
      " 13  lat                        137888 non-null  float64\n",
      " 14  年份                         137888 non-null  float64\n",
      " 15  区县                         137888 non-null  object \n",
      " 16  房屋总数                       137888 non-null  float64\n",
      " 17  楼栋总数                       137888 non-null  float64\n",
      " 18  环线_ordinal                 137888 non-null  int64  \n",
      " 19  室                          137888 non-null  float64\n",
      " 20  厅                          137888 non-null  float64\n",
      " 21  卫                          137888 non-null  float64\n",
      " 22  总楼层                        137888 non-null  int64  \n",
      " 23  楼层_中楼层                     137888 non-null  bool   \n",
      " 24  楼层_低楼层                     137888 non-null  bool   \n",
      " 25  楼层_地下室                     137888 non-null  bool   \n",
      " 26  楼层_底层                      137888 non-null  bool   \n",
      " 27  楼层_顶层                      137888 non-null  bool   \n",
      " 28  楼层_高楼层                     137888 non-null  bool   \n",
      " 29  建筑结构分组                     137888 non-null  object \n",
      " 30  结构稳定性评分                    137888 non-null  int64  \n",
      " 31  结构安全性评分                    137888 non-null  int64  \n",
      " 32  得房率                        137888 non-null  float64\n",
      " 33  is_朝南                      137888 non-null  int64  \n",
      " 34  is_朝东                      137888 non-null  int64  \n",
      " 35  is_朝西                      137888 non-null  int64  \n",
      " 36  is_朝北                      137888 non-null  int64  \n",
      " 37  电梯数                        0 non-null       float64\n",
      " 38  每层户数                       0 non-null       float64\n",
      " 39  户梯比                        137888 non-null  float64\n",
      " 40  高密度标志                      137888 non-null  int64  \n",
      " 41  低密度标志                      137888 non-null  int64  \n",
      " 42  is_Adv_地铁                  137888 non-null  int64  \n",
      " 43  is_Adv_装修                  137888 non-null  int64  \n",
      " 44  Adv_Tenure_Ordinal         137888 non-null  int64  \n",
      " 45  has_Developer              137888 non-null  int64  \n",
      " 46  has_PropertyMgmt           137888 non-null  int64  \n",
      " 47  avg_units_per_building     137888 non-null  float64\n",
      " 48  交易年份                       137888 non-null  int32  \n",
      " 49  房龄                         137888 non-null  float64\n",
      " 50  GreeneryRate               137888 non-null  float64\n",
      " 51  PlotRatio                  137888 non-null  float64\n",
      " 52  PropertyFee                137888 non-null  float64\n",
      " 53  is_Water_Civil             137888 non-null  int64  \n",
      " 54  is_Water_Commercial        137888 non-null  int64  \n",
      " 55  is_Heating_Central         137888 non-null  int64  \n",
      " 56  is_Heating_Self            137888 non-null  int64  \n",
      " 57  is_Heating_None            137888 non-null  int64  \n",
      " 58  is_Electricity_Civil       137888 non-null  int64  \n",
      " 59  is_Electricity_Commercial  137888 non-null  int64  \n",
      " 60  GasFee                     137888 non-null  float64\n",
      " 61  HeatingFee                 137888 non-null  float64\n",
      " 62  ParkingSpots               137888 non-null  float64\n",
      "dtypes: bool(6), float64(23), int32(1), int64(23), object(10)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "source": [
    "def apply_common_preprocessing(df, n_train):\n",
    "    \"\"\"\n",
    "    按顺序调用所有的特征工程辅助函数。\n",
    "    \"\"\"\n",
    "    print(f\"--- 开始执行常规数据处理(n_train={n_train}) ---\")\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- 逐个调用辅助函数 ---\n",
    "    \n",
    "    # 区域/环线 \n",
    "    df_processed = handle_region(df_processed, rare_threshold=0.03)\n",
    "    df_processed = handle_ring_road(df_processed)\n",
    "    \n",
    "    # 户型/楼层 \n",
    "    df_processed = handle_house_type(df_processed, col_name='房屋户型', n_train=n_train)\n",
    "    df_processed = handle_floor(df_processed, n_train=n_train, col='所在楼层')\n",
    "    \n",
    "    # 结构/面积/朝向 \n",
    "    df_processed = handle_building_structure(df_processed)\n",
    "    df_processed = handle_area(df_processed, n_train=n_train) \n",
    "    df_processed = handle_orientation(df_processed)\n",
    "    \n",
    "    # 装修/梯户/别墅/用途/权属/优势 \n",
    "    df_processed = handle_decoration(df_processed)\n",
    "    df_processed = handle_elevator_ratio(df_processed, n_train=n_train) \n",
    "    df_processed = handle_villa_type(df_processed)\n",
    "    df_processed = handle_property_use(df_processed, rare_threshold=0.005)\n",
    "    df_processed = handle_transaction_ownership(df_processed, rare_threshold=0.005)\n",
    "    df_processed = handle_house_advantages(df_processed)\n",
    "    \n",
    "    # 开发商/物业/区县/社区统计/房龄 \n",
    "    df_processed = handle_developer(df_processed)\n",
    "    df_processed = handle_property_management(df_processed)\n",
    "    df_processed = handle_district(df_processed)\n",
    "    df_processed = handle_community_stats(df_processed, n_train=n_train)\n",
    "    # 交易年份需要先处理\n",
    "    df_processed['交易年份'] = pd.to_datetime(df_processed['交易时间']).dt.year\n",
    "    df_processed = handle_building_age(\n",
    "        df_processed, \n",
    "        n_train=n_train, \n",
    "        col='建筑年代', \n",
    "        trans_year_col='交易年份',\n",
    "        group_col_l1='板块', \n",
    "        group_col_l2='区域'\n",
    "    )\n",
    "\n",
    "    df_processed = handle_greenery_rate(df_processed, n_train=n_train)\n",
    "    \n",
    "    # 容积率/物业费 \n",
    "    df_processed = handle_plot_ratio(df_processed, n_train=n_train)\n",
    "    df_processed = handle_property_fee(df_processed, n_train=n_train)\n",
    "    \n",
    "    # 水/暖/电/燃气费/供热费/停车位\n",
    "    df_processed = handle_water_supply(df_processed)\n",
    "    df_processed = handle_heating(df_processed)\n",
    "    df_processed = handle_electricity(df_processed)\n",
    "    df_processed = handle_gas_fee(df_processed, n_train=n_train)\n",
    "    df_processed = handle_heating_fee(df_processed, n_train=n_train)\n",
    "\n",
    "    df_processed = handle_parking_spots(df_processed, n_train=n_train)\n",
    "    df_processed = handle_elevator_equipped(df_processed)\n",
    "    df_processed = handle_housing_tenure(df_processed)\n",
    "\n",
    "    df_processed = df_processed.drop(columns=['交易时间'], errors='ignore')\n",
    "\n",
    "    print(f\"--- 完成 apply_common_preprocessing ---\")\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "df_price_processed = apply_common_preprocessing(df_price, n_train_price)\n",
    "\n",
    "print(\"\\\\n--- 初步预处理完成 ---\")\n",
    "print(f\"处理后的 df_price_processed 形状: {df_price_processed.shape}\")\n",
    "df_price_processed.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- 步骤 5: 正在创建地理空间特征 ---\n",
      "开始计算地理空间特征 (距离中心)...\n",
      "  计算了 12 个城市的中心点 (基于训练集)。\n",
      "  计算了 '距离中心_公里'，并用训练集中位数 (14.58 km) 填充了 NaN。\n",
      "地理距离特征创建完毕。\n",
      "开始创建地理聚类 (每个城市 50 个簇)...\n",
      "地理聚类特征创建完毕。\n"
     ]
    }
   ],
   "source": [
    "# 步骤 5: 创建地理空间特征\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def compute_city_center_and_distances(df):\n",
    "    \"\"\"计算城市中心点及房源到中心点的距离。\"\"\"\n",
    "    df_out = df.copy()\n",
    "    print(\"开始计算地理空间特征 (距离中心)...\")\n",
    "    if '城市' not in df_out.columns or 'lon' not in df_out.columns or 'lat' not in df_out.columns:\n",
    "        print(\"  警告: 缺少 '城市', 'lon', 或 'lat' 列，无法计算距离特征。\")\n",
    "        return df_out\n",
    "\n",
    "    train_part = df_out.iloc[:n_train_price] \n",
    "    city_centers = train_part.groupby('城市', observed=True)[['lon', 'lat']].mean().reset_index()\n",
    "    city_centers = city_centers.rename(columns={'lon': 'center_lon', 'lat': 'center_lat'})\n",
    "    print(f\"  计算了 {len(city_centers)} 个城市的中心点 (基于训练集)。\")\n",
    "\n",
    "    df_out = pd.merge(df_out, city_centers, on='城市', how='left')\n",
    "\n",
    "    def compute_distance(row):\n",
    "        if pd.isna(row['lat']) or pd.isna(row['lon']) or pd.isna(row['center_lat']) or pd.isna(row['center_lon']):\n",
    "            return np.nan\n",
    "        try:\n",
    "            return geodesic((row['lat'], row['lon']), (row['center_lat'], row['center_lon'])).km\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    df_out['距离中心_公里'] = df_out.apply(compute_distance, axis=1)\n",
    "    median_dist_train = df_out.iloc[:n_train_price]['距离中心_公里'].median()\n",
    "    df_out['距离中心_公里'].fillna(median_dist_train, inplace=True)\n",
    "    print(f\"  计算了 '距离中心_公里'，并用训练集中位数 ({median_dist_train:.2f} km) 填充了 NaN。\")\n",
    "\n",
    "    df_out['距离中心_公里_平方'] = df_out['距离中心_公里'] ** 2\n",
    "    df_out = df_out.drop(columns=['center_lon', 'center_lat'], errors='ignore')\n",
    "    print(\"地理距离特征创建完毕。\")\n",
    "    return df_out\n",
    "\n",
    "def create_geo_clusters(df, n_train, n_clusters=50, city_col='城市', lon_col='lon', lat_col='lat'):\n",
    "    \"\"\"为每个城市计算地理聚类并进行独热编码。\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    print(f\"开始创建地理聚类 (每个城市 {n_clusters} 个簇)...\")\n",
    "\n",
    "    required_cols = [city_col, lon_col, lat_col]\n",
    "    if not all(col in df_processed.columns for col in required_cols):\n",
    "        print(f\"  错误: DataFrame 缺少必需的列: {required_cols}。跳过聚类...\")\n",
    "        return df_processed\n",
    "\n",
    "    coord_nan_mask = df_processed[[lon_col, lat_col]].isnull().any(axis=1)\n",
    "    cluster_col_temp = '地理聚类_temp'\n",
    "    df_processed[cluster_col_temp] = -1 # 未分配\n",
    "    all_city_labels = df_processed[city_col].unique()\n",
    "    \n",
    "    train_mask = df_processed.index < n_train\n",
    "    test_mask = df_processed.index >= n_train\n",
    "\n",
    "    for city_label in all_city_labels:\n",
    "        city_mask = (df_processed[city_col] == city_label) & (~coord_nan_mask)\n",
    "        city_data_train = df_processed.loc[city_mask & train_mask, [lon_col, lat_col]]\n",
    "        city_data_test = df_processed.loc[city_mask & test_mask, [lon_col, lat_col]]\n",
    "\n",
    "        if len(city_data_train) >= n_clusters:\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "                kmeans.fit(city_data_train)\n",
    "                \n",
    "                if not city_data_train.empty:\n",
    "                    clusters_train = kmeans.predict(city_data_train)\n",
    "                    df_processed.loc[city_data_train.index, cluster_col_temp] = clusters_train\n",
    "                if not city_data_test.empty:\n",
    "                    clusters_test = kmeans.predict(city_data_test)\n",
    "                    df_processed.loc[city_data_test.index, cluster_col_temp] = clusters_test\n",
    "            except Exception as e:\n",
    "                print(f\"    警告: 城市 {city_label} KMeans 失败: {e}\")\n",
    "        elif len(city_data_train) > 0:\n",
    "             print(f\"    警告: 城市 {city_label} 训练集数据点不足 ({len(city_data_train)})，跳过聚类。\")\n",
    "\n",
    "    combined_label_col = '地理聚类_带城市'\n",
    "    df_processed[combined_label_col] = np.where(\n",
    "         (df_processed[cluster_col_temp] != -1) & df_processed[city_col].notna(),\n",
    "         'C' + df_processed[city_col].astype(str).str.split('.').str[0] + '_' + df_processed[cluster_col_temp].astype(str),\n",
    "         'GeoCluster_Unknown'\n",
    "    )\n",
    "    df_processed = pd.get_dummies(df_processed, columns=[combined_label_col], prefix='GeoCluster', drop_first=False)\n",
    "    df_processed = df_processed.drop(columns=[cluster_col_temp], errors='ignore')\n",
    "    print(\"地理聚类特征创建完毕。\")\n",
    "    return df_processed\n",
    "\n",
    "print(\"\\\\n--- 步骤 5: 正在创建地理空间特征 ---\")\n",
    "df_price_with_geo = compute_city_center_and_distances(df_price_processed) \n",
    "df_price_with_geo = create_geo_clusters(df_price_with_geo, n_train=n_train_price, n_clusters=50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- 步骤 4: 正在执行目标编码 ---\n",
      "处理 ['城市', '区域', '区县', '板块'] (K-Fold Target Encoding, n_splits=6)...\n",
      "  将对以下存在的列进行编码: ['城市', '区域', '区县', '板块']\n",
      "  计算了基于 103871 训练样本的完整均值图谱。\n",
      "  已将完整图谱应用于 34017 个测试样本。\n",
      "    填充训练集中 K-Fold 后剩余的 95 个 NaN...\n",
      "  K-Fold Target Encoding 完成。新特征: 'Location_Target_Encoded'。\n",
      "\\n==============================\n",
      "第五步：系统性特征工程\n",
      "==============================\n",
      "开始创建比率特征...\n",
      "  创建比率特征 [平均每栋房屋数]...\n",
      "  创建比率特征 [停车位与房屋总数比]...\n",
      "  创建比率特征 [绿化率与容积率比]...\n",
      "  创建比率特征 [室厅比]...\n",
      "  创建比率特征 [室卫比]...\n",
      "比率特征创建完毕。\n",
      "开始进行对数变换...\n",
      "  已对 'PlotRatio' 应用 log1p 转换 -> 'log_PlotRatio'\n",
      "  已对 '楼栋总数' 应用 log1p 转换 -> 'log_楼栋总数'\n",
      "  已对 'HeatingFee' 应用 log1p 转换 -> 'log_HeatingFee'\n",
      "  已对 'ParkingSpots' 应用 log1p 转换 -> 'log_ParkingSpots'\n",
      "  已对 '房屋总数' 应用 log1p 转换 -> 'log_房屋总数'\n",
      "  已对 'PropertyFee' 应用 log1p 转换 -> 'log_PropertyFee'\n",
      "  已对 '距离中心_公里' 应用 log1p 转换 -> 'log_距离中心_公里'\n",
      "  已对 '距离中心_公里_平方' 应用 log1p 转换 -> 'log_距离中心_公里_平方'\n",
      "  已对 '平均每栋房屋数' 应用 log1p 转换 -> 'log_平均每栋房屋数'\n",
      "  已对 '停车位与房屋总数比' 应用 log1p 转换 -> 'log_停车位与房屋总数比'\n",
      "  已对 '建筑面积' 应用 log1p 转换 -> 'log_建筑面积'\n",
      "  已对 '套内面积' 应用 log1p 转换 -> 'log_套内面积'\n",
      "  已对 'Location_Target_Encoded' 应用 log1p 转换 -> 'log_Location_Target_Encoded'\n",
      "对数变换完成。共处理了 13 列。\n",
      "开始对特征 [房龄] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (15.50) 填充了 '房龄' 的 NaN。\n",
      "  已创建分箱特征 '房龄_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [总楼层] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (18.00) 填充了 '总楼层' 的 NaN。\n",
      "  已创建分箱特征 '总楼层_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_距离中心_公里] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (2.75) 填充了 'log_距离中心_公里' 的 NaN。\n",
      "  已创建分箱特征 'log_距离中心_公里_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_建筑面积] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (4.52) 填充了 'log_建筑面积' 的 NaN。\n",
      "  已创建分箱特征 'log_建筑面积_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_套内面积] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (4.33) 填充了 'log_套内面积' 的 NaN。\n",
      "  已创建分箱特征 'log_套内面积_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_Location_Target_Encoded] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (14.30) 填充了 'log_Location_Target_Encoded' 的 NaN。\n",
      "  已创建分箱特征 'log_Location_Target_Encoded_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "分箱处理完成。\n",
      "开始创建交互项 (degree=2, interaction_only=True)...\n",
      "  将为以下 4 个连续特征创建交互项: ['得房率', '室', '厅', '卫']\n",
      "  使用训练集的中位数填充交互项特征中的 NaN...\n",
      "  生成了 10 个多项式/交互项特征。\n",
      "交互项创建完毕。数据集新维度: (137888, 697)\n",
      "\\n==============================\n",
      "系统性特征工程完成后的数据信息:\n",
      "最终形状: (137888, 697)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137888 entries, 0 to 137887\n",
      "Columns: 697 entries, 建筑结构 to 厅_TIMES_卫\n",
      "dtypes: bool(636), float64(31), int32(1), int64(21), object(8)\n",
      "memory usage: 147.3+ MB\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def apply_target_encoding_combined(df, y_target, n_train, loc_cols, n_splits=6, random_state=111):\n",
    "    \"\"\"在合并的数据框上对 loc_cols 进行 K-Fold 目标编码。\"\"\"\n",
    "    df_te = df.copy()\n",
    "    print(f\"处理 {loc_cols} (K-Fold Target Encoding, n_splits={n_splits})...\")\n",
    "    if not loc_cols: return df_te \n",
    "\n",
    "    existing_loc_cols = [col for col in loc_cols if col in df_te.columns]\n",
    "    if not existing_loc_cols: return df_te\n",
    "    print(f\"  将对以下存在的列进行编码: {existing_loc_cols}\")\n",
    "\n",
    "    new_col = 'Location_Target_Encoded'\n",
    "    global_mean = y_target.mean()\n",
    "\n",
    "    def create_key(df_slice):\n",
    "        return df_slice[existing_loc_cols].astype(str).agg('_'.join, axis=1)\n",
    "    df_te['key'] = create_key(df_te)\n",
    "\n",
    "    X_train_part = df_te.iloc[:n_train].copy()\n",
    "    X_train_part['target'] = y_target\n",
    "    full_train_map = X_train_part.groupby('key')['target'].mean()\n",
    "    print(f\"  计算了基于 {n_train} 训练样本的完整均值图谱。\")\n",
    "\n",
    "    X_test_part = df_te.iloc[n_train:].copy()\n",
    "    df_te.loc[X_test_part.index, new_col] = X_test_part['key'].map(full_train_map).fillna(global_mean)\n",
    "    print(f\"  已将完整图谱应用于 {len(X_test_part)} 个测试样本。\")\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    df_te.loc[X_train_part.index, new_col] = np.nan\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_part)):\n",
    "        X_train_fold = X_train_part.iloc[train_idx]\n",
    "        X_val_fold = X_train_part.iloc[val_idx]\n",
    "        fold_map = X_train_fold.groupby('key')['target'].mean()\n",
    "        df_te.loc[X_val_fold.index, new_col] = X_val_fold['key'].map(fold_map)\n",
    "\n",
    "    \n",
    "    is_train_mask = df_te.index < n_train\n",
    "    is_nan_mask = df_te[new_col].isnull()\n",
    "    fill_mask = is_train_mask & is_nan_mask\n",
    "    fill_count = fill_mask.sum()\n",
    "    if fill_count > 0:\n",
    "        print(f\"    填充训练集中 K-Fold 后剩余的 {fill_count} 个 NaN...\")\n",
    "        values_to_fill = df_te.loc[fill_mask, 'key'].map(full_train_map).values\n",
    "        df_te.loc[fill_mask, new_col] = values_to_fill\n",
    "\n",
    "    df_te[new_col] = df_te[new_col].fillna(global_mean)\n",
    "    cols_to_drop = existing_loc_cols + ['key', 'target']\n",
    "    df_te = df_te.drop(columns=cols_to_drop, errors='ignore')\n",
    "    print(f\"  K-Fold Target Encoding 完成。新特征: '{new_col}'。\")\n",
    "    return df_te\n",
    "\n",
    "print(\"\\\\n--- 步骤 4: 正在执行目标编码 ---\")\n",
    "location_columns_to_encode = ['城市', '区域', '区县', '板块']\n",
    "df_price_te = apply_target_encoding_combined(df_price_with_geo, y_train_price, n_train_price, location_columns_to_encode)\n",
    "\n",
    "\n",
    "\n",
    "# 第五步：系统性特征工程 \n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 30)\n",
    "print(\"第五步：系统性特征工程\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# --- 5.a 创建比率特征 ---\n",
    "def cal_ratio(df, NumeratorValue, DenominatorValue, new_col_name):\n",
    "    \"\"\"计算比率特征，处理除以零的情况。\"\"\"\n",
    "    if NumeratorValue in df.columns and DenominatorValue in df.columns:\n",
    "        print(f\"  创建比率特征 [{new_col_name}]...\")\n",
    "        df[new_col_name] = np.where(\n",
    "            df[DenominatorValue] > 0,\n",
    "            df[NumeratorValue] / df[DenominatorValue],\n",
    "            0\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  警告: 无法创建比率 '{new_col_name}'，缺少列。\")\n",
    "\n",
    "\n",
    "def create_ratio_features(df):\n",
    "    df_out = df.copy()\n",
    "    print(\"开始创建比率特征...\")\n",
    "    cal_ratio(df_out, '房屋总数', '楼栋总数', '平均每栋房屋数')\n",
    "    cal_ratio(df_out, 'ParkingSpots', '房屋总数', '停车位与房屋总数比')\n",
    "    cal_ratio(df_out, 'GreeneryRate', 'PlotRatio', '绿化率与容积率比')\n",
    "    cal_ratio(df_out, '室', '厅', '室厅比')\n",
    "    cal_ratio(df_out, '室', '卫', '室卫比')\n",
    "    print(\"比率特征创建完毕。\")\n",
    "    return df_out\n",
    "\n",
    "df_price_eng = create_ratio_features(df_price_te)\n",
    "\n",
    "\n",
    "# --- 5.c 对数变换 (处理偏度) ---\n",
    "def log_transform(df, skewed_cols):\n",
    "    \"\"\"对指定的偏斜数值特征应用 log1p 转换。\"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    print(\"开始进行对数变换...\")\n",
    "    transformed_cols = []\n",
    "    for col in skewed_cols:\n",
    "        col_log = f'log_{col}'\n",
    "        if col in df_transformed.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_transformed[col]):\n",
    "                min_val = df_transformed[col].min()\n",
    "                if min_val >= 0:\n",
    "                    df_transformed[col_log] = np.log1p(df_transformed[col])\n",
    "                    df_transformed = df_transformed.drop(columns=[col], errors='ignore')\n",
    "                    transformed_cols.append(col)\n",
    "                    print(f\"  已对 '{col}' 应用 log1p 转换 -> '{col_log}'\")\n",
    "                else:\n",
    "                    print(f\"  警告: 列 '{col}' 包含负值 (最小值: {min_val})，跳过 log1p 转换。\")\n",
    "            else:\n",
    "                print(f\"  警告: 列 '{col}' 非数值类型，跳过 log1p 转换。\")\n",
    "    print(f\"对数变换完成。共处理了 {len(transformed_cols)} 列。\")\n",
    "    return df_transformed\n",
    "\n",
    "# (使用更新后的新列名)\n",
    "skewed_cols_to_transform = [\n",
    "    'PlotRatio', '楼栋总数', 'HeatingFee', 'ParkingSpots', '房屋总数', \n",
    "    'PropertyFee', '距离中心_公里', '距离中心_公里_平方', '平均每栋房屋数', \n",
    "    '停车位与房屋总数比', '建筑面积', '套内面积', 'Location_Target_Encoded'\n",
    "]\n",
    "skewed_cols_to_transform = [col for col in skewed_cols_to_transform if col in df_price_eng.columns]\n",
    "\n",
    "df_price_eng = log_transform(df_price_eng, skewed_cols_to_transform)\n",
    "\n",
    "\n",
    "# --- 5.d 分箱 (Binning) ---\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bin_and_encode(df, n_train, feature, n_bins=5, strategy='kmeans'):\n",
    "    \"\"\"对特征进行分箱和独热编码。\"\"\"\n",
    "    df_binned = df.copy()\n",
    "    print(f\"开始对特征 [{feature}] 进行分箱 (n_bins={n_bins}, strategy='{strategy}')...\")\n",
    "    if feature not in df_binned.columns: return df_binned\n",
    "    if not pd.api.types.is_numeric_dtype(df_binned[feature]): return df_binned\n",
    "\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=None)\n",
    "    feature_binned_col = f'{feature}_分箱'\n",
    "\n",
    "    median_val_train = df_binned.iloc[:n_train][feature].median()\n",
    "    df_binned[feature].fillna(median_val_train, inplace=True)\n",
    "    print(f\"  使用训练集中位数 ({median_val_train:.2f}) 填充了 '{feature}' 的 NaN。\")\n",
    "\n",
    "    try:\n",
    "        train_data_for_fit = df_binned.iloc[:n_train][[feature]]\n",
    "        if train_data_for_fit[feature].nunique() < n_bins:\n",
    "             actual_bins = train_data_for_fit[feature].nunique()\n",
    "             print(f\"  警告: 唯一值 ({actual_bins}) < 箱数 ({n_bins})。\")\n",
    "             if actual_bins < 2: return df_binned\n",
    "             binner = KBinsDiscretizer(n_bins=actual_bins, encode='ordinal', strategy='uniform', subsample=None)\n",
    "        \n",
    "        binner.fit(train_data_for_fit)\n",
    "        df_binned[feature_binned_col] = binner.transform(df_binned[[feature]])\n",
    "        print(f\"  已创建分箱特征 '{feature_binned_col}'。\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  错误: 对特征 '{feature}' 分箱时出错: {e}。跳过...\")\n",
    "        return df_binned\n",
    "\n",
    "    df_binned = pd.get_dummies(df_binned, columns=[feature_binned_col], prefix=f'{feature}段', drop_first=False)\n",
    "    df_binned = df_binned.drop(columns=[feature], errors='ignore')\n",
    "    print(f\"  已对分箱结果进行独热编码并移除原始特征。\")\n",
    "    return df_binned\n",
    "\n",
    "# (使用 log 变换后的特征名)\n",
    "features_to_bin = [\n",
    "    '房龄', '总楼层', # 原始数值\n",
    "    'log_距离中心_公里',\n",
    "    'log_建筑面积',\n",
    "    'log_套内面积',\n",
    "    'log_Location_Target_Encoded'\n",
    "]\n",
    "features_to_bin = [col for col in features_to_bin if col in df_price_eng.columns] # 确保存在\n",
    "\n",
    "for feature in features_to_bin:\n",
    "    df_price_eng = bin_and_encode(df_price_eng, n_train_price, feature=feature, n_bins=5, strategy='kmeans')\n",
    "\n",
    "print(\"分箱处理完成。\")\n",
    "\n",
    "\n",
    "# --- 5.e 创建交互项 ---\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_polynomial_interactions(df, n_train, continuous_cols_candidates, degree=2, interaction_only=True):\n",
    "    \"\"\"为数据集创建多项式交互项特征。\"\"\"\n",
    "    df_poly = df.copy()\n",
    "    print(f\"开始创建交互项 (degree={degree}, interaction_only={interaction_only})...\")\n",
    "\n",
    "    binary_cols = [col for col in df_poly.columns\n",
    "                   if df_poly[col].nunique(dropna=False) == 2 and\n",
    "                      df_poly[col].min() == 0 and df_poly[col].max() == 1]\n",
    "    all_numeric_cols = df_poly.select_dtypes(include=np.number).columns.tolist()\n",
    "    current_continuous_cols = [col for col in all_numeric_cols if col not in binary_cols]\n",
    "    cols_for_poly = [col for col in continuous_cols_candidates if col in current_continuous_cols]\n",
    "\n",
    "    print(f\"  将为以下 {len(cols_for_poly)} 个连续特征创建交互项: {cols_for_poly}\")\n",
    "    if not cols_for_poly: return df_poly\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n",
    "\n",
    "    print(\"  使用训练集的中位数填充交互项特征中的 NaN...\")\n",
    "    train_part_poly = df_poly.iloc[:n_train]\n",
    "    medians_poly = train_part_poly[cols_for_poly].median()\n",
    "    df_poly[cols_for_poly] = df_poly[cols_for_poly].fillna(medians_poly)\n",
    "    if df_poly[cols_for_poly].isnull().any().any():\n",
    "         df_poly[cols_for_poly] = df_poly[cols_for_poly].fillna(0)\n",
    "\n",
    "    try:\n",
    "        poly.fit(df_poly.iloc[:n_train][cols_for_poly])\n",
    "        poly_features = poly.transform(df_poly[cols_for_poly])\n",
    "        poly_feature_names = [name.replace(' ', '_TIMES_').replace('^2', '_SQ') for name in poly.get_feature_names_out(cols_for_poly)]\n",
    "        print(f\"  生成了 {len(poly_feature_names)} 个多项式/交互项特征。\")\n",
    "\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_poly.index)\n",
    "        \n",
    "        df_poly = df_poly.drop(columns=cols_for_poly, errors='ignore')\n",
    "        df_final = pd.concat([df_poly, poly_df], axis=1)\n",
    "\n",
    "        print(f\"交互项创建完毕。数据集新维度: {df_final.shape}\")\n",
    "        return df_final\n",
    "    except Exception as e:\n",
    "        print(f\"  错误: 创建交互项时出错: {e}。返回未修改的数据框。\")\n",
    "        return df_poly\n",
    "\n",
    "# (使用未被分箱的特征)\n",
    "continuous_cols_for_interactions = [\n",
    "    '得房率',\n",
    "    '室', '厅', '卫'\n",
    "]\n",
    "continuous_cols_for_interactions = [col for col in continuous_cols_for_interactions if col in df_price_eng.columns]\n",
    "df_price_final_eng = create_polynomial_interactions(df_price_eng, n_train_price, continuous_cols_for_interactions)\n",
    "\n",
    "\n",
    "# --- 最终检查 ---\n",
    "print(\"\\\\n\" + \"=\" * 30)\n",
    "print(\"系统性特征工程完成后的数据信息:\")\n",
    "print(f\"最终形状: {df_price_final_eng.shape}\")\n",
    "df_price_final_eng.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 1: 数据准备 ---\n",
      "完整训练集 X 形状: (103871, 697)\n",
      "Kaggle测试集 X 形状: (34017, 697)\n",
      "\n",
      "--- 步骤 2: y 异常值处理 (IQR 移除) ---\n",
      "IQR 异常值边界: [-9112782, 15866578]\n",
      "移除 y 异常值前: 103871 行\n",
      "移除 y 异常值后: 103264 行\n",
      "移除了 607 行\n",
      "\n",
      "--- 步骤 3: 拆分训练集 (80%) 和验证集 (20%) ---\n",
      "X_train 形状: (82611, 697)\n",
      "X_val 形状: (20653, 697)\n",
      "\n",
      "--- 步骤 4: X 特征最终处理 (使用 ColumnTransformer) ---\n",
      "  正在分离 数值型, 分类型(object), 和 布尔型(bool) 列...\n",
      "    找到了 53 个数值型特征 (将进行缩放)。\n",
      "    找到了 8 个分类型特征 (将进行独热编码)。\n",
      "    找到了 636 个布尔型特征 。\n",
      "    总计: 697 / 697 列将被处理。\n",
      "  正在计算和应用截断 (Clipping) 到数值型特征...\n",
      "  正在应用 ColumnTransformer (Impute, Scale, OneHotEncode)...\n",
      "\n",
      "--- 步骤 4 输出 ---\n",
      "完整清理并缩放的训练集 X 形状: (103264, 725)\n",
      "Kaggle测试集 (已缩放) X 形状: (34017, 725)\n",
      "  最终 NaN 检查 (应为 0): 0\n",
      "  注意：最终特征数量： 725 (因为独热编码)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LassoCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# 步骤 1: 数据准备 (分离 Kaggle 测试集)\n",
    "\n",
    "print(\"--- 步骤 1: 数据准备 ---\")\n",
    "df_final = df_price_final_eng.copy()\n",
    "\n",
    "# 将特征拆分为 训练集 (用于建模) 和 测试集 (用于Kaggle提交)\n",
    "X_train_full = df_final.iloc[:n_train_price]\n",
    "X_test_kaggle = df_final.iloc[n_train_price:]\n",
    "\n",
    "# 目标变量 (log 转换后)\n",
    "y_train_full_log = y_train_ln_price.copy()\n",
    "# 目标变量 (原始价格)\n",
    "y_train_full_orig = y_train_price.copy()\n",
    "\n",
    "print(f\"完整训练集 X 形状: {X_train_full.shape}\")\n",
    "print(f\"Kaggle测试集 X 形状: {X_test_kaggle.shape}\")\n",
    "\n",
    "\n",
    "# 步骤 2: y 异常值处理 \n",
    "\n",
    "print(\"\\n--- 步骤 2: y 异常值处理 (IQR 移除) ---\")\n",
    "\n",
    "# 在原始价格 y_train_full_orig 上计算 IQR\n",
    "Q1 = y_train_full_orig.quantile(0.01)\n",
    "Q3 = y_train_full_orig.quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# 创建掩码\n",
    "mask = (y_train_full_orig >= lower_bound) & (y_train_full_orig <= upper_bound)\n",
    "rows_before = len(y_train_full_orig)\n",
    "rows_after = mask.sum()\n",
    "\n",
    "print(f\"IQR 异常值边界: [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "print(f\"移除 y 异常值前: {rows_before} 行\")\n",
    "print(f\"移除 y 异常值后: {rows_after} 行\")\n",
    "print(f\"移除了 {rows_before - rows_after} 行\")\n",
    "\n",
    "# 应用掩码到 X 和 y\n",
    "X_train_clean = X_train_full[mask].copy()\n",
    "y_train_clean_log = y_train_full_log[mask].copy()\n",
    "y_train_clean_orig = y_train_full_orig[mask].copy()\n",
    "\n",
    "\n",
    "# 步骤 3: 训练集/验证集拆分 (80/20)\n",
    "\n",
    "print(\"\\n--- 步骤 3: 拆分训练集 (80%) 和验证集 (20%) ---\")\n",
    "X_train, X_val, y_train_log, y_val_log = train_test_split(\n",
    "    X_train_clean, \n",
    "    y_train_clean_log, \n",
    "    test_size=0.2, \n",
    "    random_state=111\n",
    ")\n",
    "\n",
    "# 确保 y_orig 也被正确拆分\n",
    "y_train_orig = y_train_clean_orig.loc[y_train_log.index]\n",
    "y_val_orig = y_train_clean_orig.loc[y_val_log.index]\n",
    "\n",
    "print(f\"X_train 形状: {X_train.shape}\")\n",
    "print(f\"X_val 形状: {X_val.shape}\")\n",
    "\n",
    "# 步骤 4: X 特征最终处理 (使用 ColumnTransformer)\n",
    "\n",
    "print(\"\\n--- 步骤 4: X 特征最终处理 (使用 ColumnTransformer) ---\")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# --- 4.1: 定义三种类型的列名 ---\n",
    "print(\"  正在分离 数值型, 分类型(object), 和 布尔型(bool) 列...\")\n",
    "\n",
    "# 1. 数值型特征 (int/float, 但排除 bool)\n",
    "numeric_features = X_train.select_dtypes(include=np.number, exclude=[bool]).columns\n",
    "print(f\"    找到了 {len(numeric_features)} 个数值型特征 (将进行缩放)。\")\n",
    "\n",
    "# 2. 分类型特征 (object)\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "print(f\"    找到了 {len(categorical_features)} 个分类型特征 (将进行独热编码)。\")\n",
    "\n",
    "# 3. 布尔型特征 (bool)\n",
    "boolean_features = X_train.select_dtypes(include=[bool]).columns\n",
    "print(f\"    找到了 {len(boolean_features)} 个布尔型特征 。\")\n",
    "\n",
    "# 检查总数是否匹配\n",
    "total_features_processed = len(numeric_features) + len(categorical_features) + len(boolean_features)\n",
    "print(f\"    总计: {total_features_processed} / {X_train.shape[1]} 列将被处理。\")\n",
    "if total_features_processed != X_train.shape[1]:\n",
    "    print(\"  警告: 有部分列的类型未被匹配，它们将被'remainder'规则处理！\")\n",
    "\n",
    "\n",
    "# --- 4.2: (Clipping) 异常值截断 (只对数值型) ---\n",
    "print(\"  正在计算和应用截断 (Clipping) 到数值型特征...\")\n",
    "lower_bounds_X = X_train[numeric_features].quantile(0.01)\n",
    "upper_bounds_X = X_train[numeric_features].quantile(0.99)\n",
    "\n",
    "# 使用 .loc 来避免 SettingWithCopyWarning\n",
    "X_train.loc[:, numeric_features] = X_train[numeric_features].clip(lower_bounds_X, upper_bounds_X, axis=1)\n",
    "X_val.loc[:, numeric_features] = X_val[numeric_features].clip(lower_bounds_X, upper_bounds_X, axis=1)\n",
    "X_train_clean.loc[:, numeric_features] = X_train_clean[numeric_features].clip(lower_bounds_X, upper_bounds_X, axis=1)\n",
    "X_test_kaggle.loc[:, numeric_features] = X_test_kaggle[numeric_features].clip(lower_bounds_X, upper_bounds_X, axis=1)\n",
    "\n",
    "\n",
    "# --- 4.3: 定义三条流水线 ---\n",
    "\n",
    "# 流水线 1: 数值型\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 流水线 2: 分类型 \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='未知')), # 填充缺失的分类\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # 独热编码\n",
    "])\n",
    "\n",
    "# 流水线 3: 布尔型 什么都不做，保持原样\n",
    "boolean_transformer = 'passthrough'\n",
    "\n",
    "# 组合三条流水线\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bool', boolean_transformer, boolean_features)\n",
    "    ],\n",
    "    remainder='drop' # 丢弃任何不属于这三类的列 \n",
    ")\n",
    "\n",
    "\n",
    "# --- 4.4: 应用 ColumnTransformer ---\n",
    "print(\"  正在应用 ColumnTransformer (Impute, Scale, OneHotEncode)...\")\n",
    "\n",
    "# 在 X_train (80%数据) 上 .fit()\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# 在所有数据集上 .transform()\n",
    "X_train_scaled = preprocessor.transform(X_train)\n",
    "X_val_scaled = preprocessor.transform(X_val)\n",
    "X_train_clean_scaled = preprocessor.transform(X_train_clean)\n",
    "X_test_kaggle_scaled = preprocessor.transform(X_test_kaggle)\n",
    "\n",
    "\n",
    "print(\"\\n--- 步骤 4 输出 ---\")\n",
    "print(f\"完整清理并缩放的训练集 X 形状: {X_train_clean_scaled.shape}\")\n",
    "print(f\"Kaggle测试集 (已缩放) X 形状: {X_test_kaggle_scaled.shape}\")\n",
    "\n",
    "# 检查 NaN\n",
    "nan_check = np.isnan(X_train_clean_scaled).sum()\n",
    "print(f\"  最终 NaN 检查 (应为 0): {nan_check}\")\n",
    "\n",
    "# 最终列数 = (数值列数) + (独热编码产生的新列数) + (布尔列数)\n",
    "print(f\"  注意：最终特征数量： {X_train_scaled.shape[1]} (因为独热编码)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- 步骤 4: 正在执行目标编码 ---\n",
      "处理 ['城市', '区域', '区县', '板块'] (K-Fold Target Encoding, n_splits=6)...\n",
      "  将对以下存在的列进行编码: ['城市', '区域', '区县', '板块']\n",
      "  计算了基于 103871 训练样本的完整均值图谱。\n",
      "  已将完整图谱应用于 34017 个测试样本。\n",
      "    填充训练集中 K-Fold 后剩余的 95 个 NaN...\n",
      "  K-Fold Target Encoding 完成。新特征: 'Location_Target_Encoded'。\n",
      "\\n==============================\n",
      "第五步：系统性特征工程 (剩余部分)\n",
      "==============================\n",
      "开始创建比率特征...\n",
      "  创建比率特征 [平均每栋房屋数]...\n",
      "  创建比率特征 [停车位与房屋总数比]...\n",
      "  创建比率特征 [绿化率与容积率比]...\n",
      "  创建比率特征 [室厅比]...\n",
      "  创建比率特征 [室卫比]...\n",
      "比率特征创建完毕。\n",
      "开始进行对数变换...\n",
      "  已对 'PlotRatio' 应用 log1p 转换 -> 'log_PlotRatio'\n",
      "  已对 '楼栋总数' 应用 log1p 转换 -> 'log_楼栋总数'\n",
      "  已对 'HeatingFee' 应用 log1p 转换 -> 'log_HeatingFee'\n",
      "  已对 'ParkingSpots' 应用 log1p 转换 -> 'log_ParkingSpots'\n",
      "  已对 '房屋总数' 应用 log1p 转换 -> 'log_房屋总数'\n",
      "  已对 'PropertyFee' 应用 log1p 转换 -> 'log_PropertyFee'\n",
      "  已对 '距离中心_公里' 应用 log1p 转换 -> 'log_距离中心_公里'\n",
      "  已对 '距离中心_公里_平方' 应用 log1p 转换 -> 'log_距离中心_公里_平方'\n",
      "  已对 '平均每栋房屋数' 应用 log1p 转换 -> 'log_平均每栋房屋数'\n",
      "  已对 '停车位与房屋总数比' 应用 log1p 转换 -> 'log_停车位与房屋总数比'\n",
      "  已对 '建筑面积' 应用 log1p 转换 -> 'log_建筑面积'\n",
      "  已对 '套内面积' 应用 log1p 转换 -> 'log_套内面积'\n",
      "  已对 'Location_Target_Encoded' 应用 log1p 转换 -> 'log_Location_Target_Encoded'\n",
      "对数变换完成。共处理了 13 列。\n",
      "开始对特征 [房龄] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (15.50) 填充了 '房龄' 的 NaN。\n",
      "  已创建分箱特征 '房龄_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [总楼层] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (18.00) 填充了 '总楼层' 的 NaN。\n",
      "  已创建分箱特征 '总楼层_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_距离中心_公里] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (2.75) 填充了 'log_距离中心_公里' 的 NaN。\n",
      "  已创建分箱特征 'log_距离中心_公里_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_建筑面积] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (4.52) 填充了 'log_建筑面积' 的 NaN。\n",
      "  已创建分箱特征 'log_建筑面积_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_套内面积] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (4.33) 填充了 'log_套内面积' 的 NaN。\n",
      "  已创建分箱特征 'log_套内面积_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "开始对特征 [log_Location_Target_Encoded] 进行分箱 (n_bins=5, strategy='kmeans')...\n",
      "  使用训练集中位数 (14.30) 填充了 'log_Location_Target_Encoded' 的 NaN。\n",
      "  已创建分箱特征 'log_Location_Target_Encoded_分箱'。\n",
      "  已对分箱结果进行独热编码并移除原始特征。\n",
      "分箱处理完成。\n",
      "开始创建交互项 (degree=2, interaction_only=True)...\n",
      "  将为以下 4 个连续特征创建交互项: ['得房率', '室', '厅', '卫']\n",
      "  使用训练集的中位数填充交互项特征中的 NaN...\n",
      "  生成了 10 个多项式/交互项特征。\n",
      "交互项创建完毕。数据集新维度: (137888, 697)\n",
      "\\n==============================\n",
      "系统性特征工程完成后的数据信息:\n",
      "最终形状: (137888, 697)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137888 entries, 0 to 137887\n",
      "Columns: 697 entries, 建筑结构 to 厅_TIMES_卫\n",
      "dtypes: bool(636), float64(31), int32(1), int64(21), object(8)\n",
      "memory usage: 147.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# 目标编码 (Target Encoding)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def apply_target_encoding_combined(df, y_target, n_train, loc_cols, n_splits=6, random_state=111):\n",
    "    \"\"\"在合并的数据框上对 loc_cols 进行 K-Fold 目标编码。\"\"\"\n",
    "    df_te = df.copy()\n",
    "    print(f\"处理 {loc_cols} (K-Fold Target Encoding, n_splits={n_splits})...\")\n",
    "    if not loc_cols: return df_te\n",
    "\n",
    "    existing_loc_cols = [col for col in loc_cols if col in df_te.columns]\n",
    "    if not existing_loc_cols: return df_te\n",
    "    print(f\"  将对以下存在的列进行编码: {existing_loc_cols}\")\n",
    "\n",
    "    new_col = 'Location_Target_Encoded'\n",
    "    global_mean = y_target.mean()\n",
    "\n",
    "    def create_key(df_slice):\n",
    "        return df_slice[existing_loc_cols].astype(str).agg('_'.join, axis=1)\n",
    "    df_te['key'] = create_key(df_te)\n",
    "\n",
    "    X_train_part = df_te.iloc[:n_train].copy()\n",
    "    X_train_part['target'] = y_target\n",
    "    full_train_map = X_train_part.groupby('key')['target'].mean()\n",
    "    print(f\"  计算了基于 {n_train} 训练样本的完整均值图谱。\")\n",
    "\n",
    "    X_test_part = df_te.iloc[n_train:].copy()\n",
    "    df_te.loc[X_test_part.index, new_col] = X_test_part['key'].map(full_train_map).fillna(global_mean)\n",
    "    print(f\"  已将完整图谱应用于 {len(X_test_part)} 个测试样本。\")\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    df_te.loc[X_train_part.index, new_col] = np.nan\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_part)):\n",
    "        X_train_fold = X_train_part.iloc[train_idx]\n",
    "        X_val_fold = X_train_part.iloc[val_idx]\n",
    "        fold_map = X_train_fold.groupby('key')['target'].mean()\n",
    "        df_te.loc[X_val_fold.index, new_col] = X_val_fold['key'].map(fold_map)\n",
    "\n",
    "    is_train_mask = df_te.index < n_train\n",
    "    is_nan_mask = df_te[new_col].isnull()\n",
    "    fill_mask = is_train_mask & is_nan_mask\n",
    "    fill_count = fill_mask.sum()\n",
    "    if fill_count > 0:\n",
    "        print(f\"    填充训练集中 K-Fold 后剩余的 {fill_count} 个 NaN...\")\n",
    "        values_to_fill = df_te.loc[fill_mask, 'key'].map(full_train_map).values\n",
    "        df_te.loc[fill_mask, new_col] = values_to_fill\n",
    "    \n",
    "\n",
    "    df_te[new_col] = df_te[new_col].fillna(global_mean)\n",
    "    cols_to_drop = existing_loc_cols + ['key', 'target']\n",
    "    df_te = df_te.drop(columns=cols_to_drop, errors='ignore')\n",
    "    print(f\"  K-Fold Target Encoding 完成。新特征: '{new_col}'。\")\n",
    "    return df_te\n",
    "\n",
    "# --- (调用 4) ---\n",
    "print(\"\\\\n--- 步骤 4: 正在执行目标编码 ---\")\n",
    "location_columns_to_encode = ['城市', '区域', '区县', '板块']\n",
    "df_price_te = apply_target_encoding_combined(df_price_with_geo, y_train_price, n_train_price, location_columns_to_encode)\n",
    "\n",
    "\n",
    "\n",
    "# 第五步：系统性特征工程\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 30)\n",
    "print(\"第五步：系统性特征工程 (剩余部分)\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# --- 5.a 创建比率特征 ---\n",
    "def cal_ratio(df, NumeratorValue, DenominatorValue, new_col_name):\n",
    "    \"\"\"计算比率特征，处理除以零的情况。\"\"\"\n",
    "    if NumeratorValue in df.columns and DenominatorValue in df.columns:\n",
    "        print(f\"  创建比率特征 [{new_col_name}]...\")\n",
    "        df[new_col_name] = np.where(\n",
    "            df[DenominatorValue] > 0,\n",
    "            df[NumeratorValue] / df[DenominatorValue],\n",
    "            0\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  警告: 无法创建比率 '{new_col_name}'，缺少列。\")\n",
    "\n",
    "\n",
    "def create_ratio_features(df):\n",
    "    df_out = df.copy()\n",
    "    print(\"开始创建比率特征...\")\n",
    "    cal_ratio(df_out, '房屋总数', '楼栋总数', '平均每栋房屋数')\n",
    "    cal_ratio(df_out, 'ParkingSpots', '房屋总数', '停车位与房屋总数比')\n",
    "    cal_ratio(df_out, 'GreeneryRate', 'PlotRatio', '绿化率与容积率比')\n",
    "    cal_ratio(df_out, '室', '厅', '室厅比')\n",
    "    cal_ratio(df_out, '室', '卫', '室卫比')\n",
    "    print(\"比率特征创建完毕。\")\n",
    "    return df_out\n",
    "df_price_eng = create_ratio_features(df_price_te)\n",
    "\n",
    "\n",
    "# --- 5.c 对数变换 (处理偏度) ---\n",
    "def log_transform(df, skewed_cols):\n",
    "    \"\"\"对指定的偏斜数值特征应用 log1p 转换。\"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    print(\"开始进行对数变换...\")\n",
    "    transformed_cols = []\n",
    "    for col in skewed_cols:\n",
    "        col_log = f'log_{col}'\n",
    "        if col in df_transformed.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_transformed[col]):\n",
    "                min_val = df_transformed[col].min()\n",
    "                if min_val >= 0:\n",
    "                    df_transformed[col_log] = np.log1p(df_transformed[col])\n",
    "                    df_transformed = df_transformed.drop(columns=[col], errors='ignore')\n",
    "                    transformed_cols.append(col)\n",
    "                    print(f\"  已对 '{col}' 应用 log1p 转换 -> '{col_log}'\")\n",
    "                else:\n",
    "                    print(f\"  警告: 列 '{col}' 包含负值 (最小值: {min_val})，跳过 log1p 转换。\")\n",
    "            else:\n",
    "                print(f\"  警告: 列 '{col}' 非数值类型，跳过 log1p 转换。\")\n",
    "    print(f\"对数变换完成。共处理了 {len(transformed_cols)} 列。\")\n",
    "    return df_transformed\n",
    "\n",
    "# (使用更新后的新列名)\n",
    "skewed_cols_to_transform = [\n",
    "    'PlotRatio', '楼栋总数', 'HeatingFee', 'ParkingSpots', '房屋总数', \n",
    "    'PropertyFee', '距离中心_公里', '距离中心_公里_平方', '平均每栋房屋数', \n",
    "    '停车位与房屋总数比', '建筑面积', '套内面积', 'Location_Target_Encoded'\n",
    "]\n",
    "skewed_cols_to_transform = [col for col in skewed_cols_to_transform if col in df_price_eng.columns]\n",
    "df_price_eng = log_transform(df_price_eng, skewed_cols_to_transform)\n",
    "\n",
    "\n",
    "# --- 5.d 分箱 (Binning) ---\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def bin_and_encode(df, n_train, feature, n_bins=5, strategy='kmeans'):\n",
    "    \"\"\"对特征进行分箱和独热编码。\"\"\"\n",
    "    df_binned = df.copy()\n",
    "    print(f\"开始对特征 [{feature}] 进行分箱 (n_bins={n_bins}, strategy='{strategy}')...\")\n",
    "    if feature not in df_binned.columns: return df_binned\n",
    "    if not pd.api.types.is_numeric_dtype(df_binned[feature]): return df_binned\n",
    "\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=None)\n",
    "    feature_binned_col = f'{feature}_分箱'\n",
    "\n",
    "    median_val_train = df_binned.iloc[:n_train][feature].median()\n",
    "    df_binned[feature].fillna(median_val_train, inplace=True)\n",
    "    print(f\"  使用训练集中位数 ({median_val_train:.2f}) 填充了 '{feature}' 的 NaN。\")\n",
    "\n",
    "    try:\n",
    "        train_data_for_fit = df_binned.iloc[:n_train][[feature]]\n",
    "        if train_data_for_fit[feature].nunique() < n_bins:\n",
    "             actual_bins = train_data_for_fit[feature].nunique()\n",
    "             print(f\"  警告: 唯一值 ({actual_bins}) < 箱数 ({n_bins})。\")\n",
    "             if actual_bins < 2: return df_binned\n",
    "             binner = KBinsDiscretizer(n_bins=actual_bins, encode='ordinal', strategy='uniform', subsample=None)\n",
    "        \n",
    "        binner.fit(train_data_for_fit)\n",
    "        df_binned[feature_binned_col] = binner.transform(df_binned[[feature]])\n",
    "        print(f\"  已创建分箱特征 '{feature_binned_col}'。\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  错误: 对特征 '{feature}' 分箱时出错: {e}。跳过...\")\n",
    "        return df_binned\n",
    "\n",
    "    df_binned = pd.get_dummies(df_binned, columns=[feature_binned_col], prefix=f'{feature}段', drop_first=False)\n",
    "    df_binned = df_binned.drop(columns=[feature], errors='ignore')\n",
    "    print(f\"  已对分箱结果进行独热编码并移除原始特征。\")\n",
    "    return df_binned\n",
    "\n",
    "# (使用 log 变换后的特征名)\n",
    "features_to_bin = [\n",
    "    '房龄', '总楼层', # 原始数值\n",
    "    'log_距离中心_公里',\n",
    "    'log_建筑面积',\n",
    "    'log_套内面积',\n",
    "    'log_Location_Target_Encoded'\n",
    "]\n",
    "features_to_bin = [col for col in features_to_bin if col in df_price_eng.columns] # 确保存在\n",
    "\n",
    "for feature in features_to_bin:\n",
    "    df_price_eng = bin_and_encode(df_price_eng, n_train_price, feature=feature, n_bins=5, strategy='kmeans')\n",
    "\n",
    "print(\"分箱处理完成。\")\n",
    "\n",
    "\n",
    "# --- 5.e 创建交互项 ---\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_polynomial_interactions(df, n_train, continuous_cols_candidates, degree=2, interaction_only=True):\n",
    "    \"\"\"为数据集创建多项式交互项特征。\"\"\"\n",
    "    df_poly = df.copy()\n",
    "    print(f\"开始创建交互项 (degree={degree}, interaction_only={interaction_only})...\")\n",
    "\n",
    "    binary_cols = [col for col in df_poly.columns\n",
    "                   if df_poly[col].nunique(dropna=False) == 2 and\n",
    "                      df_poly[col].min() == 0 and df_poly[col].max() == 1]\n",
    "    all_numeric_cols = df_poly.select_dtypes(include=np.number).columns.tolist()\n",
    "    current_continuous_cols = [col for col in all_numeric_cols if col not in binary_cols]\n",
    "    cols_for_poly = [col for col in continuous_cols_candidates if col in current_continuous_cols]\n",
    "\n",
    "    print(f\"  将为以下 {len(cols_for_poly)} 个连续特征创建交互项: {cols_for_poly}\")\n",
    "    if not cols_for_poly: return df_poly\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n",
    "\n",
    "    print(\"  使用训练集的中位数填充交互项特征中的 NaN...\")\n",
    "    train_part_poly = df_poly.iloc[:n_train]\n",
    "    medians_poly = train_part_poly[cols_for_poly].median()\n",
    "    df_poly[cols_for_poly] = df_poly[cols_for_poly].fillna(medians_poly)\n",
    "    if df_poly[cols_for_poly].isnull().any().any():\n",
    "         df_poly[cols_for_poly] = df_poly[cols_for_poly].fillna(0)\n",
    "\n",
    "    try:\n",
    "        poly.fit(df_poly.iloc[:n_train][cols_for_poly])\n",
    "        poly_features = poly.transform(df_poly[cols_for_poly])\n",
    "        poly_feature_names = [name.replace(' ', '_TIMES_').replace('^2', '_SQ') for name in poly.get_feature_names_out(cols_for_poly)]\n",
    "        print(f\"  生成了 {len(poly_feature_names)} 个多项式/交互项特征。\")\n",
    "\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_poly.index)\n",
    "        \n",
    "        df_poly = df_poly.drop(columns=cols_for_poly, errors='ignore')\n",
    "        df_final = pd.concat([df_poly, poly_df], axis=1)\n",
    "\n",
    "        print(f\"交互项创建完毕。数据集新维度: {df_final.shape}\")\n",
    "        return df_final\n",
    "    except Exception as e:\n",
    "        print(f\"  错误: 创建交互项时出错: {e}。返回未修改的数据框。\")\n",
    "        return df_poly\n",
    "\n",
    "# --- (调用 5.e) ---\n",
    "# (使用未被分箱的特征)\n",
    "continuous_cols_for_interactions = [\n",
    "    '得房率',\n",
    "    '室', '厅', '卫'\n",
    "]\n",
    "continuous_cols_for_interactions = [col for col in continuous_cols_for_interactions if col in df_price_eng.columns]\n",
    "\n",
    "df_price_final_eng = create_polynomial_interactions(df_price_eng, n_train_price, continuous_cols_for_interactions)\n",
    "\n",
    "\n",
    "# --- 最终检查 ---\n",
    "print(\"\\\\n\" + \"=\" * 30)\n",
    "print(\"系统性特征工程完成后的数据信息:\")\n",
    "print(f\"最终形状: {df_price_final_eng.shape}\")\n",
    "df_price_final_eng.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 5: 使用 LassoCV 进行特征选择 ---\n",
      "LassoCV 选出的最佳 alpha: 0.001017\n",
      "LassoCV 选出了 92 个特征 (总共 725 个)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 步骤 5: 特征选择 (LassoCV)\n",
    "\n",
    "print(\"\\n--- 步骤 5: 使用 LassoCV 进行特征选择 ---\")\n",
    "\n",
    "# 使用 6 折交叉验证\n",
    "lasso_cv_selector = LassoCV(\n",
    "    cv=6, \n",
    "    random_state=111, \n",
    "    max_iter=5000, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# 在完整的 (清理后的) 训练数据上拟合\n",
    "lasso_cv_selector.fit(X_train_clean_scaled, y_train_clean_log)\n",
    "\n",
    "print(f\"LassoCV 选出的最佳 alpha: {lasso_cv_selector.alpha_:.6f}\")\n",
    "\n",
    "# 创建被选中特征的掩码\n",
    "mask_selected = lasso_cv_selector.coef_ != 0\n",
    "num_selected = mask_selected.sum()\n",
    "num_total = len(mask_selected)\n",
    "\n",
    "print(f\"LassoCV 选出了 {num_selected} 个特征 (总共 {num_total} 个)\")\n",
    "\n",
    "# 应用特征掩码\n",
    "X_train_selected = X_train_scaled[:, mask_selected]\n",
    "X_val_selected = X_val_scaled[:, mask_selected]\n",
    "X_train_clean_selected = X_train_clean_scaled[:, mask_selected]\n",
    "X_test_kaggle_selected = X_test_kaggle_scaled[:, mask_selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 6: 建模、调参与评估 ---\n",
      "  正在训练 OLS...\n",
      "  正在调优 Lasso...\n",
      "    Lasso 最佳 alpha: 0.000046\n",
      "  正在精调 Ridge ...\n",
      "    Ridge 精调后最佳 alpha: 0.000001\n",
      "  正在调优 ElasticNet...\n",
      "    ElasticNet 最佳 alpha: 0.000100\n",
      "    ElasticNet 最佳 l1_ratio: 1.00\n",
      "  正在调优 LightGBM...\n",
      "    LGBM 最佳参数: {'learning_rate': 0.1, 'n_estimators': 500, 'num_leaves': 40, 'subsample': 0.8}\n",
      "\n",
      "--- 步骤 7: 最终结果报告 ---\n",
      "最佳 线性 模型 (基于 CV MAE): ElasticNet\n",
      "\n",
      "========================================\n",
      " 性能报告 (MAE - 原始价格) \n",
      "========================================\n",
      "                   In-Sample MAE  Out-of-Sample MAE  6-Fold CV MAE\n",
      "OLS                   471087.256         469931.274     471480.134\n",
      "Lasso                 471265.017         469349.701     471381.119\n",
      "Ridge                 471339.828         469500.234     471480.134\n",
      "Best Linear Model     471274.902         469259.371     471370.248\n",
      "LightGBM              213423.555         213989.713     241424.393\n",
      "\n",
      "========================================\n",
      " 性能报告 (RMSE - 原始价格) \n",
      "========================================\n",
      "                   In-Sample RMSE  Out-of-Sample RMSE\n",
      "OLS                    847721.839          850417.140\n",
      "Lasso                  848856.335          849736.184\n",
      "Ridge                  848896.918          849937.913\n",
      "Best Linear Model      848974.324          849704.165\n",
      "LightGBM               397015.876          402497.802\n",
      "\n",
      "========================================\n",
      "报告所用的总预测样本数 (移除y异常值后): 103264\n",
      "========================================\n",
      "\n",
      "--- 步骤 8: 生成 Kaggle 提交文件 ---\n",
      "文件将保存到: './output' 文件夹\n",
      "  正在为模型 'OLS' 生成预测...\n",
      "    已生成: './output\\submission_price_OLS.csv'\n",
      "  正在为模型 'Lasso' 生成预测...\n",
      "    已生成: './output\\submission_price_Lasso.csv'\n",
      "  正在为模型 'Ridge' 生成预测...\n",
      "    已生成: './output\\submission_price_Ridge.csv'\n",
      "  正在为模型 'ElasticNet' 生成预测...\n",
      "    已生成: './output\\submission_price_ElasticNet.csv'\n",
      "  正在为模型 'LightGBM' 生成预测...\n",
      "    已生成: './output\\submission_price_LightGBM.csv'\n",
      "  正在为模型 'Best Linear Model' 生成预测...\n",
      "    已生成: './output\\submission_price_Best_Linear_Model.csv'\n",
      "\n",
      "--- 所有提交文件已生成完毕 ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 步骤 6: 建模、调参与评估\n",
    "\n",
    "print(\"\\n--- 步骤 6: 建模、调参与评估 ---\")\n",
    "\n",
    "# 准备 6 折交叉验证\n",
    "cv_6 = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "# 自定义评分函数 \n",
    "# 注意：GridSearchCV 和 cross_val_score 默认 \"分数越大越好\"，而 MAE 是 \"越小越好\"，所以返回负数\n",
    "def mae_original_scorer(y_true_log, y_pred_log):\n",
    "    y_true_orig = np.expm1(y_true_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    return -mae # 返回负MAE\n",
    "\n",
    "mae_scorer = make_scorer(mae_original_scorer, greater_is_better=True)\n",
    "\n",
    "# 存储结果的字典\n",
    "results = {}\n",
    "best_models = {} # 存储最佳模型对象\n",
    "\n",
    "# --- 辅助函数：用于计算和打印 IS 和 OOS 指标 ---\n",
    "def evaluate_model(model, model_name):\n",
    "    # 1. In-Sample (IS) 评估 (在 X_train_selected 上)\n",
    "    y_pred_log_is = model.predict(X_train_selected)\n",
    "    y_pred_orig_is = np.expm1(y_pred_log_is)\n",
    "    is_mae = mean_absolute_error(y_train_orig, y_pred_orig_is)\n",
    "    is_rmse = np.sqrt(mean_squared_error(y_train_orig, y_pred_orig_is))\n",
    "    \n",
    "    # 2. Out-of-Sample (OOS) 评估 (在 X_val_selected 上)\n",
    "    y_pred_log_oos = model.predict(X_val_selected)\n",
    "    y_pred_orig_oos = np.expm1(y_pred_log_oos)\n",
    "    oos_mae = mean_absolute_error(y_val_orig, y_pred_orig_oos)\n",
    "    oos_rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred_orig_oos))\n",
    "    \n",
    "    return {\n",
    "        \"IS_MAE\": is_mae, \"IS_RMSE\": is_rmse,\n",
    "        \"OOS_MAE\": oos_mae, \"OOS_RMSE\": oos_rmse,\n",
    "        \"CV_MAE\": np.nan, \"CV_RMSE\": np.nan # CV 将单独计算\n",
    "    }\n",
    "\n",
    "# --- 6.1: OLS (Linear Regression) ---\n",
    "print(\"  正在训练 OLS...\")\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X_train_selected, y_train_log)\n",
    "\n",
    "# 评估 IS 和 OOS\n",
    "results['OLS'] = evaluate_model(model_ols, 'OLS')\n",
    "\n",
    "# 评估 CV (使用完整的清理后数据)\n",
    "cv_scores = cross_val_score(model_ols, X_train_clean_selected, y_train_clean_log, \n",
    "                            cv=cv_6, scoring=mae_scorer, n_jobs=-1)\n",
    "results['OLS']['CV_MAE'] = -np.mean(cv_scores)\n",
    "best_models['OLS'] = model_ols\n",
    "\n",
    "\n",
    "# --- 6.2: Lasso ---\n",
    "print(\"  正在调优 Lasso...\")\n",
    "param_grid_lasso = {'alpha': np.logspace(-6, -1, 10)}\n",
    "grid_lasso = GridSearchCV(\n",
    "    Lasso(max_iter=5000, random_state=111),\n",
    "    param_grid_lasso,\n",
    "    cv=cv_6,\n",
    "    scoring=mae_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# 在完整的清理后数据上拟合\n",
    "grid_lasso.fit(X_train_clean_selected, y_train_clean_log)\n",
    "model_lasso_best = grid_lasso.best_estimator_\n",
    "\n",
    "# 评估 IS 和 OOS\n",
    "results['Lasso'] = evaluate_model(model_lasso_best, 'Lasso')\n",
    "# CV 分数直接来自 GridSearch\n",
    "results['Lasso']['CV_MAE'] = -grid_lasso.best_score_\n",
    "best_models['Lasso'] = model_lasso_best\n",
    "print(f\"    Lasso 最佳 alpha: {grid_lasso.best_params_['alpha']:.6f}\")\n",
    "\n",
    "\n",
    "# --- 6.3: Ridge  ---\n",
    "print(\"  正在精调 Ridge ...\")\n",
    "param_grid_ridge_fine = {'alpha': np.logspace(-6, 0, 10)} \n",
    "grid_ridge = GridSearchCV(\n",
    "    Ridge(random_state=111),\n",
    "    param_grid_ridge_fine, \n",
    "    cv=cv_6,\n",
    "    scoring=mae_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_ridge.fit(X_train_clean_selected, y_train_clean_log)\n",
    "model_ridge_best = grid_ridge.best_estimator_\n",
    "\n",
    "\n",
    "results['Ridge'] = evaluate_model(model_ridge_best, 'Ridge')\n",
    "results['Ridge']['CV_MAE'] = -grid_ridge.best_score_\n",
    "best_models['Ridge'] = model_ridge_best\n",
    "print(f\"    Ridge 精调后最佳 alpha: {grid_ridge.best_params_['alpha']:.6f}\")\n",
    "\n",
    "# --- 6.4: ElasticNet---\n",
    "print(\"  正在调优 ElasticNet...\")\n",
    "param_grid_enet = {\n",
    "    'alpha': np.logspace(-6, -1, 6),\n",
    "    'l1_ratio': [0.1, 0.5, 0.9, 0.95, 1.0]\n",
    "}\n",
    "grid_enet = GridSearchCV(\n",
    "    ElasticNet(max_iter=5000, random_state=111),\n",
    "    param_grid_enet,\n",
    "    cv=cv_6,\n",
    "    scoring=mae_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_enet.fit(X_train_clean_selected, y_train_clean_log)\n",
    "model_enet_best = grid_enet.best_estimator_\n",
    "\n",
    "# 评估 IS 和 OOS\n",
    "results['ElasticNet'] = evaluate_model(model_enet_best, 'ElasticNet')\n",
    "# CV 分数\n",
    "results['ElasticNet']['CV_MAE'] = -grid_enet.best_score_\n",
    "best_models['ElasticNet'] = model_enet_best\n",
    "print(f\"    ElasticNet 最佳 alpha: {grid_enet.best_params_['alpha']:.6f}\")\n",
    "print(f\"    ElasticNet 最佳 l1_ratio: {grid_enet.best_params_['l1_ratio']:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "print(\"  正在调优 LightGBM...\")\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [20, 31, 40],\n",
    "    'subsample': [0.8] \n",
    "}\n",
    "\n",
    "grid_lgbm = GridSearchCV(\n",
    "    \n",
    "    lgb.LGBMRegressor(random_state=111, n_jobs=1, verbose=-1, subsample_freq=1),\n",
    "    param_grid_lgbm,\n",
    "    cv=cv_6,\n",
    "    scoring=mae_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=0 \n",
    ")\n",
    "\n",
    "# 在完整的清理后数据上拟合\n",
    "grid_lgbm.fit(X_train_clean_selected, y_train_clean_log)\n",
    "model_lgbm_best = grid_lgbm.best_estimator_\n",
    "\n",
    "# 评估 IS 和 OOS\n",
    "results['LightGBM'] = evaluate_model(model_lgbm_best, 'LightGBM')\n",
    "# CV 分数\n",
    "results['LightGBM']['CV_MAE'] = -grid_lgbm.best_score_\n",
    "best_models['LightGBM'] = model_lgbm_best\n",
    "print(f\"    LGBM 最佳参数: {grid_lgbm.best_params_}\")\n",
    "\n",
    "\n",
    "# 步骤 7: 结果报告 \n",
    "\n",
    "print(\"\\n--- 步骤 7: 最终结果报告 ---\")\n",
    "\n",
    "# --- 找到最佳的 *线性* 模型 ---\n",
    "\n",
    "linear_models_to_compare = {\n",
    "    'OLS': results['OLS']['CV_MAE'],\n",
    "    'Lasso': results['Lasso']['CV_MAE'],\n",
    "    'Ridge': results['Ridge']['CV_MAE'],\n",
    "    'ElasticNet': results['ElasticNet']['CV_MAE']\n",
    "}\n",
    "best_linear_model_name = min(linear_models_to_compare, key=linear_models_to_compare.get)\n",
    "\n",
    "# 将最佳线性模型的数据复制到 'Best Linear Model' 键\n",
    "results['Best Linear Model'] = results[best_linear_model_name]\n",
    "best_models['Best Linear Model'] = best_models[best_linear_model_name]\n",
    "\n",
    "print(f\"最佳 线性 模型 (基于 CV MAE): {best_linear_model_name}\")\n",
    "\n",
    "# --- 准备 MAE 报告 ---\n",
    "report_data_mae = {\n",
    "    'In-Sample MAE': [\n",
    "        results['OLS']['IS_MAE'],\n",
    "        results['Lasso']['IS_MAE'],\n",
    "        results['Ridge']['IS_MAE'],\n",
    "        results['Best Linear Model']['IS_MAE'],\n",
    "        results['LightGBM']['IS_MAE']  \n",
    "    ],\n",
    "    'Out-of-Sample MAE': [\n",
    "        results['OLS']['OOS_MAE'],\n",
    "        results['Lasso']['OOS_MAE'],\n",
    "        results['Ridge']['OOS_MAE'],\n",
    "        results['Best Linear Model']['OOS_MAE'],\n",
    "        results['LightGBM']['OOS_MAE']  \n",
    "    ],\n",
    "    '6-Fold CV MAE': [\n",
    "        results['OLS']['CV_MAE'],\n",
    "        results['Lasso']['CV_MAE'],\n",
    "        results['Ridge']['CV_MAE'],\n",
    "        results['Best Linear Model']['CV_MAE'],\n",
    "        results['LightGBM']['CV_MAE']  \n",
    "    ]\n",
    "}\n",
    "df_report_mae = pd.DataFrame(\n",
    "    report_data_mae,\n",
    "    index=['OLS', 'Lasso', 'Ridge', 'Best Linear Model', 'LightGBM'] # <-- 新增\n",
    ")\n",
    "\n",
    "# --- 准备 RMSE 报告 (\"RMAE\") ---\n",
    "report_data_rmse = {\n",
    "    'In-Sample RMSE': [\n",
    "        results['OLS']['IS_RMSE'],\n",
    "        results['Lasso']['IS_RMSE'],\n",
    "        results['Ridge']['IS_RMSE'],\n",
    "        results['Best Linear Model']['IS_RMSE'],\n",
    "        results['LightGBM']['IS_RMSE']  # <-- 新增\n",
    "    ],\n",
    "    'Out-of-Sample RMSE': [\n",
    "        results['OLS']['OOS_RMSE'],\n",
    "        results['Lasso']['OOS_RMSE'],\n",
    "        results['Ridge']['OOS_RMSE'],\n",
    "        results['Best Linear Model']['OOS_RMSE'],\n",
    "        results['LightGBM']['OOS_RMSE']  # <-- 新增\n",
    "    ]\n",
    "}\n",
    "df_report_rmse = pd.DataFrame(\n",
    "    report_data_rmse,\n",
    "    index=['OLS', 'Lasso', 'Ridge', 'Best Linear Model', 'LightGBM']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" 性能报告 (MAE - 原始价格) \")\n",
    "print(\"=\"*40)\n",
    "print(df_report_mae)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" 性能报告 (RMSE - 原始价格) \")\n",
    "print(\"=\"*40)\n",
    "print(df_report_rmse)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"报告所用的总预测样本数 (移除y异常值后): {len(y_train_clean_orig)}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 8: 生成 Kaggle 提交文件 ---\n",
      "文件将保存到: './output' 文件夹\n",
      "  正在为模型 'OLS' 生成预测...\n",
      "    已生成: './output\\submission_price_OLS.csv'\n",
      "  正在为模型 'Lasso' 生成预测...\n",
      "    已生成: './output\\submission_price_Lasso.csv'\n",
      "  正在为模型 'Ridge' 生成预测...\n",
      "    已生成: './output\\submission_price_Ridge.csv'\n",
      "  正在为模型 'ElasticNet' 生成预测...\n",
      "    已生成: './output\\submission_price_ElasticNet.csv'\n",
      "  正在为模型 'LightGBM' 生成预测...\n",
      "    已生成: './output\\submission_price_LightGBM.csv'\n",
      "  正在为模型 'Best Linear Model' 生成预测...\n",
      "    已生成: './output\\submission_price_Best_Linear_Model.csv'\n",
      "\n",
      "--- 所有提交文件已生成完毕 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 步骤 8: 为 *所有* 模型生成 Kaggle 提交文件\n",
    "\n",
    "print(\"\\n--- 步骤 8: 生成 Kaggle 提交文件 ---\")\n",
    "\n",
    "# 1. 定义输出文件夹\n",
    "output_dir = './output'\n",
    "# 2. 创建文件夹 \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"文件将保存到: '{output_dir}' 文件夹\")\n",
    "\n",
    "# 3. 遍历 `best_models` 字典中的每一个模型\n",
    "#    (这包括 OLS, Lasso, Ridge, ElasticNet, LightGBM, 和 'Best Linear Model')\n",
    "for model_name, final_model in best_models.items():\n",
    "    \n",
    "    print(f\"  正在为模型 '{model_name}' 生成预测...\")\n",
    "    \n",
    "    # 4. 在 Kaggle 测试集上预测 (已缩放、已选择特征)\n",
    "    y_kaggle_pred_log = final_model.predict(X_test_kaggle_selected)\n",
    "\n",
    "    # 5. 转换回原始价格\n",
    "    y_kaggle_pred_orig = np.expm1(y_kaggle_pred_log)\n",
    "\n",
    "    # 6. 创建提交文件\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_ids_price,\n",
    "        'Price': y_kaggle_pred_orig\n",
    "    })\n",
    "\n",
    "    # 7. 检查是否有负数预测\n",
    "    submission['Price'] = submission['Price'].clip(lower=0)\n",
    "\n",
    "   \n",
    "    safe_model_name = model_name.replace(' ', '_') # 替换空格\n",
    "    file_name = f'submission_price_{safe_model_name}.csv'\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    #  保存文件\n",
    "    submission.to_csv(file_path, index=False)\n",
    "    print(f\"    已生成: '{file_path}'\")\n",
    "\n",
    "print(\"\\n--- 所有提交文件已生成完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rent数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "# --- 辅助函数：地理位置  ---\n",
    "\n",
    "def handle_district_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '区县' 列 (数字型分类特征)。\n",
    "    1. 填充 NaN 为 '未知'。\n",
    "    2. 转换为 'string' 类型。\n",
    "    \"\"\"\n",
    "    col = '区县'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        return df\n",
    "    df[col] = df[col].fillna('未知')\n",
    "    df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "def handle_board_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '板块' 列。\n",
    "    1. 填充 NaN 为 '未知'。\n",
    "    2. 转换为 'string' 类型。\n",
    "    \"\"\"\n",
    "    col = '板块'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        return df\n",
    "    df[col] = df[col].fillna('未知')\n",
    "    df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "def handle_ring_road_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '环线位置' 列:\n",
    "    1. 填充NaN为 '未知'\n",
    "    2. 进行有序编码\n",
    "    \"\"\"\n",
    "    col = '环线位置'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['环线_ordinal'] = 5\n",
    "        return df\n",
    "\n",
    "    df[col] = df[col].fillna('未知')\n",
    "\n",
    "    ring_map = {\n",
    "        '二环内': 1, '二至三环': 1,\n",
    "        '三至四环': 2, '四至五环': 2,\n",
    "        '五至六环': 3,\n",
    "        '六环外': 4,\n",
    "        '内环内': 1, '内环至中环': 2,\n",
    "        '中环至外环': 3, '内环至外环': 3, # Grouped similar shanghai rings\n",
    "        '外环外': 4,\n",
    "        '未知': 4 # Treat unknown as outer ring\n",
    "    }\n",
    "\n",
    "    df['环线_ordinal'] = df[col].map(ring_map).fillna(5).astype(int) # Use 5 for values not in map\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "# --- 地理空间特征  ---\n",
    "def compute_city_center_and_distances_r(df, n_train):\n",
    "    \"\"\"计算城市中心点及房源到中心点的距离。 (基于 n_train 计算中心)\"\"\"\n",
    "    df_out = df.copy()\n",
    "    print(\"Calculating geospatial features (distance to center)...\")\n",
    "    if '城市' not in df_out.columns or 'lon' not in df_out.columns or 'lat' not in df_out.columns:\n",
    "        print(\"  Warning: Missing '城市', 'lon', or 'lat'. Cannot calculate distance features.\")\n",
    "        \n",
    "        df_out['距离中心_公里'] = 15.0 \n",
    "        df_out['距离中心_公里_平方'] = 225.0\n",
    "        return df_out\n",
    "\n",
    "    train_part = df_out.iloc[:n_train]\n",
    "    city_centers = train_part.groupby('城市', observed=True)[['lon', 'lat']].mean().reset_index()\n",
    "    city_centers = city_centers.rename(columns={'lon': 'center_lon', 'lat': 'center_lat'})\n",
    "    print(f\"  Calculated centers for {len(city_centers)} cities (based on training set).\" )\n",
    "\n",
    "    df_out = pd.merge(df_out, city_centers, on='城市', how='left')\n",
    "\n",
    "    def compute_distance(row):\n",
    "        if pd.isna(row['lat']) or pd.isna(row['lon']) or pd.isna(row['center_lat']) or pd.isna(row['center_lon']):\n",
    "            return np.nan\n",
    "        try:\n",
    "            return geodesic((row['lat'], row['lon']), (row['center_lat'], row['center_lon'])).km\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    df_out['距离中心_公里'] = df_out.apply(compute_distance, axis=1)\n",
    "\n",
    "   \n",
    "    median_dist_train = df_out.iloc[:n_train]['距离中心_公里'].median()\n",
    "    if pd.isna(median_dist_train): median_dist_train = 15.0 # Fallback median\n",
    "    df_out['距离中心_公里'].fillna(median_dist_train, inplace=True)\n",
    "    print(f\"  Calculated '距离中心_公里', imputed NaNs using train median ({median_dist_train:.2f} km).\")\n",
    "\n",
    "    df_out['距离中心_公里_平方'] = df_out['距离中心_公里'] ** 2\n",
    "    df_out = df_out.drop(columns=['center_lon', 'center_lat'], errors='ignore')\n",
    "    print(\"Distance features created.\")\n",
    "    return df_out\n",
    "\n",
    "def create_geo_clusters_r(df, n_train, n_clusters=20, city_col='城市', lon_col='lon', lat_col='lat'):\n",
    "    \"\"\"为每个城市计算地理聚类并进行独热编码。(Fit ONLY on n_train)\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    print(f\"Creating geo clusters ({n_clusters} per city)...\")\n",
    "\n",
    "    required_cols = [city_col, lon_col, lat_col]\n",
    "    if not all(col in df_processed.columns for col in required_cols):\n",
    "        print(f\"  Error: Missing required columns: {required_cols}. Skipping clustering...\")\n",
    "        return df_processed\n",
    "\n",
    "    \n",
    "    train_part_coords = df_processed.iloc[:n_train]\n",
    "    median_lon_train = train_part_coords[lon_col].median()\n",
    "    median_lat_train = train_part_coords[lat_col].median()\n",
    "    if pd.isna(median_lon_train): median_lon_train = 116.4 \n",
    "    if pd.isna(median_lat_train): median_lat_train = 39.9 \n",
    "\n",
    "    df_processed[lon_col].fillna(median_lon_train, inplace=True)\n",
    "    df_processed[lat_col].fillna(median_lat_train, inplace=True)\n",
    "\n",
    "    cluster_col_temp = '地理聚类_temp'\n",
    "    df_processed[cluster_col_temp] = -1\n",
    "    all_city_labels = df_processed[city_col].unique()\n",
    "\n",
    "    kmeans_models = {} \n",
    "\n",
    "    for city_label in all_city_labels:\n",
    "        city_mask = (df_processed[city_col] == city_label)\n",
    "        city_data_train = df_processed.loc[city_mask & (df_processed.index < n_train), [lon_col, lat_col]]\n",
    "\n",
    "        if len(city_data_train) >= n_clusters:\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "                kmeans.fit(city_data_train)\n",
    "                kmeans_models[city_label] = kmeans \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: KMeans failed for city {city_label}: {e}\")\n",
    "        elif len(city_data_train) > 0:\n",
    "             print(f\"    Warning: Not enough training data points ({len(city_data_train)}) for city {city_label}. Skipping clustering.\")\n",
    "\n",
    "    # Now predict for ALL data using the stored models\n",
    "    for city_label, kmeans in kmeans_models.items():\n",
    "        city_mask = (df_processed[city_col] == city_label)\n",
    "        city_data_all = df_processed.loc[city_mask, [lon_col, lat_col]]\n",
    "        if not city_data_all.empty:\n",
    "            clusters_all = kmeans.predict(city_data_all)\n",
    "            df_processed.loc[city_data_all.index, cluster_col_temp] = clusters_all\n",
    "\n",
    "    combined_label_col = '地理聚类_带城市'\n",
    "    df_processed[combined_label_col] = np.where(\n",
    "         (df_processed[cluster_col_temp] != -1) & df_processed[city_col].notna(),\n",
    "         'C' + df_processed[city_col].astype(str).str.split('.').str[0] + '_' + df_processed[cluster_col_temp].astype(str),\n",
    "         'GeoCluster_Unknown'\n",
    "    )\n",
    "    df_processed = pd.get_dummies(df_processed, columns=[combined_label_col], prefix='GeoCluster', drop_first=False)\n",
    "    df_processed = df_processed.drop(columns=[cluster_col_temp], errors='ignore')\n",
    "    print(\"Geo cluster features created.\")\n",
    "    return df_processed\n",
    "\n",
    "# --- 辅助函数：房屋基本属性 ---\n",
    "\n",
    "def handle_rent_house_type_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '户型' 列:\n",
    "    1. 预处理 '.', '车库', '未知室', '居室', '房间'\n",
    "    2. 提取 '室', '厅', '卫' 数量\n",
    "    3. 使用【训练集】中位数填充所有NaN值\n",
    "    \"\"\"\n",
    "    print(\"Processing [户型]...\")\n",
    "    col = '户型'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        # Create dummy columns if missing\n",
    "        df['卧室数'] = 2.0\n",
    "        df['客厅数'] = 1.0\n",
    "        df['卫生间数'] = 1.0\n",
    "        return df\n",
    "\n",
    "    house_type = df[col].copy()\n",
    "    house_type = house_type.replace('.', np.nan)\n",
    "    house_type = house_type.astype(str) # Ensure string type before replace\n",
    "    house_type = house_type.replace('车库', '0室0厅0卫', regex=False)\n",
    "    house_type = house_type.replace('未知室', '0室', regex=False)\n",
    "    house_type = house_type.replace('房间', '室', regex=False)\n",
    "    house_type = house_type.replace('居室', '室', regex=False)\n",
    "\n",
    "    # Use regex that handles missing parts better\n",
    "    layout_info = house_type.str.extract(r'(\\d+)[室房居](?:(\\d+)厅)?(?:.*?(\\d+)卫)?', expand=True)\n",
    "    layout_info.columns = ['卧室数', '客厅数', '卫生间数']\n",
    "    df_extracted = pd.DataFrame(index=df.index)\n",
    "    for c in layout_info.columns:\n",
    "        df_extracted[c] = pd.to_numeric(layout_info[c], errors='coerce')\n",
    "\n",
    "    print(f\"  Calculating medians based on first {n_train} rows...\")\n",
    "    train_part = df_extracted.iloc[:n_train]\n",
    "    medians = {}\n",
    "    fallback_medians = {'卧室数': 2.0, '客厅数': 1.0, '卫生间数': 1.0}\n",
    "\n",
    "    for col_name in ['卧室数', '客厅数', '卫生间数']:\n",
    "        # Calculate median excluding 0 for more representative imputation\n",
    "        temp_col_for_median = train_part[col_name].replace(0, np.nan).dropna()\n",
    "        median_val = temp_col_for_median.median() if not temp_col_for_median.empty else np.nan\n",
    "\n",
    "        if pd.isna(median_val) or median_val == 0:\n",
    "            median_val = fallback_medians[col_name]\n",
    "            print(f\"    Warning: Median for '{col_name}' failed or was 0. Using fallback {median_val}\")\n",
    "        else:\n",
    "            print(f\"    Median for '{col_name}' (excluding 0): {median_val}\")\n",
    "        medians[col_name] = median_val\n",
    "        # Impute NaNs in the FULL extracted dataframe\n",
    "        df_extracted[col_name].fillna(median_val, inplace=True)\n",
    "\n",
    "    # Add the processed columns to the original dataframe\n",
    "    df[['卧室数', '客厅数', '卫生间数']] = df_extracted[['卧室数', '客厅数', '卫生间数']]\n",
    "    print(\"Imputation complete.\")\n",
    "\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "def handle_listing_date_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '交易时间' 列 (YYYY-MM-DD 格式)\n",
    "    1. 提取 'Listing_Year' (分类)\n",
    "    2. 提取 'Listing_Month' (分类)\n",
    "    3. 用【训练集】众数填充未知值 (NaN)\n",
    "    \"\"\"\n",
    "    col = '交易时间'\n",
    "    print(f\"Processing [{col}] (Extracting Year and Month categories)...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "    \n",
    "        df['Listing_Year'] = '2024'\n",
    "        df['Listing_Month'] = '6'\n",
    "        return df\n",
    "\n",
    "    parsed_dates = pd.to_datetime(df[col], errors='coerce')\n",
    "    df['Listing_Year'] = parsed_dates.dt.year.astype(str)\n",
    "    df['Listing_Month'] = parsed_dates.dt.month.astype(str)\n",
    "\n",
    "    # --- 仅使用训练集进行插值 ---\n",
    "    train_part = df.iloc[:n_train]\n",
    "    year_mode = train_part['Listing_Year'].mode()\n",
    "    month_mode = train_part['Listing_Month'].mode()\n",
    "\n",
    "    fill_year = year_mode[0] if not year_mode.empty else '2024' # Fallback mode\n",
    "    fill_month = month_mode[0] if not month_mode.empty else '6'  # Fallback mode\n",
    "\n",
    "    df['Listing_Year'] = df['Listing_Year'].replace('NaT', fill_year)\n",
    "    df['Listing_Month'] = df['Listing_Month'].replace('NaT', fill_month)\n",
    "\n",
    "    print(f\"  Created 'Listing_Year' (imputed with train mode: {fill_year})\")\n",
    "    print(f\"  Created 'Listing_Month' (imputed with train mode: {fill_month})\")\n",
    "\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "def handle_rent_floor(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '楼层' 列 (Rent version, revised)\n",
    "    Imputation based on n_train.\n",
    "    \"\"\"\n",
    "    print(\"Processing [楼层]...\")\n",
    "    col = '楼层'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        # Create dummy/default columns\n",
    "        df['总楼层数'] = 18.0\n",
    "        df['楼层位置'] = '中楼层'\n",
    "        df['相对楼层'] = 0.5\n",
    "        df['绝对楼层'] = 9.0\n",
    "        df['总楼层类型'] = '中高层(12-19)'\n",
    "        df['地下室标志'] = 0\n",
    "        df['顶层标志'] = 0\n",
    "        df['相对楼层_总楼层数'] = 9.0\n",
    "        df['相对楼层_平方'] = 0.25\n",
    "        df['总楼层数_平方'] = 324.0\n",
    "        df['相对楼层分箱'] = '中(0.4-0.6)'\n",
    "        df['总楼层数分箱'] = '12-19层'\n",
    "        df['楼层位置_总楼层类型'] = '中楼层_中高层(12-19)'\n",
    "        return df\n",
    "\n",
    "    floor_col = df[col].copy().fillna('未知')\n",
    "    floor_col = floor_col.replace('.', '未知')\n",
    "    floor_col = floor_col.astype(str) # Ensure string for extraction\n",
    "\n",
    "    # --- 1. Extract Total Floors ---\n",
    "    df['总楼层数'] = floor_col.str.extract(r'/(\\d+)层?', expand=False).astype(float)\n",
    "\n",
    "    # --- 仅使用训练集进行插值 ---\n",
    "    train_part = df.iloc[:n_train]\n",
    "    median_total_floor_train = train_part.loc[train_part['总楼层数'] > 0, '总楼层数'].median()\n",
    "    if pd.isna(median_total_floor_train) or median_total_floor_train == 0:\n",
    "        median_total_floor_train = 18.0 \n",
    "    print(f\"  Median '总楼层数' (train): {median_total_floor_train}\")\n",
    "    df['总楼层数'].fillna(median_total_floor_train, inplace=True)\n",
    "    df['总楼层数'] = df['总楼层数'].replace(0, median_total_floor_train)\n",
    "\n",
    "    # --- 2. 提取精确绝对楼层 ---\n",
    "    abs_floor_1 = floor_col.str.extract(r'^(\\d+)/', expand=False).astype(float)\n",
    "    abs_floor_2 = floor_col.str.extract(r'地下(\\d+)层', expand=False).astype(float) * -1\n",
    "    abs_floor_3 = floor_col.str.contains('地下室/').map({True: -1.0, False: np.nan})\n",
    "    df['绝对楼层_precise'] = abs_floor_1.fillna(abs_floor_2).fillna(abs_floor_3)\n",
    "\n",
    "    # --- 3. 提取楼层位置 ---\n",
    "    df['楼层位置'] = floor_col.str.extract(r'(地下室|地下|低楼层|中楼层|高楼层)', expand=False)\n",
    "    df['楼层位置'] = df['楼层位置'].replace('地下', '地下室')\n",
    "    is_top = (df['绝对楼层_precise'] == df['总楼层数']) & (df['总楼层数'] > 0)\n",
    "    is_bottom = (df['绝对楼层_precise'] == 1)\n",
    "    is_basement = (df['绝对楼层_precise'] < 0)\n",
    "    df.loc[is_top, '楼层位置'] = '顶层'\n",
    "    df.loc[is_bottom, '楼层位置'] = '底层'\n",
    "    df.loc[is_basement, '楼层位置'] = '地下室'\n",
    "\n",
    "   \n",
    "    mode_floor_pos_train = df.iloc[:n_train]['楼层位置'].mode()\n",
    "    fill_pos_train = mode_floor_pos_train[0] if not mode_floor_pos_train.empty else '中楼层'\n",
    "    df['楼层位置'].fillna(fill_pos_train, inplace=True)\n",
    "\n",
    "    # --- 4. 估算相对下限 ---\n",
    "    floor_position_map = {'地下室': 0.0, '底层': 0.05, '低楼层': 0.25, '中楼层': 0.5, '高楼层': 0.75, '顶层': 1.0, '未知': 0.5}\n",
    "    df['相对楼层'] = df['楼层位置'].map(floor_position_map)\n",
    "    precise_relative = (df['绝对楼层_precise'] / df['总楼层数']).replace([np.inf, -np.inf], np.nan)\n",
    "    df['相对楼层'] = precise_relative.fillna(df['相对楼层'])\n",
    "    df['相对楼层'] = df['相对楼层'].clip(lower=-0.1) # Clip potential negatives from 지하\n",
    "\n",
    "    df['绝对楼层'] = df['绝对楼层_precise']\n",
    "    estimated_absolute = (df['相对楼层'] * df['总楼层数']).round()\n",
    "    df['绝对楼层'].fillna(estimated_absolute, inplace=True)\n",
    "    # --- 最终绝对下限插值仅使用训练集中位数 ---\n",
    "    median_abs_floor_train = df.iloc[:n_train].loc[df.iloc[:n_train]['绝对楼层'] > 0, '绝对楼层'].median()\n",
    "    if pd.isna(median_abs_floor_train): median_abs_floor_train = 3.0 # Fallback\n",
    "    df['绝对楼层'].fillna(median_abs_floor_train, inplace=True)\n",
    "\n",
    "    # --- 6. 创建衍生特征 ---\n",
    "    df['总楼层类型'] = pd.cut(df['总楼层数'], bins=[0, 6, 11, 19, 100], labels=['低层(1-6)', '小高层(7-11)', '中高层(12-19)', '高层(20+)'], include_lowest=True, right=True)\n",
    "    df['总楼层类型'] = df['总楼层类型'].cat.add_categories('未知').fillna('未知')\n",
    "    df['地下室标志'] = (df['楼层位置'] == '地下室').astype(int)\n",
    "    df['顶层标志'] = (df['楼层位置'] == '顶层').astype(int)\n",
    "    df['相对楼层_总楼层数'] = df['相对楼层'] * df['总楼层数']\n",
    "    df['相对楼层_平方'] = df['相对楼层'] ** 2\n",
    "    df['总楼层数_平方'] = df['总楼层数'] ** 2\n",
    "    df['相对楼层分箱'] = pd.cut(df['相对楼层'], bins=[-0.1, 0.2, 0.4, 0.6, 0.8, 1.0], labels=['地下/极低', '低(0.2-0.4)', '中(0.4-0.6)', '高(0.6-0.8)', '极高(0.8-1.0)'], include_lowest=True)\n",
    "    df['相对楼层分箱'] = df['相对楼层分箱'].cat.add_categories('未知').fillna('未知')\n",
    "    df['总楼层数分箱'] = pd.cut(df['总楼层数'], bins=[0, 6, 11, 19, 100], labels=['1-6层', '7-11层', '12-19层', '20+层'], include_lowest=True)\n",
    "    df['总楼层数分箱'] = df['总楼层数分箱'].cat.add_categories('未知').fillna('未知')\n",
    "    df['楼层位置_总楼层类型'] = df['楼层位置'].astype(str) + '_' + df['总楼层类型'].astype(str)\n",
    "\n",
    "    print(f\"  Floor position distribution (modes): {df.iloc[:n_train]['楼层位置'].mode().tolist()}\")\n",
    "\n",
    "    \n",
    "    return df.drop([col, '绝对楼层_precise'], axis=1, errors='ignore')\n",
    "\n",
    "def handle_building_structure_r(df, n_train):  \n",
    "    \"\"\"\n",
    "    处理 '建筑结构' 列 (Multi-Hot)\n",
    "    \"\"\"\n",
    "    print(\"Processing [建筑结构] (Rent version - Multi-Hot encoding)...\")\n",
    "    col = '建筑结构'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        # Create dummy columns\n",
    "        df['is_BType_Tower'] = 0\n",
    "        df['is_BType_Slab'] = 0\n",
    "        df['is_BType_Combo'] = 0\n",
    "        df['is_BType_Bungalow'] = 0\n",
    "        return df\n",
    "\n",
    "    structure_col = df[col].copy().fillna('未知')\n",
    "    structure_col = structure_col.replace(r'^\\s*$', '未知', regex=True)\n",
    "    structure_col = structure_col.replace('.', '未知')\n",
    "    structure_col = structure_col.astype(str) # Ensure string\n",
    "\n",
    "    df['is_BType_Tower'] = structure_col.str.contains('塔楼').astype(int)\n",
    "    df['is_BType_Slab'] = structure_col.str.contains('板楼').astype(int)\n",
    "    df['is_BType_Combo'] = structure_col.str.contains('塔板结合').astype(int)\n",
    "    df['is_BType_Bungalow'] = structure_col.str.contains('平房').astype(int)\n",
    "    print(\"  Created 'is_BType_Tower', 'is_BType_Slab', 'is_BType_Combo', 'is_BType_Bungalow'.\")\n",
    "\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "def handle_rent_area(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '面积' 列\n",
    "    \"\"\"\n",
    "    print(\"Processing [面积]...\")\n",
    "    col = '面积'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = 70.0  \n",
    "        return df\n",
    "\n",
    "    series_str = df[col].astype(str).copy()\n",
    "    cleaned_str_series = series_str.str.replace(r'[\\s\\xa0\\u3000]+|㎡', '', regex=True)\n",
    "    extracted_area_str = cleaned_str_series.str.extract(r'^(\\d+\\.?\\d*)$', expand=False)\n",
    "    area_numeric = pd.to_numeric(extracted_area_str, errors='coerce')\n",
    "\n",
    "    # --- 仅使用训练集中位数进行插值 ---\n",
    "    train_part = area_numeric.iloc[:n_train]\n",
    "    valid_areas_train = train_part.dropna()\n",
    "    valid_areas_train = valid_areas_train[valid_areas_train > 0]\n",
    "    median_area_train = np.nan\n",
    "    if not valid_areas_train.empty:\n",
    "        median_area_train = valid_areas_train.median()\n",
    "    if pd.isna(median_area_train):\n",
    "        median_area_train = 70.0 # Fallback median\n",
    "        print(f\"  Warning: Could not calculate valid train median for '{col}'. Using fallback {median_area_train}.\")\n",
    "\n",
    "    filled_area = area_numeric.fillna(median_area_train)\n",
    "   \n",
    "    df[col] = filled_area.apply(lambda x: median_area_train if pd.isna(x) or x <= 0 else x)\n",
    "\n",
    "    print(f\"  Cleaned and imputed '{col}' using train median ({median_area_train:.2f}).\")\n",
    "    return df\n",
    "\n",
    "def handle_rent_orientation(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '朝向' 列 (Multi-Hot, Unknown sets all flags to 0).\n",
    "    \"\"\"\n",
    "    print(\"Processing [朝向]...\")\n",
    "    col = '朝向'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['is_朝南'] = 0\n",
    "        df['is_朝东'] = 0\n",
    "        df['is_朝西'] = 0\n",
    "        df['is_朝北'] = 0\n",
    "        return df\n",
    "\n",
    "    orientation_col = df[col].copy().fillna('未知')\n",
    "    orientation_col = orientation_col.replace('.', '未知')\n",
    "    orientation_col = orientation_col.astype(str) # Ensure string\n",
    "\n",
    "    df['is_朝南'] = orientation_col.str.contains('南').astype(int)\n",
    "    df['is_朝东'] = orientation_col.str.contains('东').astype(int)\n",
    "    df['is_朝西'] = orientation_col.str.contains('西').astype(int)\n",
    "    df['is_朝北'] = orientation_col.str.contains('北').astype(int)\n",
    "\n",
    "    unknown_mask = orientation_col.str.contains('未知')\n",
    "    direction_cols = ['is_朝南', 'is_朝东', 'is_朝西', 'is_朝北']\n",
    "    df.loc[unknown_mask, direction_cols] = 0\n",
    "\n",
    "    print(\"  Created 'is_朝南/东/西/北' features.\")\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "def handle_rent_decoration(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '装修' 列 (精装修 / 非精装修).\n",
    "    \"\"\"\n",
    "    print(\"Processing [装修]...\")\n",
    "    col = '装修'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = '非精装修' \n",
    "        return df\n",
    "\n",
    "    deco_col = df[col].copy().fillna('非精装修')\n",
    "    deco_col = deco_col.astype(str).str.strip()\n",
    "    df[col] = deco_col.map(lambda x: '精装修' if x == '精装修' else '非精装修')\n",
    "\n",
    "    print(f\"  Processed '{col}' into '精装修'/'非精装修'. Unique values: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- 辅助函数：租赁特定属性 ---\n",
    "\n",
    "def handle_rent_elevator(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '电梯' 列 (有 / 无).\n",
    "    \"\"\"\n",
    "    print(\"Processing [电梯]...\")\n",
    "    col = '电梯'\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = '无' \n",
    "        return df\n",
    "\n",
    "    elevator_col = df[col].copy().fillna('无')\n",
    "    elevator_col = elevator_col.astype(str).str.strip()\n",
    "    df[col] = elevator_col.map(lambda x: '有' if x == '有' else '无')\n",
    "\n",
    "    print(f\"  Processed '{col}' into '有'/'无'. Unique values: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "def handle_property_type_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '物业类别' 列 (Multi-Hot).\n",
    "    \"\"\"\n",
    "    col = '物业类别'\n",
    "    print(f\"Processing [{col}] (Multi-Hot encoding)...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        # Create dummy columns\n",
    "        df['is_Property_Res'] = 0\n",
    "        df['is_Property_Com'] = 0\n",
    "        df['is_Property_Villa'] = 0\n",
    "        df['is_Property_Apt'] = 0\n",
    "        df['is_Property_Storage'] = 0\n",
    "        df['is_Property_Indust'] = 0\n",
    "        return df\n",
    "\n",
    "    prop_col = df[col].copy().fillna('未知')\n",
    "    prop_col = prop_col.replace(r'^\\s*\\(空白\\)\\s*$', '未知', regex=True)\n",
    "    prop_col = prop_col.replace(r'^\\s*$', '未知', regex=True)\n",
    "    prop_col = prop_col.replace('.', '未知')\n",
    "    prop_col = prop_col.astype(str).str.replace(' ', '')\n",
    "\n",
    "    res_terms = '普通住宅|住宅|平房|四合院|花园洋房|里弄|老公寓|商住两用'\n",
    "    com_terms = '商业|底商|写字楼|办公|商住两用'\n",
    "    villa_terms = '别墅'\n",
    "    apt_terms = '公寓'\n",
    "    storage_terms = '车库|车位|仓储|库房'\n",
    "    indust_terms = '工业厂房'\n",
    "\n",
    "    df['is_Property_Res'] = prop_col.str.contains(res_terms).astype(int)\n",
    "    df['is_Property_Com'] = prop_col.str.contains(com_terms).astype(int)\n",
    "    df['is_Property_Villa'] = prop_col.str.contains(villa_terms).astype(int)\n",
    "    df['is_Property_Apt'] = prop_col.str.contains(apt_terms).astype(int)\n",
    "    df['is_Property_Storage'] = prop_col.str.contains(storage_terms).astype(int)\n",
    "    df['is_Property_Indust'] = prop_col.str.contains(indust_terms).astype(int)\n",
    "\n",
    "    unknown_mask = (prop_col == '未知')\n",
    "    flag_cols = ['is_Property_Res', 'is_Property_Com', 'is_Property_Villa', 'is_Property_Apt', 'is_Property_Storage', 'is_Property_Indust']\n",
    "    df.loc[unknown_mask, flag_cols] = 0\n",
    "\n",
    "    print(f\"  Created {len(flag_cols)} Multi-Hot features.\")\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "def handle_transaction_ownership_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '产权描述' 列 (Grouping).\n",
    "    \"\"\"\n",
    "    col = '产权描述'\n",
    "    print(f\"Processing [{col}] (Grouping)...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = '未知' \n",
    "        return df\n",
    "\n",
    "    prop_col = df[col].copy().fillna('未知')\n",
    "    prop_col = prop_col.replace(r'^\\s*\\(空白\\)\\s*$', '未知', regex=True)\n",
    "    prop_col = prop_col.replace(r'^\\s*$', '未知', regex=True)\n",
    "    prop_col = prop_col.replace('.', '未知')\n",
    "    prop_col = prop_col.astype(str)\n",
    "\n",
    "    conditions = [\n",
    "        prop_col.str.contains('经济适用房'),\n",
    "        prop_col.str.contains('安置房|回迁房|还建房'),\n",
    "        prop_col.str.contains('公房|房改房|央产房|公租房'),\n",
    "        prop_col.str.contains('商品房'),\n",
    "        prop_col.str.contains('乡产|使用权|共有产权房|军产|宅基房|廉租房|校产|私产|集资房'),\n",
    "        prop_col == '未知'\n",
    "    ]\n",
    "\n",
    "    choices = [\n",
    "        '经济适用房',\n",
    "        '安置房',\n",
    "        '政策房',\n",
    "        '商品房',\n",
    "        '其他稀有产权',\n",
    "        '未知'\n",
    "    ]\n",
    "\n",
    "    df[col] = np.select(conditions, choices, default='其他稀有产权')\n",
    "\n",
    "    print(f\"  Processed categories into: {np.unique(df[col])}\")\n",
    "    return df\n",
    "\n",
    "def handle_payment_method_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '付款方式' 列\n",
    "    \"\"\"\n",
    "    col = '付款方式'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = '未知' \n",
    "        return df\n",
    "\n",
    "    payment_col = df[col].copy().fillna('未知')\n",
    "    payment_col = payment_col.astype(str).str.strip()\n",
    "\n",
    "    valid_categories = ['半年付价', '季付价', '年付价', '双月付价', '月付价']\n",
    "\n",
    "    df[col] = np.where(payment_col.isin(valid_categories), payment_col, '未知')\n",
    "\n",
    "    print(f\"  Processed '{col}'. Unique values: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "def handle_lease_type_r(df, n_train): \n",
    "    \"\"\"\n",
    "    处理 '租赁方式' 列\n",
    "    \"\"\"\n",
    "    col = '租赁方式'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = '未知' \n",
    "        return df\n",
    "\n",
    "    df[col] = df[col].fillna('未知')\n",
    "    print(f\"  Processed '{col}'. Unique values: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "def parse_term(term_str):\n",
    "    \"\"\"辅助函数：解析 '租期' 字符串为月数\"\"\"\n",
    "    term_str = str(term_str).replace(' ', '')\n",
    "    if term_str in ['(缺失值)', '未知', 'nan']:\n",
    "        return np.nan\n",
    "    try:\n",
    "        range_month = re.search(r'(\\d+)~(\\d+)个月', term_str)\n",
    "        if range_month: return (float(range_month.group(1)) + float(range_month.group(2))) / 2\n",
    "        range_year = re.search(r'(\\d+)~(\\d+)年', term_str)\n",
    "        if range_year: return ((float(range_year.group(1)) + float(range_year.group(2))) / 2) * 12\n",
    "        within_year = re.search(r'(\\d+)年以内', term_str)\n",
    "        if within_year: return float(within_year.group(1)) * 12 / 2\n",
    "        within_month = re.search(r'(\\d+)个月以内', term_str)\n",
    "        if within_month: return float(within_month.group(1)) / 2\n",
    "        above_year = re.search(r'(\\d+)年以上', term_str)\n",
    "        if above_year: return float(above_year.group(1)) * 12 + 6\n",
    "        above_month = re.search(r'(\\d+)个月以上', term_str)\n",
    "        if above_month: return float(above_month.group(1)) + 6\n",
    "        exact_year = re.search(r'^(\\d+)年$', term_str)\n",
    "        if exact_year: return float(exact_year.group(1)) * 12\n",
    "        exact_month = re.search(r'^(\\d+)个月$', term_str)\n",
    "        if exact_month: return float(exact_month.group(1))\n",
    "        return np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def handle_lease_term(df, n_train): \n",
    "    \"\"\"处理 '租期' 列 中位数填充\"\"\"\n",
    "    col = '租期'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['租期_月'] = 6.0 \n",
    "        return df\n",
    "\n",
    "    s = df[col].copy().astype(str)\n",
    "    df['租期_月'] = s.apply(parse_term)\n",
    "\n",
    "    # --- 仅使用训练集中位数进行插值 ---\n",
    "    median_term_train = df.iloc[:n_train]['租期_月'].median()\n",
    "    if pd.isna(median_term_train) or median_term_train == 0:\n",
    "        print(f\"  Warning: Could not calculate valid train median for '{col}'. Using fallback 6.0.\")\n",
    "        median_term_train = 6.0 # Fallback median\n",
    "\n",
    "    df['租期_月'].fillna(median_term_train, inplace=True)\n",
    "    print(f\"  Created '租期_月' (imputed with train median: {median_term_train:.1f}).\")\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "\n",
    "# --- 辅助函数：小区与费用 ---\n",
    "\n",
    "def handle_developer_r(df, n_train): \n",
    "    \"\"\"处理 '开发商' 列:\"\"\"\n",
    "    col = '开发商'\n",
    "    new_col = 'has_Developer'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 0\n",
    "        return df\n",
    "\n",
    "    no_developer_list = ['无', '无开发公司', '无开发商', '暂无信息', '暂无资料','暂无']\n",
    "    df[col] = df[col].fillna('无')\n",
    "    df[new_col] = (~df[col].isin(no_developer_list)).astype(int)\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    print(f\"  Created new feature '{new_col}'\")\n",
    "    return df\n",
    "\n",
    "def handle_property_management_r(df, n_train): \n",
    "    \"\"\"处理 '物业公司' 列:\"\"\"\n",
    "    col = '物业公司'\n",
    "    new_col = 'has_PropertyMgmt'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 0\n",
    "        return df\n",
    "\n",
    "    no_management_list = ['无', '无物业', '无物业管理', '无物业管理服务', '暂时无物业公司', '暂无信息']\n",
    "    df[col] = df[col].fillna('无物业')\n",
    "    df[new_col] = (~df[col].isin(no_management_list)).astype(int)\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    print(f\"  Created new feature '{new_col}'\")\n",
    "    return df\n",
    "\n",
    "def clean_numeric_str(s):\n",
    "    \"\"\"辅助函数：从 '1317户' 或 '19栋' 提取数字。\"\"\"\n",
    "    if pd.isna(s): return np.nan\n",
    "    match = re.search(r'(\\d+)', str(s))\n",
    "    return int(match.group(1)) if match else np.nan\n",
    "\n",
    "def handle_community_stats(df, n_train): \n",
    "    \"\"\"处理 '房屋总数' 和 '楼栋总数':\"\"\"\n",
    "    houses_col = '房屋总数'\n",
    "    buildings_col = '楼栋总数'\n",
    "    print(f\"Processing [{houses_col}] and [{buildings_col}]...\")\n",
    "\n",
    "    if houses_col not in df.columns or buildings_col not in df.columns:\n",
    "        print(f\"  Warning: Missing '{houses_col}' or '{buildings_col}'.\")\n",
    "        # Create dummy columns if missing\n",
    "        df[houses_col] = 1000.0\n",
    "        df[buildings_col] = 10.0\n",
    "        df['avg_units_per_building'] = 100.0\n",
    "        return df\n",
    "\n",
    "    df[houses_col] = df[houses_col].apply(clean_numeric_str)\n",
    "    df[buildings_col] = df[buildings_col].apply(clean_numeric_str)\n",
    "\n",
    "    temp_buildings = df[buildings_col].replace(0, np.nan)\n",
    "    df['avg_units_per_building'] = df[houses_col] / temp_buildings\n",
    "\n",
    "    # --- 仅使用训练集中位数进行插值 ---\n",
    "    train_part = df.iloc[:n_train]\n",
    "    for col in [houses_col, buildings_col, 'avg_units_per_building']:\n",
    "        median_val_train = train_part[col].median()\n",
    "        if pd.isna(median_val_train):\n",
    "          \n",
    "            if col == houses_col: median_val_train = 1000.0\n",
    "            elif col == buildings_col: median_val_train = 10.0\n",
    "            else: median_val_train = 100.0\n",
    "            print(f\"    Warning: Could not calculate train median for '{col}'. Using fallback {median_val_train}.\")\n",
    "        df[col].fillna(median_val_train, inplace=True)\n",
    "        print(f\"    Imputed '{col}' using train median ({median_val_train:.2f}).\")\n",
    "\n",
    "    print(f\"  Cleaned and imputed '{houses_col}', '{buildings_col}', 'avg_units_per_building'.\")\n",
    "    return df\n",
    "\n",
    "def parse_building_year(s):\n",
    "    \"\"\"辅助函数：从字符串中解析年份\"\"\"\n",
    "    if pd.isna(s): return np.nan\n",
    "    s_str = str(s)\n",
    "    nums = re.findall(r'(\\d{4})', s_str)\n",
    "    if not nums: return np.nan\n",
    "    return float(nums[0]) if len(nums) == 1 else (float(nums[0]) + float(nums[-1])) / 2 # Use first/last if range\n",
    "\n",
    "def handle_building_age(df, n_train, current_year=2025): \n",
    "    \"\"\"处理 '建筑年代' 列:\"\"\"\n",
    "    col = '建筑年代'\n",
    "    new_col = 'BuildingAge'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 20.0 \n",
    "        return df\n",
    "\n",
    "    df['Parsed_Year'] = df[col].apply(parse_building_year)\n",
    "    df[new_col] = current_year - df['Parsed_Year']\n",
    "\n",
    "    \n",
    "    median_age_train = df.iloc[:n_train][new_col].median()\n",
    "    if pd.isna(median_age_train) or median_age_train < 0:\n",
    "        print(f\"  Warning: 无法计算有效的训练样本中位数。使用备用值20.0。\")\n",
    "        median_age_train = 20.0 # Fallback median age\n",
    "\n",
    "    df[new_col].fillna(median_age_train, inplace=True)\n",
    "\n",
    "    df[new_col] = df[new_col].clip(lower=0)\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (中位数: {median_age_train:.2f}).\")\n",
    "    df = df.drop([col, 'Parsed_Year'], axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def _parse_fee(s):\n",
    "    \"\"\"辅助函数：解析费用\"\"\"\n",
    "    if pd.isna(s): return np.nan\n",
    "    s_str = str(s).replace(' ', '')\n",
    "    if '空白' in s_str or '暂无' in s_str: return np.nan\n",
    "    nums = re.findall(r'(\\d+\\.?\\d*)', s_str)\n",
    "    if not nums: return np.nan\n",
    "    return float(nums[0]) if len(nums) == 1 else (float(nums[0]) + float(nums[-1])) / 2\n",
    "\n",
    "def handle_greenery_rate(df, n_train): \n",
    "    \"\"\"处理 '绿化率' 列 (中位数填充):\"\"\"\n",
    "    col = '绿化率'\n",
    "    new_col = 'GreeneryRate'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 30.0 \n",
    "        return df\n",
    "\n",
    "    s = df[col].astype(str).str.replace(r'[\\s%]', '', regex=True)\n",
    "    s_numeric = pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "    # --- 仅使用训练集中位数进行插值 ---\n",
    "    valid_train_rates = s_numeric.iloc[:n_train][s_numeric.iloc[:n_train].between(0, 100, inclusive='both')]\n",
    "    median_val_train = valid_train_rates.median()\n",
    "    if pd.isna(median_val_train):\n",
    "        print(f\"  Warning: Could not calculate valid train median green rate. Using fallback 30.0.\")\n",
    "        median_val_train = 30.0 # Fallback\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val_train)\n",
    "    # Clip ALL values outside 0-100 range after imputation\n",
    "    df[new_col] = df[new_col].clip(lower=0, upper=100)\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (imputed with train median: {median_val_train:.2f}, clipped to 0-100).\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_plot_ratio(df, n_train): \n",
    "    \"\"\"处理 '容积率' 列 中位数填充\"\"\"\n",
    "    col = '容积率'\n",
    "    new_col = 'PlotRatio'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 2.0 # Default\n",
    "        return df\n",
    "\n",
    "    s_numeric = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    median_val_train = s_numeric.iloc[:n_train].median()\n",
    "    if pd.isna(median_val_train) or median_val_train <= 0:\n",
    "        print(f\"  Warning: Could not calculate valid train median plot ratio. Using fallback 2.0.\")\n",
    "        median_val_train = 2.0 # Fallback\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val_train)\n",
    "    # Clip potentially negative values after imputation\n",
    "    df[new_col] = df[new_col].clip(lower=0.01) # Ensure positive\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (imputed with train median: {median_val_train:.2f}).\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_property_fee(df, n_train):\n",
    "    \"\"\"处理 '物业费' 列 中位数填充\"\"\"\n",
    "    col = '物业费'\n",
    "    new_col = 'PropertyFee'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 0.0 # Default\n",
    "        return df\n",
    "\n",
    "    s_numeric = df[col].apply(_parse_fee)\n",
    "\n",
    "    median_val_train = s_numeric.iloc[:n_train].median()\n",
    "    if pd.isna(median_val_train) or median_val_train < 0:\n",
    "         print(f\"  Warning: Could not calculate valid train median property fee. Using fallback 0.0.\")\n",
    "         median_val_train = 0.0 # Fallback\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val_train)\n",
    "    # Clip potentially negative values\n",
    "    df[new_col] = df[new_col].clip(lower=0)\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (imputed with train median: {median_val_train:.2f}).\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- 辅助函数：能源与设施 ---\n",
    "\n",
    "def handle_rent_water_supply(df, n_train): \n",
    "    \"\"\"处理 '用水' 列\"\"\"\n",
    "    col = '用水'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['is_Water_Civil'] = 0\n",
    "        df['is_Water_Commercial'] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('')\n",
    "    df['is_Water_Civil'] = s.str.contains('民水').astype(int)\n",
    "    df['is_Water_Commercial'] = s.str.contains('商水').astype(int)\n",
    "    print(\"  Created 'is_Water_Civil' and 'is_Water_Commercial' features.\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_rent_heating(df, n_train): \n",
    "    \"\"\"处理 '采暖' 列\"\"\"\n",
    "    col = '采暖'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['is_Heating_Central'] = 0\n",
    "        df['is_Heating_Self'] = 0\n",
    "        df['is_Heating_None'] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('')\n",
    "    df['is_Heating_Central'] = s.str.contains('集中供暖').astype(int)\n",
    "    df['is_Heating_Self'] = s.str.contains('自采暖').astype(int)\n",
    "    df['is_Heating_None'] = s.str.contains('无').astype(int) # Check for '无' instead of '无供暖' for broader match\n",
    "    print(\"  Created 'is_Heating_Central', 'is_Heating_Self', 'is_Heating_None' features.\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_rent_electricity(df, n_train): \n",
    "    \"\"\"处理 '用电' 列\"\"\"\n",
    "    col = '用电'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['is_Electricity_Civil'] = 0\n",
    "        df['is_Electricity_Commercial'] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('')\n",
    "    df['is_Electricity_Civil'] = s.str.contains('民电').astype(int)\n",
    "    df['is_Electricity_Commercial'] = s.str.contains('商电').astype(int)\n",
    "    print(\"  Created 'is_Electricity_Civil' and 'is_Electricity_Commercial' features.\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_gas_supply(df, n_train): \n",
    "    \"\"\"处理 '燃气' 列 (有/无)\"\"\"\n",
    "    col = '燃气'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df['is_Gas_Available'] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('未知')\n",
    "    df['is_Gas_Available'] = (s == '有').astype(int)\n",
    "    print(\"  Created 'is_Gas_Available' feature.\")\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n",
    "def _parse_fee2(s): \n",
    "    \"\"\"辅助函数：解析费用\"\"\"\n",
    "    if pd.isna(s): return np.nan\n",
    "    s_str = str(s).replace(' ', '')\n",
    "    if '空白' in s_str: return np.nan\n",
    "    nums = re.findall(r'(\\d+\\.?\\d*)', s_str)\n",
    "    if not nums: return np.nan\n",
    "    return float(nums[0]) if len(nums) == 1 else (float(nums[0]) + float(nums[-1])) / 2\n",
    "\n",
    "def handle_gas_fee(df, n_train): \n",
    "    \"\"\"处理 '燃气费' 列 中位数填充\"\"\"\n",
    "    col = '燃气费'\n",
    "    new_col = 'GasFee'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 0.0 # Default\n",
    "        return df\n",
    "\n",
    "    s_numeric = df[col].apply(_parse_fee2) \n",
    "\n",
    "    \n",
    "    median_val_train = s_numeric.iloc[:n_train].median()\n",
    "    if pd.isna(median_val_train) or median_val_train < 0:\n",
    "        print(f\"  Warning: Could not calculate valid train median gas fee. Using fallback 0.0.\")\n",
    "        median_val_train = 0.0 # Fallback\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val_train)\n",
    "    df[new_col] = df[new_col].clip(lower=0)\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (imputed with train median: {median_val_train:.2f}).\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_heating_fee(df, n_train): \n",
    "    \"\"\"处理 '供热费' 列 中位数填充\"\"\"\n",
    "    col = '供热费'\n",
    "    new_col = 'HeatingFee'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 0.0 \n",
    "        return df\n",
    "\n",
    "    s_numeric = df[col].apply(_parse_fee2) \n",
    "\n",
    "   \n",
    "    median_val_train = s_numeric.iloc[:n_train].median()\n",
    "    if pd.isna(median_val_train) or median_val_train < 0:\n",
    "        print(f\"  Warning: Could not calculate valid train median heating fee. Using fallback 0.0.\")\n",
    "        median_val_train = 0.0 # Fallback\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val_train)\n",
    "    df[new_col] = df[new_col].clip(lower=0)\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (imputed with train median: {median_val_train:.2f}).\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- 辅助函数：其他设施 ---\n",
    "\n",
    "def handle_parking_spots(df, n_train): \n",
    "    \"\"\"处理 '停车位' 列 中位数填充\"\"\"\n",
    "    col = '停车位'\n",
    "    new_col = 'ParkingSpots'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col] = 0.0\n",
    "        return df\n",
    "\n",
    "    s_numeric = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # --- 仅使用训练集中位数进行插值 ---\n",
    "    median_val_train = s_numeric.iloc[:n_train].median()\n",
    "    if pd.isna(median_val_train) or median_val_train < 0:\n",
    "        print(f\"  Warning: Could not calculate valid train median parking spots. Using fallback 0.0.\")\n",
    "        median_val_train = 0.0 # Fallback\n",
    "\n",
    "    df[new_col] = s_numeric.fillna(median_val_train)\n",
    "    df[new_col] = df[new_col].clip(lower=0)\n",
    "\n",
    "    print(f\"  Created new feature '{new_col}' (imputed with train median: {median_val_train:.2f}).\")\n",
    "    df = df.drop(col, axis=1, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def handle_parking_type(df, n_train): \n",
    "    \"\"\"处理 '车位' 列\"\"\"\n",
    "    col = '车位'\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[col] = '未知' \n",
    "        return df\n",
    "\n",
    "    parking_col = df[col].copy().fillna('未知')\n",
    "    valid_categories = ['免费使用', '租用车位']\n",
    "    df[col] = np.where(parking_col.isin(valid_categories), parking_col, '未知')\n",
    "\n",
    "    print(f\"  Processed '{col}'. Unique values: {df[col].unique()}\")\n",
    "    return df\n",
    "\n",
    "def handle_amenities(df, n_train): \n",
    "    \"\"\"处理 '配套设施' 列:\"\"\"\n",
    "    col = '配套设施'\n",
    "    new_col_count = 'amenity_count'\n",
    "    new_col_metro = 'is_Amenity_Metro'\n",
    "\n",
    "    print(f\"Processing [{col}]...\")\n",
    "    if col not in df.columns:\n",
    "        print(f\"  Warning: Column '{col}' not found.\")\n",
    "        df[new_col_count] = 0\n",
    "        df[new_col_metro] = 0\n",
    "        return df\n",
    "\n",
    "    s = df[col].fillna('')\n",
    "    df[new_col_count] = s.apply(lambda x: len(x.split('、')) if x else 0)\n",
    "    df[new_col_metro] = s.str.contains('地铁').astype(int)\n",
    "\n",
    "    print(f\"  Created '{new_col_count}' and '{new_col_metro}' features.\")\n",
    "    return df.drop(col, axis=1, errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 2: 数据加载、合并、初步清理 ---\n",
      "训练数据加载成功: (98899, 46)\n",
      "Kaggle 测试数据加载成功: (9773, 46)\n",
      "原始训练集行数 (n_train): 98899\n",
      "合并训练集和测试集...\n",
      "合并后 df_full 形状: (108672, 46)\n",
      "初步删除 10 列: ['年份', '物业办公电话', '供水', '供暖', '供电', '停车费用', 'coord_x', 'coord_y', '客户反馈', 'source']\n",
      "初步列删除后 df_full 形状: (108672, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 2: 数据加载、合并、初步清理 ---\")\n",
    "\n",
    "# --- 2a: 加载 ---\n",
    "try:\n",
    "    df_train_raw = pd.read_csv('./data/ruc_Class25Q2_train_rent.csv')\n",
    "    df_test_raw = pd.read_csv('./data/ruc_Class25Q2_test_rent.csv')\n",
    "    print(f\"训练数据加载成功: {df_train_raw.shape}\")\n",
    "    print(f\"Kaggle 测试数据加载成功: {df_test_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"错误：未找到数据文件。请检查 './data/' 目录下的 CSV 文件名。\")\n",
    "    raise\n",
    "\n",
    "# --- 2b: 存储原始信息 ---\n",
    "n_train = df_train_raw.shape[0]\n",
    "print(f\"原始训练集行数 (n_train): {n_train}\")\n",
    "\n",
    "y_train_full = df_train_raw['Price'].copy() # Store original target\n",
    "y_train_ln_full = np.log1p(y_train_full) # Store log target\n",
    "\n",
    "test_ids = df_test_raw['ID'].copy() # Store test IDs\n",
    "\n",
    "# --- 2c: 合并 ---\n",
    "print(\"合并训练集和测试集...\")\n",
    "df_train_to_concat = df_train_raw.drop(columns=['Price'], errors='ignore')\n",
    "df_test_to_concat = df_test_raw.drop(columns=['ID', 'Price'], errors='ignore') \n",
    "\n",
    "df_train_to_concat['source'] = 'train'\n",
    "df_test_to_concat['source'] = 'test'\n",
    "\n",
    "df_full = pd.concat([df_train_to_concat, df_test_to_concat], ignore_index=True)\n",
    "print(f\"合并后 df_full 形状: {df_full.shape}\")\n",
    "\n",
    "# --- 2d: 初步列删除 ---\n",
    "columns_to_drop_initial = [\n",
    "    '年份', '物业办公电话', '供水', '供暖', '供电', '停车费用',\n",
    "    'coord_x', 'coord_y', '客户反馈', 'source' \n",
    "]\n",
    "existing_cols_to_drop = [col for col in columns_to_drop_initial if col in df_full.columns]\n",
    "print(f\"初步删除 {len(existing_cols_to_drop)} 列: {existing_cols_to_drop}\")\n",
    "df_full = df_full.drop(columns=existing_cols_to_drop, errors='ignore')\n",
    "print(f\"初步列删除后 df_full 形状: {df_full.shape}\")\n",
    "\n",
    "del df_train_raw, df_test_raw, df_train_to_concat, df_test_to_concat # Clean up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 3: 定义并应用通用特征工程管道 ---\n",
      "\n",
      ">>> Starting base feature engineering (Shape: (108672, 36))\n",
      "Processing [区县]...\n",
      "Processing [板块]...\n",
      "Processing [环线位置]...\n",
      "Processing [户型]...\n",
      "  Calculating medians based on first 98899 rows...\n",
      "    Median for '卧室数' (excluding 0): 2.0\n",
      "    Median for '客厅数' (excluding 0): 1.0\n",
      "    Median for '卫生间数' (excluding 0): 1.0\n",
      "Imputation complete.\n",
      "Processing [交易时间] (Extracting Year and Month categories)...\n",
      "  Created 'Listing_Year' (imputed with train mode: 2024)\n",
      "  Created 'Listing_Month' (imputed with train mode: 5)\n",
      "Processing [楼层]...\n",
      "  Median '总楼层数' (train): 18.0\n",
      "  Floor position distribution (modes): ['中楼层']\n",
      "Processing [建筑结构] (Rent version - Multi-Hot encoding)...\n",
      "  Created 'is_BType_Tower', 'is_BType_Slab', 'is_BType_Combo', 'is_BType_Bungalow'.\n",
      "Processing [面积]...\n",
      "  Cleaned and imputed '面积' using train median (79.00).\n",
      "Processing [朝向]...\n",
      "  Created 'is_朝南/东/西/北' features.\n",
      "Processing [装修]...\n",
      "  Processed '装修' into '精装修'/'非精装修'. Unique values: ['精装修' '非精装修']\n",
      "Processing [电梯]...\n",
      "  Processed '电梯' into '有'/'无'. Unique values: ['无' '有']\n",
      "Processing [物业类别] (Multi-Hot encoding)...\n",
      "  Created 6 Multi-Hot features.\n",
      "Processing [产权描述] (Grouping)...\n",
      "  Processed categories into: ['其他稀有产权' '商品房' '安置房' '政策房' '未知' '经济适用房']\n",
      "Processing [付款方式]...\n",
      "  Processed '付款方式'. Unique values: ['季付价' '未知' '年付价' '半年付价' '月付价' '双月付价']\n",
      "Processing [租赁方式]...\n",
      "  Processed '租赁方式'. Unique values: ['整租' '合租']\n",
      "Processing [租期]...\n",
      "  Created '租期_月' (imputed with train median: 6.0).\n",
      "Processing [开发商]...\n",
      "  Created new feature 'has_Developer'\n",
      "Processing [物业公司]...\n",
      "  Created new feature 'has_PropertyMgmt'\n",
      "Processing [房屋总数] and [楼栋总数]...\n",
      "    Imputed '房屋总数' using train median (1445.00).\n",
      "    Imputed '楼栋总数' using train median (13.00).\n",
      "    Imputed 'avg_units_per_building' using train median (115.92).\n",
      "  Cleaned and imputed '房屋总数', '楼栋总数', 'avg_units_per_building'.\n",
      "Processing [建筑年代]...\n",
      "  Created new feature 'BuildingAge' (中位数: 17.00).\n",
      "Processing [绿化率]...\n",
      "  Created new feature 'GreeneryRate' (imputed with train median: 35.00, clipped to 0-100).\n",
      "Processing [容积率]...\n",
      "  Created new feature 'PlotRatio' (imputed with train median: 2.67).\n",
      "Processing [物业费]...\n",
      "  Created new feature 'PropertyFee' (imputed with train median: 2.20).\n",
      "Processing [用水]...\n",
      "  Created 'is_Water_Civil' and 'is_Water_Commercial' features.\n",
      "Processing [采暖]...\n",
      "  Created 'is_Heating_Central', 'is_Heating_Self', 'is_Heating_None' features.\n",
      "Processing [用电]...\n",
      "  Created 'is_Electricity_Civil' and 'is_Electricity_Commercial' features.\n",
      "Processing [燃气]...\n",
      "  Created 'is_Gas_Available' feature.\n",
      "Processing [燃气费]...\n",
      "  Created new feature 'GasFee' (imputed with train median: 2.95).\n",
      "Processing [供热费]...\n",
      "  Created new feature 'HeatingFee' (imputed with train median: 25.00).\n",
      "Processing [停车位]...\n",
      "  Created new feature 'ParkingSpots' (imputed with train median: 779.00).\n",
      "Processing [车位]...\n",
      "  Processed '车位'. Unique values: ['未知' '租用车位' '免费使用']\n",
      "Processing [配套设施]...\n",
      "  Created 'amenity_count' and 'is_Amenity_Metro' features.\n",
      "<<< Base feature engineering complete (Shape: (108672, 68))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 步骤 3: 定义并应用通用特征工程管道 ---\")\n",
    "\n",
    "def apply_base_feature_engineering(df, n_train):\n",
    "    \"\"\"\n",
    "    Apply all basic handle_... functions in sequence.\n",
    "    Uses n_train for leakage prevention.\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    print(f\"\\n>>> Starting base feature engineering (Shape: {df_processed.shape})\")\n",
    "\n",
    "    # Geography related\n",
    "    df_processed = handle_district_r(df_processed, n_train)\n",
    "    df_processed = handle_board_r(df_processed, n_train)\n",
    "    df_processed = handle_ring_road_r(df_processed, n_train)\n",
    "\n",
    "    # Basic property attributes\n",
    "    df_processed = handle_rent_house_type_r(df_processed, n_train)\n",
    "    df_processed = handle_listing_date_r(df_processed, n_train)\n",
    "    df_processed = handle_rent_floor(df_processed, n_train)\n",
    "    df_processed = handle_building_structure_r(df_processed, n_train)\n",
    "    df_processed = handle_rent_area(df_processed, n_train)\n",
    "    df_processed = handle_rent_orientation(df_processed, n_train)\n",
    "    df_processed = handle_rent_decoration(df_processed, n_train)\n",
    "\n",
    "    # Rent-specific attributes\n",
    "    df_processed = handle_rent_elevator(df_processed, n_train)\n",
    "    df_processed = handle_property_type_r(df_processed, n_train)\n",
    "    df_processed = handle_transaction_ownership_r(df_processed, n_train)\n",
    "    df_processed = handle_payment_method_r(df_processed, n_train)\n",
    "    df_processed = handle_lease_type_r(df_processed, n_train)\n",
    "    df_processed = handle_lease_term(df_processed, n_train)\n",
    "\n",
    "    # Community and fees\n",
    "    df_processed = handle_developer_r(df_processed, n_train)\n",
    "    df_processed = handle_property_management_r(df_processed, n_train)\n",
    "    df_processed = handle_community_stats(df_processed, n_train)\n",
    "    df_processed = handle_building_age(df_processed, n_train) # Uses n_train implicit median calc\n",
    "    df_processed = handle_greenery_rate(df_processed, n_train)\n",
    "    df_processed = handle_plot_ratio(df_processed, n_train)\n",
    "    df_processed = handle_property_fee(df_processed, n_train)\n",
    "\n",
    "    # Utilities\n",
    "    df_processed = handle_rent_water_supply(df_processed, n_train)\n",
    "    df_processed = handle_rent_heating(df_processed, n_train)\n",
    "    df_processed = handle_rent_electricity(df_processed, n_train)\n",
    "    df_processed = handle_gas_supply(df_processed, n_train)\n",
    "    df_processed = handle_gas_fee(df_processed, n_train)\n",
    "    df_processed = handle_heating_fee(df_processed, n_train)\n",
    "\n",
    "    # Parking and amenities\n",
    "    df_processed = handle_parking_spots(df_processed, n_train)\n",
    "    df_processed = handle_parking_type(df_processed, n_train)\n",
    "    df_processed = handle_amenities(df_processed, n_train)\n",
    "\n",
    "    print(f\"<<< Base feature engineering complete (Shape: {df_processed.shape})\")\n",
    "    return df_processed\n",
    "\n",
    "# Apply base engineering\n",
    "df_processed = apply_base_feature_engineering(df_full, n_train)\n",
    "\n",
    "del df_full # Clean up memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 4: 应用高级特征工程 ---\n",
      "\n",
      "--- 4a: 创建地理空间特征 ---\n",
      "Calculating geospatial features (distance to center)...\n",
      "  Calculated centers for 12 cities (based on training set).\n",
      "  Calculated '距离中心_公里', imputed NaNs using train median (13.38 km).\n",
      "Distance features created.\n",
      "Creating geo clusters (20 per city)...\n",
      "Geo cluster features created.\n",
      "\n",
      "--- 4b: 执行目标编码 ---\n",
      "Processing ['城市', '区县', '板块'] (K-Fold Target Encoding, n_splits=6)...\n",
      "  Encoding existing columns: ['城市', '区县', '板块']\n",
      "  Calculated global mean from original train target: 582908.98\n",
      "  Calculated full mean map based on 98899 training samples.\n",
      "  Applied full map to 9773 test samples.\n",
      "    Filling 53 remaining NaNs in train set using full map...\n",
      "  K-Fold Target Encoding complete. New feature: 'Location_Target_Encoded'.\n",
      "\n",
      "--- 4c: 创建比率特征 ---\n",
      "Creating ratio features...\n",
      "  Creating ratio [室厅比]...\n",
      "  Creating ratio [室卫比]...\n",
      "  Creating ratio [平均每卧室面积]...\n",
      "  Creating ratio [绿化容积比]...\n",
      "  Creating ratio [车位房屋比]...\n",
      "  Creating ratio [设施密度]...\n",
      "Ratio features created.\n",
      "\n",
      "--- 4d: 执行对数变换 ---\n",
      "Applying log1p transform...\n",
      "  Transformed '面积' -> 'log_面积'\n",
      "  Transformed '总楼层数' -> 'log_总楼层数'\n",
      "  Warning: '绝对楼层' has negative values (min: -8.0), skipping log1p.\n",
      "  Warning: '相对楼层_总楼层数' has negative values (min: -1.8), skipping log1p.\n",
      "  Transformed '总楼层数_平方' -> 'log_总楼层数_平方'\n",
      "  Transformed '房屋总数' -> 'log_房屋总数'\n",
      "  Transformed '楼栋总数' -> 'log_楼栋总数'\n",
      "  Transformed 'avg_units_per_building' -> 'log_avg_units_per_building'\n",
      "  Transformed 'BuildingAge' -> 'log_BuildingAge'\n",
      "  Transformed 'GreeneryRate' -> 'log_GreeneryRate'\n",
      "  Transformed 'PlotRatio' -> 'log_PlotRatio'\n",
      "  Transformed 'PropertyFee' -> 'log_PropertyFee'\n",
      "  Transformed 'GasFee' -> 'log_GasFee'\n",
      "  Transformed 'HeatingFee' -> 'log_HeatingFee'\n",
      "  Transformed 'ParkingSpots' -> 'log_ParkingSpots'\n",
      "  Transformed 'amenity_count' -> 'log_amenity_count'\n",
      "  Transformed '距离中心_公里' -> 'log_距离中心_公里'\n",
      "  Transformed '距离中心_公里_平方' -> 'log_距离中心_公里_平方'\n",
      "  Transformed 'Location_Target_Encoded' -> 'log_Location_Target_Encoded'\n",
      "  Transformed '室厅比' -> 'log_室厅比'\n",
      "  Transformed '室卫比' -> 'log_室卫比'\n",
      "  Transformed '平均每卧室面积' -> 'log_平均每卧室面积'\n",
      "  Transformed '绿化容积比' -> 'log_绿化容积比'\n",
      "  Transformed '车位房屋比' -> 'log_车位房屋比'\n",
      "  Transformed '设施密度' -> 'log_设施密度'\n",
      "  Transformed '租期_月' -> 'log_租期_月'\n",
      "Log transform complete. Processed 24 columns.\n",
      "\n",
      "--- 4e: 执行特征分箱 ---\n",
      "Binning feature [log_BuildingAge] (n_bins=5, strategy='kmeans')...\n",
      "  Created binned feature 'log_BuildingAge_分箱'.\n",
      "  One-hot encoded bins and removed original 'log_BuildingAge'.\n",
      "Binning feature [log_总楼层数] (n_bins=5, strategy='kmeans')...\n",
      "  Created binned feature 'log_总楼层数_分箱'.\n",
      "  One-hot encoded bins and removed original 'log_总楼层数'.\n",
      "Binning feature [log_距离中心_公里] (n_bins=5, strategy='kmeans')...\n",
      "  Created binned feature 'log_距离中心_公里_分箱'.\n",
      "  One-hot encoded bins and removed original 'log_距离中心_公里'.\n",
      "Binning feature [log_面积] (n_bins=5, strategy='kmeans')...\n",
      "  Created binned feature 'log_面积_分箱'.\n",
      "  One-hot encoded bins and removed original 'log_面积'.\n",
      "Binning feature [log_Location_Target_Encoded] (n_bins=5, strategy='kmeans')...\n",
      "  Created binned feature 'log_Location_Target_Encoded_分箱'.\n",
      "  One-hot encoded bins and removed original 'log_Location_Target_Encoded'.\n",
      "Binning feature [log_租期_月] (n_bins=5, strategy='kmeans')...\n",
      "  Created binned feature 'log_租期_月_分箱'.\n",
      "  One-hot encoded bins and removed original 'log_租期_月'.\n",
      "Binning complete.\n",
      "\n",
      "--- 4f: 创建交互项 ---\n",
      "Creating interaction terms (degree=2, interaction_only=True)...\n",
      "  Creating interactions for 5 features: ['卧室数', '客厅数', '卫生间数', '相对楼层', 'log_平均每卧室面积']\n",
      "  Generated 15 polynomial/interaction features.\n",
      "Interaction terms created. New shape: (108672, 348)\n",
      "\n",
      "--- Advanced Feature Engineering Complete ---\n",
      "Final engineered shape: (108672, 348)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 4: 应用高级特征工程 ---\")\n",
    "\n",
    "# --- 4a: Geospatial features ---\n",
    "print(\"\\n--- 4a: 创建地理空间特征 ---\")\n",
    "df_processed = compute_city_center_and_distances_r(df_processed, n_train)\n",
    "df_processed = create_geo_clusters_r(df_processed, n_train, n_clusters=20)\n",
    "\n",
    "print(\"\\n--- 4b: 执行目标编码 ---\")\n",
    "def apply_target_encoding_combined(df, y_target, n_train, loc_cols, n_splits=6, random_state=111):\n",
    "    \"\"\"在合并的数据框上对 loc_cols 进行 K-Fold 目标编码。(Fit ONLY on n_train)\"\"\"\n",
    "    df_te = df.copy()\n",
    "    print(f\"Processing {loc_cols} (K-Fold Target Encoding, n_splits={n_splits})...\")\n",
    "    if not loc_cols: return df_te\n",
    "\n",
    "    existing_loc_cols = [col for col in loc_cols if col in df_te.columns]\n",
    "    if not existing_loc_cols:\n",
    "        print(\"  Warning: No location columns found for target encoding.\")\n",
    "        return df_te\n",
    "    print(f\"  Encoding existing columns: {existing_loc_cols}\")\n",
    "\n",
    "    new_col = 'Location_Target_Encoded'\n",
    "    \n",
    "    global_mean = y_target.iloc[:n_train].mean() # Use y_train_full (original price)\n",
    "    print(f\"  Calculated global mean from original train target: {global_mean:.2f}\")\n",
    "\n",
    "    def create_key(df_slice):\n",
    "        return df_slice[existing_loc_cols].astype(str).agg('_'.join, axis=1)\n",
    "    df_te['key'] = create_key(df_te)\n",
    "\n",
    "    \n",
    "    X_train_part = df_te.iloc[:n_train].copy()\n",
    "    X_train_part['target'] = y_target.iloc[:n_train] # Use original train target\n",
    "    full_train_map = X_train_part.groupby('key')['target'].mean()\n",
    "    print(f\"  Calculated full mean map based on {n_train} training samples.\")\n",
    "\n",
    "    \n",
    "    X_test_part = df_te.iloc[n_train:].copy()\n",
    "    df_te.loc[X_test_part.index, new_col] = X_test_part['key'].map(full_train_map).fillna(global_mean)\n",
    "    print(f\"  Applied full map to {len(X_test_part)} test samples.\")\n",
    "\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    df_te.loc[X_train_part.index, new_col] = np.nan\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_part)):\n",
    "        X_train_fold = X_train_part.iloc[train_idx]\n",
    "        X_val_fold = X_train_part.iloc[val_idx]\n",
    "        fold_map = X_train_fold.groupby('key')['target'].mean()\n",
    "        df_te.loc[X_val_fold.index, new_col] = X_val_fold['key'].map(fold_map)\n",
    "\n",
    "    \n",
    "    is_train_mask = df_te.index < n_train\n",
    "    is_nan_mask = df_te[new_col].isnull()\n",
    "    fill_mask = is_train_mask & is_nan_mask\n",
    "    fill_count = fill_mask.sum()\n",
    "    if fill_count > 0:\n",
    "        print(f\"    Filling {fill_count} remaining NaNs in train set using full map...\")\n",
    "        values_to_fill = df_te.loc[fill_mask, 'key'].map(full_train_map).values\n",
    "        df_te.loc[fill_mask, new_col] = values_to_fill\n",
    "\n",
    "    \n",
    "    df_te[new_col].fillna(global_mean, inplace=True)\n",
    "\n",
    "    # --- Clean up ---\n",
    "    cols_to_drop = existing_loc_cols + ['key', 'target'] # Remove target if it exists\n",
    "    df_te = df_te.drop(columns=cols_to_drop, errors='ignore')\n",
    "    print(f\"  K-Fold Target Encoding complete. New feature: '{new_col}'.\")\n",
    "    return df_te\n",
    "\n",
    "location_columns_to_encode = ['城市', '区县', '板块'] # Rent uses these\n",
    "df_processed = apply_target_encoding_combined(df_processed, y_train_full, n_train, location_columns_to_encode)\n",
    "\n",
    "# --- 4c: Ratio features ---\n",
    "print(\"\\n--- 4c: 创建比率特征 ---\")\n",
    "def create_ratio_features(df):\n",
    "    df_out = df.copy()\n",
    "    print(\"Creating ratio features...\")\n",
    "    def cal_ratio(df_r, num, den, new_name):\n",
    "        if num in df_r.columns and den in df_r.columns:\n",
    "            print(f\"  Creating ratio [{new_name}]...\")\n",
    "            df_r[new_name] = np.where(df_r[den] > 0, df_r[num] / df_r[den], 0)\n",
    "        else: print(f\"  Warning: Cannot create '{new_name}', missing columns.\")\n",
    "\n",
    "    cal_ratio(df_out, '卧室数', '客厅数', '室厅比')\n",
    "    cal_ratio(df_out, '卧室数', '卫生间数', '室卫比')\n",
    "    cal_ratio(df_out, '面积', '卧室数', '平均每卧室面积')\n",
    "    cal_ratio(df_out, 'GreeneryRate', 'PlotRatio', '绿化容积比')\n",
    "    cal_ratio(df_out, 'ParkingSpots', '房屋总数', '车位房屋比')\n",
    "    cal_ratio(df_out, 'amenity_count', '面积', '设施密度')\n",
    "    print(\"Ratio features created.\")\n",
    "    return df_out\n",
    "\n",
    "df_processed = create_ratio_features(df_processed)\n",
    "\n",
    "# --- 4d: Log Transform ---\n",
    "print(\"\\n--- 4d: 执行对数变换 ---\")\n",
    "def log_transform(df, skewed_cols):\n",
    "    df_transformed = df.copy()\n",
    "    print(\"Applying log1p transform...\")\n",
    "    transformed_count = 0\n",
    "    for col in skewed_cols:\n",
    "        if col in df_transformed.columns and pd.api.types.is_numeric_dtype(df_transformed[col]):\n",
    "            min_val = df_transformed[col].min()\n",
    "            if min_val >= 0:\n",
    "                col_log = f'log_{col}'\n",
    "                df_transformed[col_log] = np.log1p(df_transformed[col])\n",
    "                df_transformed = df_transformed.drop(columns=[col], errors='ignore')\n",
    "                transformed_count += 1\n",
    "                print(f\"  Transformed '{col}' -> '{col_log}'\")\n",
    "            else:\n",
    "                print(f\"  Warning: '{col}' has negative values (min: {min_val}), skipping log1p.\")\n",
    "        else:\n",
    "             if col in df_transformed.columns: print(f\"  Warning: '{col}' is not numeric, skipping log1p.\")\n",
    "    print(f\"Log transform complete. Processed {transformed_count} columns.\")\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "skewed_cols = [\n",
    "    '面积', '总楼层数', '绝对楼层', '相对楼层_总楼层数', '总楼层数_平方',\n",
    "    '房屋总数', '楼栋总数', 'avg_units_per_building', 'BuildingAge',\n",
    "    'GreeneryRate', 'PlotRatio', 'PropertyFee', 'GasFee', 'HeatingFee',\n",
    "    'ParkingSpots', 'amenity_count', '距离中心_公里', '距离中心_公里_平方',\n",
    "    'Location_Target_Encoded', '室厅比', '室卫比', '平均每卧室面积',\n",
    "    '绿化容积比', '车位房屋比', '设施密度', '租期_月'\n",
    "]\n",
    "skewed_cols_existing = [col for col in skewed_cols if col in df_processed.columns]\n",
    "df_processed = log_transform(df_processed, skewed_cols_existing)\n",
    "\n",
    "# --- 4e: Binning ---\n",
    "print(\"\\n--- 4e: 执行特征分箱 ---\")\n",
    "def bin_and_encode(df, n_train, feature, n_bins=5, strategy='kmeans'):\n",
    "    df_binned = df.copy()\n",
    "    print(f\"Binning feature [{feature}] (n_bins={n_bins}, strategy='{strategy}')...\")\n",
    "    if feature not in df_binned.columns or not pd.api.types.is_numeric_dtype(df_binned[feature]):\n",
    "        print(f\"  Warning: Feature '{feature}' not found or not numeric. Skipping.\")\n",
    "        return df_binned\n",
    "\n",
    "    \n",
    "    median_val_train = df_binned.iloc[:n_train][feature].median()\n",
    "    if pd.isna(median_val_train):\n",
    "        print(f\"  Warning: Could not calculate train median for '{feature}'. Using 0 for imputation.\")\n",
    "        median_val_train = 0.0 # Fallback\n",
    "    df_binned[feature].fillna(median_val_train, inplace=True)\n",
    "    \n",
    "\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=None)\n",
    "    feature_binned_col = f'{feature}_分箱'\n",
    "\n",
    "    try:\n",
    "        train_data_for_fit = df_binned.iloc[:n_train][[feature]]\n",
    "        \n",
    "        if train_data_for_fit[feature].nunique() < n_bins:\n",
    "             actual_bins = train_data_for_fit[feature].nunique()\n",
    "             print(f\"  Warning: Unique values ({actual_bins}) < n_bins ({n_bins}). Using uniform strategy with {actual_bins} bins.\")\n",
    "             if actual_bins < 2:\n",
    "                 print(f\"  Skipping binning for '{feature}' due to < 2 unique train values.\")\n",
    "                 return df_binned\n",
    "             binner = KBinsDiscretizer(n_bins=actual_bins, encode='ordinal', strategy='uniform', subsample=None)\n",
    "\n",
    "        binner.fit(train_data_for_fit)\n",
    "        df_binned[feature_binned_col] = binner.transform(df_binned[[feature]])\n",
    "        print(f\"  Created binned feature '{feature_binned_col}'.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  Error binning '{feature}': {e}. Skipping...\")\n",
    "        return df_binned # Return original df if binning fails\n",
    "\n",
    "    # One-hot encode the binned feature and remove original\n",
    "    df_binned = pd.get_dummies(df_binned, columns=[feature_binned_col], prefix=f'{feature}段', drop_first=False)\n",
    "    df_binned = df_binned.drop(columns=[feature], errors='ignore')\n",
    "    print(f\"  One-hot encoded bins and removed original '{feature}'.\")\n",
    "    return df_binned\n",
    "\n",
    "features_to_bin = [\n",
    "    'log_BuildingAge', 'log_总楼层数', 'log_距离中心_公里', 'log_面积',\n",
    "    'log_Location_Target_Encoded', 'log_租期_月'\n",
    "]\n",
    "features_to_bin_existing = [col for col in features_to_bin if col in df_processed.columns]\n",
    "for feature in features_to_bin_existing:\n",
    "    df_processed = bin_and_encode(df_processed, n_train, feature=feature, n_bins=5, strategy='kmeans')\n",
    "\n",
    "print(\"Binning complete.\")\n",
    "\n",
    "# --- 4f: Interaction Terms ---\n",
    "print(\"\\n--- 4f: 创建交互项 ---\")\n",
    "def create_polynomial_interactions(df, n_train, continuous_cols_candidates, degree=2, interaction_only=True):\n",
    "    df_poly = df.copy()\n",
    "    print(f\"Creating interaction terms (degree={degree}, interaction_only={interaction_only})...\")\n",
    "\n",
    "    \n",
    "    binary_cols = [col for col in df_poly.columns if df_poly[col].nunique(dropna=False) == 2 and df_poly[col].min() == 0 and df_poly[col].max() == 1]\n",
    "    all_numeric_cols = df_poly.select_dtypes(include=np.number).columns.tolist()\n",
    "    current_continuous_cols = [col for col in all_numeric_cols if col not in binary_cols]\n",
    "    cols_for_poly = [col for col in continuous_cols_candidates if col in current_continuous_cols]\n",
    "\n",
    "    print(f\"  Creating interactions for {len(cols_for_poly)} features: {cols_for_poly}\")\n",
    "    if not cols_for_poly: return df_poly\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n",
    "\n",
    "    # --- Impute using ONLY train median before fitting ---\n",
    "    train_part_poly = df_poly.iloc[:n_train][cols_for_poly]\n",
    "    medians_poly = train_part_poly.median()\n",
    "    \n",
    "    df_poly[cols_for_poly] = df_poly[cols_for_poly].fillna(medians_poly)\n",
    "    # Final fallback fill if medians were NaN\n",
    "    if df_poly[cols_for_poly].isnull().any().any():\n",
    "        print(\"    Warning: Fallback filling remaining NaNs with 0 before interaction.\")\n",
    "        df_poly[cols_for_poly] = df_poly[cols_for_poly].fillna(0)\n",
    "\n",
    "    try:\n",
    "        poly.fit(df_poly.iloc[:n_train][cols_for_poly])\n",
    "        poly_features = poly.transform(df_poly[cols_for_poly])\n",
    "        poly_feature_names = [name.replace(' ', '_TIMES_').replace('^2', '_SQ') for name in poly.get_feature_names_out(cols_for_poly)]\n",
    "        print(f\"  Generated {len(poly_feature_names)} polynomial/interaction features.\")\n",
    "\n",
    "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df_poly.index)\n",
    "\n",
    "        # Drop original columns used for interactions\n",
    "        df_poly = df_poly.drop(columns=cols_for_poly, errors='ignore')\n",
    "        df_final = pd.concat([df_poly, poly_df], axis=1)\n",
    "\n",
    "        print(f\"Interaction terms created. New shape: {df_final.shape}\")\n",
    "        return df_final\n",
    "    except Exception as e:\n",
    "        print(f\"  Error creating interaction terms: {e}. Returning unmodified dataframe.\")\n",
    "        return df_poly\n",
    "\n",
    "# Select features for interaction \n",
    "# Use the un-transformed bedroom/livingroom/bathroom counts\n",
    "interaction_candidates = ['卧室数', '客厅数', '卫生间数', '相对楼层', 'log_平均每卧室面积'] # Example\n",
    "interaction_candidates_existing = [col for col in interaction_candidates if col in df_processed.columns]\n",
    "df_final_eng = create_polynomial_interactions(df_processed, n_train, interaction_candidates_existing)\n",
    "\n",
    "del df_processed # Clean up memory\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Advanced Feature Engineering Complete ---\")\n",
    "print(f\"Final engineered shape: {df_final_eng.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 5: 最终数据准备 ---\n",
      "分离后: X_train_full_eng=(98899, 348), X_test_kaggle_eng=(9773, 348)\n",
      "\n",
      "--- 5b: 处理 Y 异常值 (IQR on original Price) ---\n",
      "IQR outlier bounds (original Price, 0.01-0.95): [-2067162, 3693197]\n",
      "移除 Y 异常值前: 98899 行\n",
      "移除 Y 异常值后: 98240 行 (移除了 659 行)\n",
      "\n",
      "--- 5c: 划分训练/验证集 (80/20) ---\n",
      "划分后: X_train=(78592, 348), X_val=(19648, 348)\n",
      "目标变量: y_train_ln=(78592,), y_val_ln=(19648,)\n",
      "评估用目标: y_train_orig=(78592,), y_val_orig=(19648,)\n",
      "\n",
      "--- 5d: X 特征最终处理 (Clipping & ColumnTransformer) ---\n",
      "  Calculating and applying clipping thresholds (based on 80% train split)...\n",
      "  Applying clipping to 65 columns...\n",
      "  Clipping applied.\n",
      "  Defining and fitting ColumnTransformer...\n",
      "    Found 65 numeric features.\n",
      "    Found 13 categorical features.\n",
      "    Found 270 boolean features.\n",
      "  ColumnTransformer applied. Final shapes:\n",
      "    X_train_scaled: (78592, 412)\n",
      "    X_val_scaled: (19648, 412)\n",
      "    X_test_kaggle_scaled: (9773, 412)\n",
      "    X_train_clean_scaled_full: (98240, 412)\n",
      "  Final NaN check (X_train_scaled): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 5: 最终数据准备 ---\")\n",
    "\n",
    "# --- 5a: 分离 Kaggle 测试集 ---\n",
    "X_train_full_eng = df_final_eng.iloc[:n_train].copy()\n",
    "X_test_kaggle_eng = df_final_eng.iloc[n_train:].copy()\n",
    "print(f\"分离后: X_train_full_eng={X_train_full_eng.shape}, X_test_kaggle_eng={X_test_kaggle_eng.shape}\")\n",
    "\n",
    "# Target variables (already stored: y_train_full, y_train_ln_full)\n",
    "\n",
    "# --- 5b: Y 异常值处理 (基于原始 Rent Price IQR) ---\n",
    "print(\"\\n--- 5b: 处理 Y 异常值 (IQR on original Price) ---\")\n",
    "q_low = 0.01\n",
    "q_high = 0.95\n",
    "Q1 = y_train_full.quantile(q_low)\n",
    "Q3 = y_train_full.quantile(q_high)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "mask_y_clean = (y_train_full >= lower_bound) & (y_train_full <= upper_bound)\n",
    "rows_before = len(y_train_full)\n",
    "rows_after = mask_y_clean.sum()\n",
    "\n",
    "print(f\"IQR outlier bounds (original Price, {q_low}-{q_high}): [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "print(f\"移除 Y 异常值前: {rows_before} 行\")\n",
    "print(f\"移除 Y 异常值后: {rows_after} 行 (移除了 {rows_before - rows_after} 行)\")\n",
    "\n",
    "X_train_clean_eng = X_train_full_eng[mask_y_clean].copy()\n",
    "y_train_clean = y_train_full[mask_y_clean].copy() # Original price scale, cleaned\n",
    "y_train_ln_clean = y_train_ln_full[mask_y_clean].copy() # Log price scale, cleaned\n",
    "\n",
    "# --- 5c: 训练集/验证集划分 (80/20) ---\n",
    "print(\"\\n--- 5c: 划分训练/验证集 (80/20) ---\")\n",
    "X_train, X_val, y_train_ln, y_val_ln = train_test_split(\n",
    "    X_train_clean_eng,\n",
    "    y_train_ln_clean, \n",
    "    test_size=0.2,\n",
    "    random_state=111\n",
    ")\n",
    "# Get corresponding original prices for evaluation\n",
    "y_train_orig = y_train_clean.loc[y_train_ln.index]\n",
    "y_val_orig = y_train_clean.loc[y_val_ln.index]\n",
    "\n",
    "print(f\"划分后: X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "print(f\"目标变量: y_train_ln={y_train_ln.shape}, y_val_ln={y_val_ln.shape}\")\n",
    "print(f\"评估用目标: y_train_orig={y_train_orig.shape}, y_val_orig={y_val_orig.shape}\")\n",
    "\n",
    "# --- 5d: X 特征最终处理 (Clipping & ColumnTransformer: Impute, Scale, OHE) ---\n",
    "print(\"\\n--- 5d: X 特征最终处理 (Clipping & ColumnTransformer) ---\")\n",
    "\n",
    "# --- Clipping (Thresholds from X_train ONLY) ---\n",
    "print(\"  Calculating and applying clipping thresholds (based on 80% train split)...\")\n",
    "\n",
    "\n",
    "numeric_features_final = X_train.select_dtypes(include=np.number, exclude=['category', bool]).columns\n",
    "\n",
    "lower_bounds_X = X_train[numeric_features_final].quantile(0.01)\n",
    "upper_bounds_X = X_train[numeric_features_final].quantile(0.99)\n",
    "\n",
    "# Apply clipping\n",
    "\n",
    "lower_bounds_X = lower_bounds_X.astype(float) # Ensure bounds are float\n",
    "upper_bounds_X = upper_bounds_X.astype(float)\n",
    "\n",
    "print(f\"  Applying clipping to {len(numeric_features_final)} columns...\")\n",
    "\n",
    "# Iterate through columns to clip safely\n",
    "for col in numeric_features_final:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].clip(lower=lower_bounds_X.get(col), upper=upper_bounds_X.get(col))\n",
    "    if col in X_val.columns:\n",
    "        X_val[col] = X_val[col].clip(lower=lower_bounds_X.get(col), upper=upper_bounds_X.get(col))\n",
    "    if col in X_test_kaggle_eng.columns:\n",
    "        X_test_kaggle_eng[col] = X_test_kaggle_eng[col].clip(lower=lower_bounds_X.get(col), upper=upper_bounds_X.get(col))\n",
    "    if col in X_train_clean_eng.columns:\n",
    "        X_train_clean_eng[col] = X_train_clean_eng[col].clip(lower=lower_bounds_X.get(col), upper=upper_bounds_X.get(col))\n",
    "\n",
    "\n",
    "\n",
    "print(\"  Clipping applied.\")\n",
    "\n",
    "# --- ColumnTransformer ---\n",
    "print(\"  Defining and fitting ColumnTransformer...\")\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=np.number, exclude=['category', bool]).columns.tolist() # Exclude category/bool again\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "boolean_features = X_train.select_dtypes(include=[bool]).columns.tolist() # From geo clusters\n",
    "\n",
    "print(f\"    Found {len(numeric_features)} numeric features.\")\n",
    "print(f\"    Found {len(categorical_features)} categorical features.\")\n",
    "print(f\"    Found {len(boolean_features)} boolean features.\")\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='未知')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "boolean_transformer = 'passthrough' \n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bool', boolean_transformer, boolean_features)],\n",
    "    remainder='drop')\n",
    "\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_scaled = preprocessor.transform(X_train)\n",
    "X_val_scaled = preprocessor.transform(X_val)\n",
    "X_test_kaggle_scaled = preprocessor.transform(X_test_kaggle_eng)\n",
    "X_train_clean_scaled_full = preprocessor.transform(X_train_clean_eng) # For LassoCV\n",
    "\n",
    "# Get feature names after transformation\n",
    "feature_names_out = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Convert back to DataFrame (optional but helpful for feature importance)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names_out, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_names_out, index=X_val.index)\n",
    "X_test_kaggle_scaled = pd.DataFrame(X_test_kaggle_scaled, columns=feature_names_out, index=X_test_kaggle_eng.index)\n",
    "X_train_clean_scaled_full = pd.DataFrame(X_train_clean_scaled_full, columns=feature_names_out, index=X_train_clean_eng.index)\n",
    "\n",
    "print(f\"  ColumnTransformer applied. Final shapes:\")\n",
    "print(f\"    X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"    X_val_scaled: {X_val_scaled.shape}\")\n",
    "print(f\"    X_test_kaggle_scaled: {X_test_kaggle_scaled.shape}\")\n",
    "print(f\"    X_train_clean_scaled_full: {X_train_clean_scaled_full.shape}\")\n",
    "\n",
    "\n",
    "print(f\"  Final NaN check (X_train_scaled): {X_train_scaled.isnull().sum().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 6: 特征选择 (LassoCV) ---\n",
      "LassoCV best alpha: 0.001993\n",
      "LassoCV selected 80 features out of 412.\n",
      "Shapes after selection:\n",
      "  X_train_selected: (78592, 80)\n",
      "  X_val_selected: (19648, 80)\n",
      "  X_test_kaggle_selected: (9773, 80)\n",
      "  X_train_clean_selected_full: (98240, 80)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 6: 特征选择 (LassoCV) ---\")\n",
    "lasso_cv_selector = LassoCV(cv=6, random_state=111, max_iter=5000, n_jobs=-1)\n",
    "\n",
    "# Fit on the FULL cleaned & scaled training data\n",
    "lasso_cv_selector.fit(X_train_clean_scaled_full, y_train_ln_clean)\n",
    "\n",
    "print(f\"LassoCV best alpha: {lasso_cv_selector.alpha_:.6f}\")\n",
    "\n",
    "mask_selected = lasso_cv_selector.coef_ != 0\n",
    "num_selected = mask_selected.sum()\n",
    "num_total = len(mask_selected)\n",
    "print(f\"LassoCV selected {num_selected} features out of {num_total}.\")\n",
    "\n",
    "# Apply feature mask\n",
    "X_train_selected = X_train_scaled.loc[:, mask_selected]\n",
    "X_val_selected = X_val_scaled.loc[:, mask_selected]\n",
    "X_train_clean_selected_full = X_train_clean_scaled_full.loc[:, mask_selected]\n",
    "X_test_kaggle_selected = X_test_kaggle_scaled.loc[:, mask_selected]\n",
    "\n",
    "print(f\"Shapes after selection:\")\n",
    "print(f\"  X_train_selected: {X_train_selected.shape}\")\n",
    "print(f\"  X_val_selected: {X_val_selected.shape}\")\n",
    "print(f\"  X_test_kaggle_selected: {X_test_kaggle_selected.shape}\")\n",
    "print(f\"  X_train_clean_selected_full: {X_train_clean_selected_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 7: 建模、调参与评估 ---\n",
      "  Training OLS...\n",
      "    OLS CV MAE: 114128.14\n",
      "  Tuning Lasso...\n",
      "    Lasso best alpha: 0.000599, CV MAE: 113860.24\n",
      "  Tuning Ridge...\n",
      "    Ridge best alpha: 1.000, CV MAE: 114126.87\n",
      "  Tuning ElasticNet...\n",
      "    ElasticNet best params: {'alpha': np.float64(0.001), 'l1_ratio': 0.5}, CV MAE: 113894.51\n",
      "  Tuning LightGBM...\n",
      "    LGBM best params: {'learning_rate': 0.1, 'n_estimators': 500, 'num_leaves': 40, 'subsample': 0.8}, CV MAE: 64811.11\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 7: 建模、调参与评估 ---\")\n",
    "\n",
    "cv_6 = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "# Scorer for evaluation in original scale\n",
    "def mae_original_scorer(y_true_log, y_pred_log):\n",
    "    y_true_orig = np.expm1(y_true_log)\n",
    "    y_pred_orig = np.expm1(y_pred_log)\n",
    "    return -mean_absolute_error(y_true_orig, y_pred_orig) # Return negative MAE\n",
    "\n",
    "mae_scorer = make_scorer(mae_original_scorer, greater_is_better=True)\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "# Evaluation helper\n",
    "def evaluate_model(model, model_name, X_tr, y_tr_ln, y_tr_orig, X_v, y_v_ln, y_v_orig):\n",
    "    # In-Sample\n",
    "    y_pred_log_is = model.predict(X_tr)\n",
    "    y_pred_orig_is = np.expm1(y_pred_log_is)\n",
    "    is_mae = mean_absolute_error(y_tr_orig, y_pred_orig_is)\n",
    "    is_rmse = root_mean_squared_error(y_tr_orig, y_pred_orig_is) # Use root_mean_squared_error\n",
    "\n",
    "    # Out-of-Sample\n",
    "    y_pred_log_oos = model.predict(X_v)\n",
    "    y_pred_orig_oos = np.expm1(y_pred_log_oos)\n",
    "    oos_mae = mean_absolute_error(y_v_orig, y_pred_orig_oos)\n",
    "    oos_rmse = root_mean_squared_error(y_v_orig, y_pred_orig_oos)\n",
    "\n",
    "    return {\"IS_MAE\": is_mae, \"IS_RMSE\": is_rmse, \"OOS_MAE\": oos_mae, \"OOS_RMSE\": oos_rmse}\n",
    "\n",
    "# --- OLS ---\n",
    "print(\"  Training OLS...\")\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X_train_selected, y_train_ln)\n",
    "results['OLS'] = evaluate_model(model_ols, 'OLS', X_train_selected, y_train_ln, y_train_orig, X_val_selected, y_val_ln, y_val_orig)\n",
    "cv_scores = cross_val_score(model_ols, X_train_clean_selected_full, y_train_ln_clean, cv=cv_6, scoring=mae_scorer, n_jobs=-1)\n",
    "results['OLS']['CV_MAE'] = -np.mean(cv_scores)\n",
    "best_models['OLS'] = model_ols\n",
    "print(f\"    OLS CV MAE: {results['OLS']['CV_MAE']:.2f}\")\n",
    "\n",
    "# --- Lasso ---\n",
    "print(\"  Tuning Lasso...\")\n",
    "param_grid_lasso = {'alpha': np.logspace(-6, -1, 10)}\n",
    "grid_lasso = GridSearchCV(Lasso(max_iter=5000, random_state=111), param_grid_lasso, cv=cv_6, scoring=mae_scorer, n_jobs=-1)\n",
    "grid_lasso.fit(X_train_clean_selected_full, y_train_ln_clean)\n",
    "model_lasso_best = grid_lasso.best_estimator_\n",
    "results['Lasso'] = evaluate_model(model_lasso_best, 'Lasso', X_train_selected, y_train_ln, y_train_orig, X_val_selected, y_val_ln, y_val_orig)\n",
    "results['Lasso']['CV_MAE'] = -grid_lasso.best_score_\n",
    "best_models['Lasso'] = model_lasso_best\n",
    "print(f\"    Lasso best alpha: {grid_lasso.best_params_['alpha']:.6f}, CV MAE: {results['Lasso']['CV_MAE']:.2f}\")\n",
    "\n",
    "# --- Ridge ---\n",
    "print(\"  Tuning Ridge...\")\n",
    "param_grid_ridge_fine = {'alpha': np.logspace(-4, 0, 10)}\n",
    "grid_ridge = GridSearchCV(Ridge(random_state=111), param_grid_ridge_fine, cv=cv_6, scoring=mae_scorer, n_jobs=-1)\n",
    "grid_ridge.fit(X_train_clean_selected_full, y_train_ln_clean)\n",
    "model_ridge_best = grid_ridge.best_estimator_\n",
    "results['Ridge'] = evaluate_model(model_ridge_best, 'Ridge', X_train_selected, y_train_ln, y_train_orig, X_val_selected, y_val_ln, y_val_orig)\n",
    "results['Ridge']['CV_MAE'] = -grid_ridge.best_score_\n",
    "best_models['Ridge'] = model_ridge_best\n",
    "print(f\"    Ridge best alpha: {grid_ridge.best_params_['alpha']:.3f}, CV MAE: {results['Ridge']['CV_MAE']:.2f}\")\n",
    "\n",
    "# --- ElasticNet ---\n",
    "print(\"  Tuning ElasticNet...\")\n",
    "param_grid_enet = {'alpha': np.logspace(-6, -1, 6), 'l1_ratio': [0.1, 0.5, 0.9, 0.95, 1.0]}\n",
    "grid_enet = GridSearchCV(ElasticNet(max_iter=5000, random_state=111), param_grid_enet, cv=cv_6, scoring=mae_scorer, n_jobs=-1)\n",
    "grid_enet.fit(X_train_clean_selected_full, y_train_ln_clean)\n",
    "model_enet_best = grid_enet.best_estimator_\n",
    "results['ElasticNet'] = evaluate_model(model_enet_best, 'ElasticNet', X_train_selected, y_train_ln, y_train_orig, X_val_selected, y_val_ln, y_val_orig)\n",
    "results['ElasticNet']['CV_MAE'] = -grid_enet.best_score_\n",
    "best_models['ElasticNet'] = model_enet_best\n",
    "print(f\"    ElasticNet best params: {grid_enet.best_params_}, CV MAE: {results['ElasticNet']['CV_MAE']:.2f}\")\n",
    "\n",
    "# --- LightGBM ---\n",
    "print(\"  Tuning LightGBM...\")\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [300, 500], # Reduced for speed\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'num_leaves': [31, 40],\n",
    "    'subsample': [0.8]\n",
    "}\n",
    "grid_lgbm = GridSearchCV(\n",
    "    lgb.LGBMRegressor(random_state=111, n_jobs=1, verbose=-1, subsample_freq=1, objective='mae', metric='mae'),\n",
    "    param_grid_lgbm, cv=cv_6, scoring=mae_scorer, n_jobs=-1, verbose=0)\n",
    "grid_lgbm.fit(X_train_clean_selected_full, y_train_ln_clean)\n",
    "model_lgbm_best = grid_lgbm.best_estimator_\n",
    "results['LightGBM'] = evaluate_model(model_lgbm_best, 'LightGBM', X_train_selected, y_train_ln, y_train_orig, X_val_selected, y_val_ln, y_val_orig)\n",
    "results['LightGBM']['CV_MAE'] = -grid_lgbm.best_score_\n",
    "best_models['LightGBM'] = model_lgbm_best\n",
    "print(f\"    LGBM best params: {grid_lgbm.best_params_}, CV MAE: {results['LightGBM']['CV_MAE']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 8: 最终结果报告 ---\n",
      "最佳 *线性* 模型 (基于 CV MAE): Lasso\n",
      "\n",
      "========================================\n",
      " 性能报告 (MAE - 原始价格) \n",
      "========================================\n",
      "|                   |   In-Sample MAE |   Out-of-Sample MAE |   6-Fold CV MAE |\n",
      "|:------------------|----------------:|--------------------:|----------------:|\n",
      "| OLS               |      114202.194 |          113185.516 |      114128.144 |\n",
      "| Lasso             |      113995.175 |          112894.116 |      113860.242 |\n",
      "| Ridge             |      114267.411 |          113161.249 |      114126.872 |\n",
      "| ElasticNet        |      114027.716 |          112942.791 |      113894.511 |\n",
      "| Best Linear Model |      113995.175 |          112894.116 |      113860.242 |\n",
      "| LightGBM          |       56489.993 |           56304.284 |       64811.112 |\n",
      "\n",
      "========================================\n",
      " 性能报告 (RMSE - 原始价格) \n",
      "========================================\n",
      "|                   |   In-Sample RMSE |   Out-of-Sample RMSE |\n",
      "|:------------------|-----------------:|---------------------:|\n",
      "| OLS               |       208038.696 |           204481.756 |\n",
      "| Lasso             |       208175.922 |           204388.803 |\n",
      "| Ridge             |       208327.943 |           204603.195 |\n",
      "| ElasticNet        |       208283.708 |           204520.558 |\n",
      "| Best Linear Model |       208175.922 |           204388.803 |\n",
      "| LightGBM          |       114331.777 |           113439.464 |\n",
      "\n",
      "报告使用的总训练样本数 (移除y异常值后): 98240\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 8: 最终结果报告 ---\")\n",
    "\n",
    "linear_models_to_compare = {\n",
    "    'OLS': results['OLS']['CV_MAE'],\n",
    "    'Lasso': results['Lasso']['CV_MAE'],\n",
    "    'Ridge': results['Ridge']['CV_MAE'],\n",
    "    'ElasticNet': results['ElasticNet']['CV_MAE']\n",
    "}\n",
    "best_linear_model_name = min(linear_models_to_compare, key=linear_models_to_compare.get)\n",
    "results['Best Linear Model'] = results[best_linear_model_name]\n",
    "best_models['Best Linear Model'] = best_models[best_linear_model_name]\n",
    "\n",
    "print(f\"最佳 *线性* 模型 (基于 CV MAE): {best_linear_model_name}\")\n",
    "\n",
    "# Prepare report DataFrames\n",
    "# 检查你的 index_order，如果 LightGBM 在结果中，也应包含在内\n",
    "index_order = ['OLS', 'Lasso', 'Ridge', 'ElasticNet', 'Best Linear Model']\n",
    "if 'LightGBM' in results:\n",
    "    index_order.append('LightGBM')\n",
    "\n",
    "\n",
    "report_data_mae = {\n",
    "    'In-Sample MAE': [results[m]['IS_MAE'] for m in index_order],\n",
    "    'Out-of-Sample MAE': [results[m]['OOS_MAE'] for m in index_order],\n",
    "    '6-Fold CV MAE': [results[m]['CV_MAE'] for m in index_order]\n",
    "}\n",
    "df_report_mae = pd.DataFrame(report_data_mae, index=index_order)\n",
    "\n",
    "report_data_rmse = {\n",
    "    'In-Sample RMSE': [results[m]['IS_RMSE'] for m in index_order],\n",
    "    'Out-of-Sample RMSE': [results[m]['OOS_RMSE'] for m in index_order],\n",
    "    \n",
    "}\n",
    "df_report_rmse = pd.DataFrame(report_data_rmse, index=index_order)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" 性能报告 (MAE - 原始价格) \")\n",
    "print(\"=\"*40)\n",
    "# --- 修改在这里 ---\n",
    "# 使用 floatfmt=\".3f\" \n",
    "print(df_report_mae.to_markdown(floatfmt=\".3f\"))\n",
    "# --- 修改结束 ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" 性能报告 (RMSE - 原始价格) \")\n",
    "print(\"=\"*40)\n",
    "# --- 修改在这里 ---\n",
    "# 同样应用于 RMSE 表格\n",
    "print(df_report_rmse.to_markdown(floatfmt=\".3f\"))\n",
    "# --- 修改结束 ---\n",
    "\n",
    "# 确保你使用的是正确的 'y_train_clean' 变量 (来自租金或房价模型)\n",
    "if 'y_train_clean' in locals():\n",
    "    print(f\"\\n报告使用的总训练样本数 (移除y异常值后): {len(y_train_clean)}\")\n",
    "elif 'y_train_clean_orig' in locals():\n",
    "    print(f\"\\n报告使用的总训练样本数 (移除y异常值后): {len(y_train_clean_orig)}\")\n",
    "else:\n",
    "    print(\"\\n无法确定报告样本数。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 9: 生成 Kaggle 提交文件 ---\n",
      "文件将保存到: './output' 文件夹\n",
      "  正在为模型 'OLS' 生成预测...\n",
      "    已生成: './output\\submission_rent_OLS.csv'\n",
      "  正在为模型 'Lasso' 生成预测...\n",
      "    已生成: './output\\submission_rent_Lasso.csv'\n",
      "  正在为模型 'Ridge' 生成预测...\n",
      "    已生成: './output\\submission_rent_Ridge.csv'\n",
      "  正在为模型 'ElasticNet' 生成预测...\n",
      "    已生成: './output\\submission_rent_ElasticNet.csv'\n",
      "  正在为模型 'LightGBM' 生成预测...\n",
      "    已生成: './output\\submission_rent_LightGBM.csv'\n",
      "  正在为模型 'Best Linear Model' 生成预测...\n",
      "    已生成: './output\\submission_rent_Best_Linear_Model.csv'\n",
      "\n",
      "--- 所有提交文件已生成完毕 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 步骤 9: 为 *所有* 模型生成 Kaggle 提交文件\n",
    "\n",
    "print(\"\\n--- 步骤 9: 生成 Kaggle 提交文件 ---\")\n",
    "\n",
    "# 1. 定义输出文件夹\n",
    "output_dir = './output'\n",
    "# 2. 创建文件夹 (如果它不存在)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"文件将保存到: '{output_dir}' 文件夹\")\n",
    "\n",
    "# 3. 遍历 `best_models` 字典中的每一个模型\n",
    "#    (这包括 OLS, Lasso, Ridge, ElasticNet, LightGBM, 和 'Best Linear Model')\n",
    "for model_name, final_model in best_models.items():\n",
    "    \n",
    "    print(f\"  正在为模型 '{model_name}' 生成预测...\")\n",
    "    \n",
    "    # 4. 在 Kaggle 测试集上预测 (已缩放、已选择特征)\n",
    "    #    (使用来自 7.ipynb 的 X_test_kaggle_selected)\n",
    "    y_kaggle_pred_log = final_model.predict(X_test_kaggle_selected)\n",
    "\n",
    "    # 5. 转换回原始价格\n",
    "    y_kaggle_pred_orig = np.expm1(y_kaggle_pred_log)\n",
    "\n",
    "    # 6. 检查是否有负数预测\n",
    "    y_kaggle_pred_orig = np.clip(y_kaggle_pred_orig, a_min=0, a_max=None) \n",
    "\n",
    "    # 7. 创建提交文件\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_ids,\n",
    "        'Price': y_kaggle_pred_orig\n",
    "    })\n",
    "\n",
    "    # 8. 创建动态的文件名和路径\n",
    "    safe_model_name = model_name.replace(' ', '_') # 替换空格\n",
    "    file_name = f'submission_rent_{safe_model_name}.csv'\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # 9. 保存文件\n",
    "    submission.to_csv(file_path, index=False)\n",
    "    print(f\"    已生成: '{file_path}'\")\n",
    "\n",
    "print(\"\\n--- 所有提交文件已生成完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并OLS模型的price和rent预测\n",
    "pd.concat([pd.read_csv('./output/submission_price_OLS.csv'), pd.read_csv('./output/submission_rent_OLS.csv')], axis=1).to_csv('./output/combined_submission_OLS.csv', index=False)\n",
    "\n",
    "# 合并Lasso模型的price和rent预测\n",
    "pd.concat([pd.read_csv('./output/submission_price_Lasso.csv'), pd.read_csv('./output/submission_rent_Lasso.csv')], axis=1).to_csv('./output/combined_submission_Lasso.csv', index=False)\n",
    "\n",
    "# 合并Ridge模型的price和rent预测\n",
    "pd.concat([pd.read_csv('./output/submission_price_Ridge.csv'), pd.read_csv('./output/submission_rent_Ridge.csv')], axis=1).to_csv('./output/combined_submission_Ridge.csv', index=False)\n",
    "\n",
    "# 合并ElasticNet模型的price和rent预测\n",
    "pd.concat([pd.read_csv('./output/submission_price_ElasticNet.csv'), pd.read_csv('./output/submission_rent_ElasticNet.csv')], axis=1).to_csv('./output/combined_submission_ElasticNet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
