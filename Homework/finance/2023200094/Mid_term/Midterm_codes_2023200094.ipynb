{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec2c804",
   "metadata": {},
   "source": [
    "# 房价预测建模\n",
    "## Part 1: 环境设置和数据预处理\n",
    "### 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e72a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic \n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import types\n",
    "\n",
    "# 数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据预处理与特征工程\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 机器学习模型\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# 评估指标\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 其他\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 设置 Matplotlib 支持中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimSong']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f79d15",
   "metadata": {},
   "source": [
    "### 2. 数据加载\n",
    "\n",
    "首先，从同一文件目录下加载相应的.csv格式的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5cbd8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded successfully!\n",
      "Training data shape (price): (103871, 55)\n",
      "Test data shape (price): (34017, 55)\n",
      "Training data shape (rent): (98899, 46)\n",
      "Test data shape (rent): (9773, 46)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    df_train_price = pd.read_csv('ruc_Class25Q2_train_price.csv')\n",
    "    df_test_price = pd.read_csv('ruc_Class25Q2_test_price.csv')\n",
    "    df_train_rent = pd.read_csv('ruc_Class25Q2_train_rent.csv')\n",
    "    df_test_rent = pd.read_csv('ruc_Class25Q2_test_rent.csv')\n",
    "    \n",
    "    print(\"Files loaded successfully!\")\n",
    "    print(f\"Training data shape (price): {df_train_price.shape}\")\n",
    "    print(f\"Test data shape (price): {df_test_price.shape}\")\n",
    "    print(f\"Training data shape (rent): {df_train_rent.shape}\")\n",
    "    print(f\"Test data shape (rent): {df_test_rent.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f9718",
   "metadata": {},
   "source": [
    "然后将测试集和训练集拼接在一起，并新增一列\"source\"来标明来源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1546d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练集的原始行数\n",
    "n_train_price = df_train_price.shape[0]\n",
    "n_train_rent = df_train_rent.shape[0]\n",
    "\n",
    "# b. 保存训练集的目标变量\n",
    "y_train_price = df_train_price['Price'].copy()\n",
    "y_train_rent = df_train_rent['Price'].copy()\n",
    "\n",
    "# c. 保存测试集的 ID，用于最终提交\n",
    "test_ids_price = df_test_price['ID'].copy()\n",
    "test_ids_rent = df_test_rent['ID'].copy()\n",
    "\n",
    "\n",
    "# 使用 pd.concat 将去掉 Price 和 ID 列的数据集进行纵向合并\n",
    "df_price = pd.concat([ df_train_price.drop(columns=['Price']),\n",
    "                          df_test_price.drop(columns=['ID'])], ignore_index=True)\n",
    "df_rent = pd.concat([ df_train_rent.drop(columns=['Price']),\n",
    "                          df_test_rent.drop(columns=['ID'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af26fd",
   "metadata": {},
   "source": [
    "### 3.  探索性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0005babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137888 entries, 0 to 137887\n",
      "Data columns (total 54 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   城市         137888 non-null  int64  \n",
      " 1   区域         137888 non-null  float64\n",
      " 2   板块         137888 non-null  float64\n",
      " 3   环线         56096 non-null   object \n",
      " 4   房屋户型       137294 non-null  object \n",
      " 5   所在楼层       137888 non-null  object \n",
      " 6   建筑面积       137888 non-null  object \n",
      " 7   套内面积       45899 non-null   object \n",
      " 8   房屋朝向       137887 non-null  object \n",
      " 9   建筑结构       137294 non-null  object \n",
      " 10  装修情况       137294 non-null  object \n",
      " 11  梯户比例       134634 non-null  object \n",
      " 12  配备电梯       121445 non-null  object \n",
      " 13  别墅类型       1597 non-null    object \n",
      " 14  交易时间       137888 non-null  object \n",
      " 15  交易权属       137888 non-null  object \n",
      " 16  上次交易       105198 non-null  object \n",
      " 17  房屋用途       137887 non-null  object \n",
      " 18  房屋年限       82082 non-null   object \n",
      " 19  产权所属       137888 non-null  object \n",
      " 20  抵押信息       0 non-null       float64\n",
      " 21  房屋优势       110273 non-null  object \n",
      " 22  核心卖点       111226 non-null  object \n",
      " 23  户型介绍       38783 non-null   object \n",
      " 24  周边配套       79719 non-null   object \n",
      " 25  交通出行       79266 non-null   object \n",
      " 26  lon        137888 non-null  float64\n",
      " 27  lat        137888 non-null  float64\n",
      " 28  年份         137888 non-null  float64\n",
      " 29  区县         126915 non-null  float64\n",
      " 30  板块_comm    126498 non-null  float64\n",
      " 31  环线位置       41470 non-null   object \n",
      " 32  物业类别       101942 non-null  object \n",
      " 33  建筑年代       93381 non-null   object \n",
      " 34  开发商        91155 non-null   object \n",
      " 35  房屋总数       127042 non-null  object \n",
      " 36  楼栋总数       127042 non-null  object \n",
      " 37  物业公司       89038 non-null   object \n",
      " 38  绿 化 率      95825 non-null   object \n",
      " 39  容 积 率      95431 non-null   float64\n",
      " 40  物 业 费      98241 non-null   object \n",
      " 41  建筑结构_comm  100935 non-null  object \n",
      " 42  物业办公电话     46360 non-null   object \n",
      " 43  产权描述       101942 non-null  object \n",
      " 44  供水         99395 non-null   object \n",
      " 45  供暖         46360 non-null   object \n",
      " 46  供电         99419 non-null   object \n",
      " 47  燃气费        96030 non-null   object \n",
      " 48  供热费        42372 non-null   object \n",
      " 49  停车位        93749 non-null   float64\n",
      " 50  停车费用       92413 non-null   object \n",
      " 51  coord_x    127042 non-null  float64\n",
      " 52  coord_y    127042 non-null  float64\n",
      " 53  客户反馈       137888 non-null  object \n",
      "dtypes: float64(12), int64(1), object(41)\n",
      "memory usage: 56.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_price.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec169536",
   "metadata": {},
   "source": [
    "###  4. 数据处理\n",
    "\n",
    "#### a. 删除无用和信息冗余的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cedf56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_price = [\n",
    "    '抵押信息',       # 完全为空\n",
    "    '别墅类型',       # 缺失值超过98%\n",
    "    '物业办公电话',   # 对价格无预测能力\n",
    "    '区县',           # 与'区域'列信息高度重叠\n",
    "    '开发商',\n",
    "    '物业公司',\n",
    "    '环线位置',       # 与'环线'列信息高度重叠\n",
    "    'coord_x',        # 与 lon/lat 重复\n",
    "    'coord_y',        # 与 lon/lat 重复\n",
    "    # 文本描述列\n",
    "    '房屋优势', '核心卖点', '户型介绍', '周边配套', '交通出行', '客户反馈'\n",
    "]\n",
    "\n",
    "df_price = df_price.drop(columns=cols_to_drop_price, errors='ignore')\n",
    "\n",
    "cols_to_drop_rent = [\n",
    "    '车位',   # 对价格无预测能力\n",
    "    '开发商',           # 与'区域'列信息高度重叠\n",
    "    '物业公司',\n",
    "    '物业办公电话',       # 与'环线'列信息高度重叠\n",
    "    'coord_x',        # 与 lon/lat 重复\n",
    "    'coord_y',        # 与 lon/lat 重复\n",
    "    # 文本描述列\n",
    "    '客户反馈',\n",
    "    '装修',\n",
    "    '供水',\n",
    "    '供暖',\n",
    "    '供电',\n",
    "]\n",
    "df_rent = df_rent.drop(columns=cols_to_drop_rent, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1257c2",
   "metadata": {},
   "source": [
    "#### b. 处理区块的列数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd297d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "板块",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "板块_comm",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "78091f3e-4c72-4171-8643-694fbaec20c1",
       "rows": [
        [
         "287",
         "1103.0",
         "466.0"
        ],
        [
         "372",
         "1103.0",
         "466.0"
        ],
        [
         "420",
         "1103.0",
         "466.0"
        ],
        [
         "424",
         "1103.0",
         "466.0"
        ],
        [
         "506",
         "1103.0",
         "466.0"
        ],
        [
         "519",
         "1103.0",
         "466.0"
        ],
        [
         "526",
         "1103.0",
         "466.0"
        ],
        [
         "574",
         "1103.0",
         "466.0"
        ],
        [
         "724",
         "1103.0",
         "466.0"
        ],
        [
         "828",
         "817.0",
         "927.0"
        ],
        [
         "842",
         "1103.0",
         "466.0"
        ],
        [
         "894",
         "1167.0",
         "1090.0"
        ],
        [
         "979",
         "1103.0",
         "466.0"
        ],
        [
         "1050",
         "1090.0",
         "1167.0"
        ],
        [
         "1073",
         "1090.0",
         "1167.0"
        ],
        [
         "1134",
         "1103.0",
         "466.0"
        ],
        [
         "1164",
         "1103.0",
         "466.0"
        ],
        [
         "1310",
         "1103.0",
         "466.0"
        ],
        [
         "1466",
         "1103.0",
         "466.0"
        ],
        [
         "1468",
         "1103.0",
         "466.0"
        ],
        [
         "1499",
         "1103.0",
         "466.0"
        ],
        [
         "1690",
         "1103.0",
         "466.0"
        ],
        [
         "1694",
         "1103.0",
         "466.0"
        ],
        [
         "1733",
         "1030.0",
         "355.0"
        ],
        [
         "1821",
         "1090.0",
         "1167.0"
        ],
        [
         "1834",
         "1103.0",
         "466.0"
        ],
        [
         "1869",
         "927.0",
         "817.0"
        ],
        [
         "1939",
         "1030.0",
         "355.0"
        ],
        [
         "2142",
         "1103.0",
         "466.0"
        ],
        [
         "2342",
         "1103.0",
         "466.0"
        ],
        [
         "2375",
         "1103.0",
         "466.0"
        ],
        [
         "2524",
         "1103.0",
         "466.0"
        ],
        [
         "2707",
         "407.0",
         "480.0"
        ],
        [
         "2754",
         "1103.0",
         "466.0"
        ],
        [
         "2855",
         "1103.0",
         "466.0"
        ],
        [
         "2857",
         "1090.0",
         "1167.0"
        ],
        [
         "2905",
         "1103.0",
         "466.0"
        ],
        [
         "2966",
         "1030.0",
         "355.0"
        ],
        [
         "2974",
         "1103.0",
         "466.0"
        ],
        [
         "3024",
         "1103.0",
         "466.0"
        ],
        [
         "3230",
         "1167.0",
         "1090.0"
        ],
        [
         "3405",
         "1103.0",
         "466.0"
        ],
        [
         "3513",
         "1103.0",
         "466.0"
        ],
        [
         "3530",
         "1103.0",
         "466.0"
        ],
        [
         "3549",
         "927.0",
         "817.0"
        ],
        [
         "3788",
         "927.0",
         "817.0"
        ],
        [
         "3846",
         "1103.0",
         "466.0"
        ],
        [
         "3875",
         "1103.0",
         "466.0"
        ],
        [
         "3895",
         "817.0",
         "927.0"
        ],
        [
         "3905",
         "1103.0",
         "466.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2653
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>板块</th>\n",
       "      <th>板块_comm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137781</th>\n",
       "      <td>716.0</td>\n",
       "      <td>1130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137799</th>\n",
       "      <td>716.0</td>\n",
       "      <td>1130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137802</th>\n",
       "      <td>716.0</td>\n",
       "      <td>1130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137815</th>\n",
       "      <td>716.0</td>\n",
       "      <td>1130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137818</th>\n",
       "      <td>716.0</td>\n",
       "      <td>1130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2653 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            板块  板块_comm\n",
       "287     1103.0    466.0\n",
       "372     1103.0    466.0\n",
       "420     1103.0    466.0\n",
       "424     1103.0    466.0\n",
       "506     1103.0    466.0\n",
       "...        ...      ...\n",
       "137781   716.0   1130.0\n",
       "137799   716.0   1130.0\n",
       "137802   716.0   1130.0\n",
       "137815   716.0   1130.0\n",
       "137818   716.0   1130.0\n",
       "\n",
       "[2653 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看相似的列中值不一致的行\n",
    "inconsistent_rows = df_price[(df_price['板块'] != df_price['板块_comm']) &  (df_price['板块_comm'].notnull())]\n",
    "display(inconsistent_rows[['板块', '板块_comm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3deae230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_plate_feature(df):\n",
    "    \"\"\"\n",
    "    创建一个更可靠的板块特征 '板块_final'。\n",
    "    策略：优先使用小区板块信息 ('板块_comm')，再用房产板块信息 ('板块') 填充其缺失值。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['板块_final'] = np.where(df['板块_comm'].notna() & (df['板块_comm'] != ''), df['板块_comm'], df['板块'])\n",
    "\n",
    "    # 删除原始的两列\n",
    "    df = df.drop(columns=['板块', '板块_comm'], errors='ignore')\n",
    "    \n",
    "    df['板块'] = df['板块_final'].astype(str)\n",
    "    df = df.drop(columns=['板块_final'], errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_price = process_plate_feature(df_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2fbeb",
   "metadata": {},
   "source": [
    "## Part 2: 缺失值处理\n",
    "### 1. 解析结构化文本列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e585d",
   "metadata": {},
   "source": [
    "#### a. 处理房屋户型列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8aa03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_layout(df,col_name):\n",
    "    \"\"\"\n",
    "    解析'房屋户型'列，提取'室', '厅', '卫'的数量。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "        \n",
    "    # 使用正则表达式提取室、厅、卫的数量\n",
    "    layout_info = df[col_name].str.extract(r'(\\d+)[室房](\\d+)厅(?:.*?(\\d+)卫)?', expand=True) \n",
    "    # ?: 表示厨房信息可选; (?:...) 表示非捕获组; [室房] 匹配'室'或'房'\n",
    "    layout_info.columns = ['室', '厅', '卫']\n",
    "    \n",
    "    # 将提取出的信息合并回原DataFrame\n",
    "    df = pd.concat([df, layout_info], axis=1)\n",
    "    \n",
    "    # 转换提取出的列为数值类型, errors='coerce' 会将无法转换的设为NaN\n",
    "    for col in ['室', '厅', '卫']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "    # --- 处理缺失值 ---\n",
    "    # 计算中位数（只在训练集部分计算）\n",
    "    for col in ['室', '厅', '卫']:\n",
    "         if col in df.columns:\n",
    "             median_val = df[col].median()\n",
    "             df[col].fillna(median_val, inplace=True)\n",
    "             \n",
    "    df = df.drop(columns=[col_name], errors='ignore')\n",
    "            \n",
    "    return df\n",
    "\n",
    "df_price = parse_layout(df_price,'房屋户型') \n",
    "df_rent = parse_layout(df_rent,'户型')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792687ea",
   "metadata": {},
   "source": [
    "#### b. 处理房屋面积与套内面积列\n",
    "\n",
    "使用训练集中房屋面积与套内面积的比例（得房率）来预测缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed8c4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_area_features_revised(df, n_train):\n",
    "    \"\"\"\n",
    "    专门处理建筑面积和套内面积列，并创建得房率特征\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 清理并转换为数值类型\n",
    "    for col in ['建筑面积', '套内面积']:\n",
    "        df_processed[col] = pd.to_numeric(df_processed[col].astype(str).str.replace('㎡', ''), errors='coerce')\n",
    "\n",
    "    # 计算训练集的平均得房率\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    valid_train_data = train_part[(train_part['套内面积'].notna()) & (train_part['建筑面积'].notna()) ]\n",
    "\n",
    "    # 计算得房率\n",
    "    efficiency_rate_train = valid_train_data['套内面积'].sum() / valid_train_data['建筑面积'].sum() \n",
    "\n",
    "\n",
    "    # 尝试使用计算出的得房率填充 '套内面积' 缺失值\n",
    "    missing_mask = ((df_processed['套内面积'].isna()) \n",
    "                    | (df_processed['套内面积'] / df_processed['建筑面积'] >0.95) \n",
    "                    | (df_processed['套内面积'] / df_processed['建筑面积'] <0.6))\n",
    "    df_processed.loc[missing_mask, '套内面积'] = df_processed.loc[missing_mask, '建筑面积'] * efficiency_rate_train\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 创建 '得房率' 特征\n",
    "    df_processed['得房率'] = np.nan\n",
    "    df_processed['得房率'] = df_processed['套内面积'] / df_processed['建筑面积']\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# 调用函数处理面积相关列\n",
    "df_price = process_area_features_revised(df_price, n_train_price)\n",
    "df_rent['面积'] = pd.to_numeric(df_rent['面积'].astype(str).str.replace('㎡', ''), errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f69cc8",
   "metadata": {},
   "source": [
    "#### c. 处理楼层信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05d24b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "填充了 '总楼层' 列的 12 个缺失值 (使用训练集中位数 18)。\n",
      "填充了 '楼层类别' 列的 5 个缺失值 (使用训练集众数 '中楼层')。\n",
      "\n",
      "--- df_rent 处理结果 ---\n",
      "总楼层 缺失值: 0\n",
      "楼层类别列: ['楼层_中楼层', '楼层_低楼层', '楼层_地下室', '楼层_底层', '楼层_顶层', '楼层_高楼层']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "总楼层",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "楼层_中楼层",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "楼层_低楼层",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "楼层_地下室",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "楼层_底层",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "楼层_顶层",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "楼层_高楼层",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "b57fb623-302d-4e57-b56c-10dbae35b18f",
       "rows": [
        [
         "0",
         "6.0",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True"
        ],
        [
         "1",
         "6.0",
         "False",
         "False",
         "False",
         "False",
         "False",
         "True"
        ],
        [
         "2",
         "18.0",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False"
        ],
        [
         "3",
         "10.0",
         "False",
         "False",
         "False",
         "True",
         "False",
         "False"
        ],
        [
         "4",
         "18.0",
         "False",
         "False",
         "False",
         "False",
         "True",
         "False"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>总楼层</th>\n",
       "      <th>楼层_中楼层</th>\n",
       "      <th>楼层_低楼层</th>\n",
       "      <th>楼层_地下室</th>\n",
       "      <th>楼层_底层</th>\n",
       "      <th>楼层_顶层</th>\n",
       "      <th>楼层_高楼层</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    总楼层  楼层_中楼层  楼层_低楼层  楼层_地下室  楼层_底层  楼层_顶层  楼层_高楼层\n",
       "0   6.0   False   False   False  False  False    True\n",
       "1   6.0   False   False   False  False  False    True\n",
       "2  18.0   False   False   False   True  False   False\n",
       "3  10.0   False   False   False   True  False   False\n",
       "4  18.0   False   False   False  False   True   False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "楼层类别列缺失值: 0\n"
     ]
    }
   ],
   "source": [
    "def parse_floor_robust(df, n_train, col='所在楼层'):\n",
    "    \"\"\"\n",
    "    解析楼层列，提取'总楼层'和'楼层类别'。\n",
    "    支持 \"(共X层)\"、\"Y/X层\" 和 \"地下X层\" 格式，并能推断类别，最后填充缺失值并独热编码。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    if col not in df_processed.columns:\n",
    "        print(f\"错误：列 '{col}' 不在DataFrame中，无法处理楼层。\")\n",
    "        # 可以返回原始df或者引发错误\n",
    "        return df\n",
    "\n",
    "    # --- 步骤 1 & 2: 提取总楼层和当前楼层 ---\n",
    "    # 尝试从 \"(共X层)\" 提取总楼层\n",
    "    total_floors1 = df_processed[col].str.extract(r'\\(共(\\d+)层\\)', expand=False)\n",
    "    # 尝试从 \"/X层\" 或 \"/X)\" 提取总楼层 (更兼容的正则)\n",
    "    total_floors2 = df_processed[col].str.extract(r'/(\\d+)(?:层|\\))?$', expand=False)\n",
    "    # 尝试从 \"Y/...\" 提取当前楼层\n",
    "    current_floor_ext = df_processed[col].str.extract(r'^(\\d+)/', expand=False)\n",
    "\n",
    "    # 合并总楼层：优先用 total_floors1, 缺失则用 total_floors2\n",
    "    df_processed['总楼层'] = pd.to_numeric(total_floors1.fillna(total_floors2), errors='coerce')\n",
    "    # 转换当前楼层\n",
    "    df_processed['当前楼层_temp'] = pd.to_numeric(current_floor_ext, errors='coerce')\n",
    "\n",
    "    # --- 步骤 3: 提取明确的类别 (包括地下室) ---\n",
    "    df_processed['楼层类别_explicit'] = np.nan # 初始化为 NaN\n",
    "    # 使用 .loc 确保安全赋值\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('高楼层', na=False), '楼层类别_explicit'] = '高楼层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('中楼层', na=False), '楼层类别_explicit'] = '中楼层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('低楼层', na=False), '楼层类别_explicit'] = '低楼层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('顶层', na=False), '楼层类别_explicit'] = '顶层'\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('底层', na=False), '楼层类别_explicit'] = '底层'\n",
    "    # ******** 新增：识别地下室 ********\n",
    "    df_processed.loc[df_processed[col].astype(str).str.contains('地下', na=False), '楼层类别_explicit'] = '地下室'\n",
    "    # ***********************************\n",
    "\n",
    "    # --- 步骤 4: 推断类别 (排除已明确分类和总楼层无效的) ---\n",
    "    df_processed['楼层类别_inferred'] = np.nan\n",
    "    # 推断条件：没有明确类别，且有有效的当前和总楼层 (总楼层>0)\n",
    "    infer_mask = (df_processed['楼层类别_explicit'].isna()) & \\\n",
    "                 (df_processed['当前楼层_temp'].notna()) & \\\n",
    "                 (df_processed['总楼层'].notna()) & \\\n",
    "                 (df_processed['总楼层'] > 0)\n",
    "\n",
    "    if infer_mask.any():\n",
    "        # 只在需要推断的行上操作\n",
    "        current = df_processed.loc[infer_mask, '当前楼层_temp']\n",
    "        total = df_processed.loc[infer_mask, '总楼层']\n",
    "        ratio = current / total\n",
    "\n",
    "        # 应用推断规则 (优先判断顶层/底层)\n",
    "        # 注意：这里使用 .loc[infer_mask & (condition), ...] 来赋值\n",
    "        df_processed.loc[infer_mask, '楼层类别_inferred'] = '中楼层' # 默认中楼层\n",
    "        df_processed.loc[infer_mask & (ratio <= 1/3), '楼层类别_inferred'] = '低楼层'\n",
    "        df_processed.loc[infer_mask & (ratio >= 2/3), '楼层类别_inferred'] = '高楼层'\n",
    "        df_processed.loc[infer_mask & (current == total), '楼层类别_inferred'] = '顶层' # 顶层优先级高\n",
    "        df_processed.loc[infer_mask & (current == 1), '楼层类别_inferred'] = '底层'   # 底层优先级最高\n",
    "\n",
    "    # --- 步骤 5: 合并与填充 ---\n",
    "    # 合并类别：优先 explicit, 再 inferred\n",
    "    df_processed['楼层类别'] = df_processed['楼层类别_explicit'].fillna(df_processed['楼层类别_inferred'])\n",
    "\n",
    "    # 计算填充值 (基于训练集)\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    # 计算中位数，处理可能全是NaN的情况\n",
    "    median_total_floors_train = train_part['总楼层'].median()\n",
    "    if pd.isna(median_total_floors_train): median_total_floors_train = 6 # 合理备用值，例如6层\n",
    "\n",
    "    mode_floor_category_train = train_part['楼层类别'].mode()\n",
    "    fill_category_train = mode_floor_category_train[0] if not mode_floor_category_train.empty else '中楼层' # 备用值\n",
    "\n",
    "    # 填充 '总楼层'\n",
    "    original_na_total_floors = df_processed['总楼层'].isnull().sum()\n",
    "    if original_na_total_floors > 0:\n",
    "        df_processed['总楼层'].fillna(median_total_floors_train, inplace=True)\n",
    "        print(f\"填充了 '总楼层' 列的 {original_na_total_floors} 个缺失值 (使用训练集中位数 {median_total_floors_train:.0f})。\")\n",
    "\n",
    "    # 填充 '楼层类别'\n",
    "    original_na_category = df_processed['楼层类别'].isnull().sum()\n",
    "    if original_na_category > 0:\n",
    "        df_processed['楼层类别'].fillna(fill_category_train, inplace=True)\n",
    "        print(f\"填充了 '楼层类别' 列的 {original_na_category} 个缺失值 (使用训练集众数 '{fill_category_train}')。\")\n",
    "\n",
    "\n",
    "    # --- 步骤 6: 清理与编码 ---\n",
    "    # 删除原始列和临时列\n",
    "    cols_to_drop = [col, '当前楼层_temp', '楼层类别_explicit', '楼层类别_inferred']\n",
    "    df_processed = df_processed.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # 对最终的 '楼层类别' 进行独热编码\n",
    "    df_processed = pd.get_dummies(df_processed, columns=['楼层类别'], prefix='楼层', drop_first=False, dummy_na=False) # dummy_na=False 因为已填充\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# --- 如何调用 ---\n",
    "# 假设 df_price, df_rent 已经是合并后的数据框\n",
    "# n_train_price, n_train_rent 是对应的训练集行数\n",
    "\n",
    "# print(\"--- 处理 df_price ---\")\n",
    "df_price = parse_floor_robust(df_price, n_train_price, col='所在楼层')\n",
    "# print(\"\\n--- 处理 df_rent ---\")\n",
    "df_rent = parse_floor_robust(df_rent, n_train_rent, col='楼层')\n",
    "\n",
    "# --- 检查 df_rent 的结果 ---\n",
    "print(\"\\n--- df_rent 处理结果 ---\")\n",
    "print(\"总楼层 缺失值:\", df_rent['总楼层'].isnull().sum())\n",
    "floor_category_cols = [c for c in df_rent.columns if c.startswith('楼层_')]\n",
    "print(\"楼层类别列:\", floor_category_cols)\n",
    "display(df_rent[['总楼层'] + floor_category_cols].head())\n",
    "print(\"楼层类别列缺失值:\", df_rent[floor_category_cols].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1475634",
   "metadata": {},
   "source": [
    "#### d. 处理房屋朝向\n",
    "\n",
    "将其转化为 8 个 0-1 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8982f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_orientation(df,orientation_col):\n",
    "    \"\"\"\n",
    "    处理'房屋朝向'列，将其转换为多个二元特征列。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "\n",
    "    # 生成虚拟变量\n",
    "    orientation_dummies = df_processed[orientation_col].str.get_dummies(sep=' ')\n",
    "\n",
    "    # 添加前缀，避免与其他列名冲突\n",
    "    orientation_dummies = orientation_dummies.add_prefix('朝向_')\n",
    "\n",
    "    # 将新创建的虚拟变量列合并回主数据框\n",
    "    df_processed = pd.concat([df_processed, orientation_dummies], axis=1)\n",
    "\n",
    "    # 删除原始的 '房屋朝向' 列\n",
    "    df_processed = df_processed.drop(columns=[orientation_col], errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "df_price = process_orientation(df_price, '房屋朝向')\n",
    "df_rent = process_orientation(df_rent, '朝向')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b0f16",
   "metadata": {},
   "source": [
    "#### f. 批量处理剩余的虚拟变量部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33cd2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_structure(df,structure_col,fillna_value,prefix):\n",
    "    \"\"\"\n",
    "    处理列，将其转换为虚拟变量。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy() \n",
    "\n",
    "    # 填充缺失值\n",
    "    df_processed[structure_col].fillna(fillna_value, inplace=True)\n",
    "\n",
    "    # 转化为虚拟变量\n",
    "    structure_dummies = pd.get_dummies(df_processed[structure_col], prefix=prefix, dummy_na=False)\n",
    "\n",
    "    # 虚拟变量列合并回主数据框\n",
    "    df_processed = pd.concat([df_processed, structure_dummies], axis=1)\n",
    "\n",
    "    # 删除原始的列\n",
    "    df_processed = df_processed.drop(columns=[structure_col], errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# 房屋结构\n",
    "df_price = process_structure(df_price,structure_col = '建筑结构',fillna_value='其他结构',prefix='结构')\n",
    "df_price['结构_未知或其他'] = df_price['结构_未知结构'] | df_price.get('结构_其他结构', 0)\n",
    "df_price = df_price.drop(columns=['结构_未知结构', '结构_其他结构'], errors='ignore')\n",
    "\n",
    "for col in ['装修情况','产权所属','交易权属','年份','楼层类别']:\n",
    "    if col in df_price.columns:\n",
    "        df_price = process_structure(df_price,structure_col = col,fillna_value='其他',prefix=col)\n",
    "\n",
    "for col in ['付款方式','租赁方式','电梯','燃气','装修情况','年份']:\n",
    "    if col in df_rent.columns:\n",
    "        if col == '付款方式':\n",
    "            df_rent = process_structure(df_rent,structure_col = col,fillna_value='季付价',prefix=col)\n",
    "        elif col == '电梯':\n",
    "            df_rent = process_structure(df_rent,structure_col = col,fillna_value='有',prefix=col)\n",
    "        elif col == '燃气':\n",
    "            df_rent = process_structure(df_rent,structure_col = col,fillna_value='有',prefix=col)\n",
    "        elif col == '建筑结构':\n",
    "            df_rent = process_structure(df_rent,structure_col = '建筑结构',fillna_value='其他结构',prefix='结构')\n",
    "            df_rent['结构_未知或其他'] = df_rent['结构_未知结构'] | df_rent.get('结构_其他结构', 0)\n",
    "            df_rent = df_rent.drop(columns=['结构_未知结构', '结构_其他结构'], errors='ignore')\n",
    "        else:\n",
    "            df_rent = process_structure(df_rent,structure_col = col,fillna_value='其他',prefix=col)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e88b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_process = '房屋用途'\n",
    "\n",
    "# 仅使用训练集数据计算众数\n",
    "train_part = df_price.iloc[:n_train_price]\n",
    "if not train_part[col_to_process].mode().empty:\n",
    "    global_mode = train_part[col_to_process].mode()[0]\n",
    "else:\n",
    "    global_mode = '普通住宅' \n",
    "\n",
    "df_price[col_to_process].fillna(global_mode, inplace=True)\n",
    "\n",
    "# 合并稀有类别\n",
    "keep_categories = ['普通住宅', '商住两用', '别墅']\n",
    "\n",
    "# 将所有不在保留列表中的值都标记为 '其他'\n",
    "df_price[col_to_process] = df_price[col_to_process].apply(lambda x: x if x in keep_categories else '其他')\n",
    "\n",
    "# 复用 process_structure 函数\n",
    "df_price = process_structure(\n",
    "    df_price,\n",
    "    structure_col=col_to_process,\n",
    "    fillna_value='其他', \n",
    "    prefix='用途'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cabb9",
   "metadata": {},
   "source": [
    "### 2. 地理空间特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f4cd9",
   "metadata": {},
   "source": [
    "#### a. 分组填充梯户比例等缺失值\n",
    "\n",
    "对于“有无电梯”的缺失值使用按区域分组后的众数填充，如果明确无电梯，且梯户比用 0 填充，对于有电梯且梯户比例缺失的数据，使用组内的中位数填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1356fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_to_arabic(cn_num):\n",
    "    \"\"\"将常见的中文数字（包括'十','百'）转换为阿拉伯数字\"\"\"\n",
    "\n",
    "    cn_map = {'零': 0, '一': 1, '二': 2, '三': 3, '四': 4, '五': 5, '六': 6, '七': 7, '八': 8, '九': 9}\n",
    "    cn_unit = {'十': 10, '百': 100}\n",
    "    unit = 0\n",
    "    ldig = []\n",
    "    if cn_num.startswith('十'):\n",
    "         cn_num = '一' + cn_num\n",
    "    for cndig in reversed(cn_num):\n",
    "        if cndig in cn_unit:\n",
    "            unit = cn_unit.get(cndig)\n",
    "            if unit == 100:\n",
    "                ldig.append(unit)\n",
    "        elif cndig in cn_map:\n",
    "            dig = cn_map.get(cndig)\n",
    "            if unit:\n",
    "                dig *= unit\n",
    "                unit = 0\n",
    "            ldig.append(dig)\n",
    "        else:\n",
    "            return np.nan\n",
    "    if not ldig:\n",
    "        return np.nan\n",
    "    val, tmp = 0, 0\n",
    "    for x in reversed(ldig):\n",
    "         if x == 100:\n",
    "             tmp *= x\n",
    "             val += tmp\n",
    "             tmp = 0\n",
    "         elif x == 10:\n",
    "             if tmp == 0: tmp = 1\n",
    "             tmp *= x\n",
    "         else:\n",
    "             tmp += x\n",
    "    val += tmp\n",
    "    return val\n",
    "\n",
    "def parse_elevator_ratio_strict(text):\n",
    "    \"\"\"\n",
    "    严格解析 'X梯Y户' 格式的文本, X和Y是阿拉伯数字。\n",
    "    返回 (梯数, 户数) 或 (NaN, NaN)。\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '' or text == '未知':\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    match = re.fullmatch(r'(\\d+|[一二三四五六七八九十百零]+)梯(\\d+|[一二三四五六七八九十百零]+)户', str(text))\n",
    "    if match:\n",
    "        t_str, h_str = match.groups()\n",
    "        t = chinese_to_arabic(t_str)\n",
    "        h = chinese_to_arabic(h_str)\n",
    "        if pd.notna(t) and pd.notna(h) and t > 0 and h > 0:\n",
    "            return int(t), int(h)\n",
    "    return np.nan, np.nan\n",
    "\n",
    "def process_elevator_ratio_grouped_median_impute(df, n_train, grouping_col='板块'):\n",
    "    \"\"\"\n",
    "    处理电梯特征：\n",
    "    1. 确定 '有电梯' (0/1) 状态，优先基于 '配备电梯' 并用分组众数填充。\n",
    "    2. 计算 '梯户比'。\n",
    "    3. 对 '有电梯'=1 但 '梯户比' 缺失的情况，用分组中位数填充。\n",
    "    4. '有电梯'=0 时 '梯户比' 为 0。\n",
    "    5. 最终保留 '有电梯' 和 '梯户比'。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    ratio_col = '梯户比例'\n",
    "    has_elevator_col = '配备电梯'\n",
    "\n",
    "    # 处理 '有电梯' \n",
    "    if has_elevator_col in df_processed.columns:\n",
    "        df_processed['有电梯_numeric_temp'] = df_processed[has_elevator_col].map({'有': 1, '无': 0})\n",
    "\n",
    "        # 计算每组众数（仅使用非NaN值）\n",
    "        mode_map_binary = df_processed.loc[df_processed['有电梯_numeric_temp'].notna()].groupby(grouping_col)['有电梯_numeric_temp'].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan) # 改为nan以防空组\n",
    "        \n",
    "        nan_mask_binary = df_processed['有电梯_numeric_temp'].isnull()\n",
    "        fill_values_binary = df_processed.loc[nan_mask_binary, grouping_col].map(mode_map_binary)\n",
    "        df_processed['有电梯'] = df_processed['有电梯_numeric_temp'].fillna(fill_values_binary)\n",
    "        \n",
    "        # 全局回退\n",
    "        if df_processed['有电梯'].isnull().any():\n",
    "            # 优先使用训练集的众数\n",
    "            train_mode_binary = df_processed.iloc[:n_train]['有电梯_numeric_temp'].mode()\n",
    "            global_fallback_binary = train_mode_binary[0] if not train_mode_binary.empty else 0 # 默认0(无电梯)\n",
    "            df_processed['有电梯'].fillna(global_fallback_binary, inplace=True)\n",
    "            \n",
    "        df_processed['有电梯'] = df_processed['有电梯'].astype(int)\n",
    "        df_processed = df_processed.drop(columns=['有电梯_numeric_temp'], errors='ignore')\n",
    "\n",
    "    # 解析 '梯户比例'\n",
    "    temp_elevators = pd.Series(np.nan, index=df_processed.index)\n",
    "    temp_households = pd.Series(np.nan, index=df_processed.index)\n",
    "    \n",
    "    if ratio_col in df_processed.columns:\n",
    "        parsed_ratios = df_processed[ratio_col].apply(parse_elevator_ratio_strict)\n",
    "        temp_elevators = parsed_ratios.apply(lambda x: x[0])\n",
    "        temp_households = parsed_ratios.apply(lambda x: x[1])\n",
    "\n",
    "    # 计算 '梯户比'\n",
    "    df_processed['梯户比'] = np.nan\n",
    "    df_processed.loc[df_processed['有电梯'] == 0, '梯户比'] = 0.0\n",
    "\n",
    "    # 计算 '有电梯' 且解析成功的行的梯户比\n",
    "    mask_calc_ratio = (df_processed['有电梯'] == 1) & pd.notna(temp_households) & (temp_households > 0) & pd.notna(temp_elevators)\n",
    "    df_processed.loc[mask_calc_ratio, '梯户比'] = temp_elevators[mask_calc_ratio] / temp_households[mask_calc_ratio]\n",
    "\n",
    "    # 填充 '有电梯' 但 '梯户比' 缺失的值 (关键修正)\n",
    "    fill_mask_ratio = (df_processed['有电梯'] == 1) & (df_processed['梯户比'].isna())\n",
    "\n",
    "    if fill_mask_ratio.any():\n",
    "        \n",
    "        # 计算每个分组内，有电梯且梯户比有效的房产的梯户比中位数\n",
    "        median_map_ratio = df_processed.loc[(df_processed['有电梯'] == 1) & df_processed['梯户比'].notna()].groupby(grouping_col)['梯户比'].median()\n",
    "\n",
    "        # 仅对需要填充的行应用map\n",
    "        fill_values_ratio = df_processed.loc[fill_mask_ratio, grouping_col].map(median_map_ratio)\n",
    "        df_processed.loc[fill_mask_ratio, '梯户比'] = fill_values_ratio\n",
    "\n",
    "        still_nan_mask = fill_mask_ratio & df_processed['梯户比'].isnull()\n",
    "        \n",
    "        if still_nan_mask.any():\n",
    "            \n",
    "            # 使用训练集全局中位数作为回退\n",
    "            train_part = df_processed.iloc[:n_train]\n",
    "            global_median_ratio = train_part.loc[((train_part['有电梯'] == 1) \n",
    "                                                  & train_part['梯户比'].notna()), '梯户比'].median()\n",
    "            \n",
    "            # 仅填充那些仍然为 NaN 且 '有电梯'=1 的行\n",
    "            df_processed.loc[still_nan_mask, '梯户比'] = global_median_ratio\n",
    "\n",
    "    # 清理\n",
    "    cols_to_drop_final = [ratio_col, has_elevator_col]\n",
    "    df_processed = df_processed.drop(columns=cols_to_drop_final, errors='ignore')\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "df_price = process_elevator_ratio_grouped_median_impute(df_price, n_train_price, grouping_col='板块')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865f34a",
   "metadata": {},
   "source": [
    "#### b. 分组填充电、水、暖、物业费、停车位费等缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af0b4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_utility_impute_and_encode(df, n_train, col_to_process, group_col_l1='板块', group_col_l2='区域'):\n",
    "    \"\"\"\n",
    "    处理工具类特征 (如供水、供暖、供电):\n",
    "    1. 仅使用训练集 (n_train) 数据计算填充地图 (mode)。\n",
    "    2. 优先使用 L1 (板块) 级别的众数进行填充。\n",
    "    3. 回退使用 L2 (区域) 级别的众数进行填充。\n",
    "    4. 使用全局众数（训练集）作为最终回退。\n",
    "    5. 将处理干净的列转换为哑变量。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 仅使用训练集创建填充地图\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "\n",
    "    # L1 地图: '板块' -> 众数\n",
    "    mode_map_l1 = train_part.groupby(group_col_l1)[col_to_process].agg(\n",
    "        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
    "    )\n",
    "\n",
    "    # L2 地图: '区域' -> 众数\n",
    "    mode_map_l2 = train_part.groupby(group_col_l2)[col_to_process].agg(\n",
    "        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
    "    )\n",
    "    \n",
    "    # L3 (Global) 回退值\n",
    "    global_mode_series = train_part[col_to_process].mode()\n",
    "    \n",
    "    # 设置一个硬回退值 '未知'\n",
    "    fill_global = global_mode_series[0] if not global_mode_series.empty else '未知'\n",
    "    \n",
    "    # 应用填充\n",
    "    nan_mask = df_processed[col_to_process].isnull()\n",
    "\n",
    "    # L1 填充\n",
    "    fill_values_l1 = df_processed.loc[nan_mask, group_col_l1].map(mode_map_l1)\n",
    "    df_processed[col_to_process].fillna(fill_values_l1, inplace=True)\n",
    "\n",
    "    # L2 填充\n",
    "    nan_mask_l2 = df_processed[col_to_process].isnull()\n",
    "    if nan_mask_l2.any():\n",
    "        # print(f\"... L1 填充后剩余 {nan_mask_l2.sum()} 个缺失值。使用 L2(区域) 填充...\")\n",
    "        fill_values_l2 = df_processed.loc[nan_mask_l2, group_col_l2].map(mode_map_l2)\n",
    "        df_processed[col_to_process].fillna(fill_values_l2, inplace=True)\n",
    "    \n",
    "    # L3 (Global) 填充\n",
    "    nan_mask_l3 = df_processed[col_to_process].isnull()\n",
    "    if nan_mask_l3.any():\n",
    "        df_processed[col_to_process].fillna(fill_global, inplace=True)\n",
    "\n",
    "    # 转化为哑变量\n",
    "    df_processed = pd.get_dummies(df_processed, \n",
    "                                columns=[col_to_process], \n",
    "                                prefix=col_to_process, \n",
    "                                dummy_na=False) # 我们已手动填充所有 NaNs\n",
    "    \n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# 处理供水\n",
    "df_price = process_utility_impute_and_encode(\n",
    "    df_price, \n",
    "    n_train_price,\n",
    "    col_to_process='供水',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "\n",
    "# 处理供暖\n",
    "df_price = process_utility_impute_and_encode(\n",
    "    df_price, \n",
    "    n_train_price,\n",
    "    col_to_process='供暖',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "\n",
    "# 处理供电\n",
    "df_price = process_utility_impute_and_encode(\n",
    "    df_price, \n",
    "    n_train_price,\n",
    "    col_to_process='供电',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "\n",
    "df_rent = process_utility_impute_and_encode(\n",
    "    df_rent, \n",
    "    n_train_rent,\n",
    "    col_to_process='用电',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")\n",
    "\n",
    "df_rent = process_utility_impute_and_encode(\n",
    "    df_rent, \n",
    "    n_train_rent,\n",
    "    col_to_process='用水',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")\n",
    "\n",
    "df_rent = process_utility_impute_and_encode(\n",
    "    df_rent, \n",
    "    n_train_rent,\n",
    "    col_to_process='采暖',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76992ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fee_value(text):\n",
    "    \"\"\"\n",
    "    从字符串中提取费用数值，支持区间和单位：\n",
    "    1. \"3.5-4.5元/月\" -> 4.0 (区间取中点)\n",
    "    2. \"2.5元/立方米\" -> 2.5 (单个数字)\n",
    "    3. 5 或 \"5\" -> 5.0 (纯数字)\n",
    "    4. \"未知\", \"免费\", \"包含在物业费中\" -> NaN\n",
    "    \"\"\"\n",
    "    # 辅助函数：尝试将捕获的组转换为浮点数\n",
    "    def to_float(group):\n",
    "        try:\n",
    "            return float(group)\n",
    "        except (ValueError, TypeError):\n",
    "            return np.nan\n",
    "\n",
    "    # 处理非字符串\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    if isinstance(text, (int, float)):\n",
    "        return float(text)\n",
    "\n",
    "    # 处理字符串\n",
    "    s = str(text).strip()\n",
    "    if s == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    # 尝试匹配区间 (e.g., \"3.5-4.5\")\n",
    "    range_match = re.search(r'(\\d+\\.?\\d*)\\s*[-—–]\\s*(\\d+\\.?\\d*)', s)\n",
    "    if range_match:\n",
    "        num1 = to_float(range_match.group(1))\n",
    "        num2 = to_float(range_match.group(2))\n",
    "        \n",
    "        if pd.notna(num1) and pd.notna(num2):\n",
    "            return (num1 + num2) / 2.0\n",
    "\n",
    "    # 4. 尝试匹配单个数字\n",
    "    single_match = re.search(r'(\\d+\\.?\\d*)', s)\n",
    "    if single_match:\n",
    "        num = to_float(single_match.group(1))\n",
    "        if pd.notna(num):\n",
    "            return num\n",
    "            \n",
    "    return np.nan\n",
    "\n",
    "def process_numerical_impute(df, n_train, cols_to_process, \n",
    "                             group_col_l1='板块', group_col_l2='区域', \n",
    "                             final_fallback_value=0.0):\n",
    "    \"\"\"\n",
    "    (更新版本)\n",
    "    处理数值型费用特征 (如煤气费、水费、取暖费):\n",
    "    1. 使用 extract_fee_value 清洗列，提取数字（支持区间）。\n",
    "    2. 仅使用训练集 (n_train) 数据计算填充地图 (median)。\n",
    "    3. 优先使用 L1 (板块) 级别的中位数进行填充。\n",
    "    4. 回退使用 L2 (区域) 级别的中位数进行填充。\n",
    "    5. 使用全局中位数（训练集）作为最终回退。\n",
    "    6. 使用 0.0 作为所有回退的最终值。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 循环处理传入的每一列\n",
    "    for col in cols_to_process:\n",
    "\n",
    "        # 清洗并转换为数值\n",
    "        # 使用新的、更强大的辅助函数\n",
    "        df_processed[col] = df_processed[col].apply(extract_fee_value)\n",
    "\n",
    "        if col == '绿 化 率':\n",
    "            # 找出所有 > 100 的行 (例如 10500)\n",
    "            cap_mask = (df_processed[col] > 100)\n",
    "            rows_affected = cap_mask.sum()\n",
    "            if rows_affected > 0:\n",
    "                df_processed.loc[cap_mask, col] = np.nan\n",
    "\n",
    "        if col == '容 积 率':\n",
    "            # 容积率 > 30 (约等于99.99%分位数) \n",
    "            cap_mask = (df_processed[col] > 30) \n",
    "            rows_affected = cap_mask.sum()\n",
    "            if rows_affected > 0:\n",
    "                df_processed.loc[cap_mask, col] = np.nan\n",
    "        \n",
    "        # 仅使用训练集创建填充地图\n",
    "        train_part = df_processed.iloc[:n_train]\n",
    "        \n",
    "        # 计算中位数\n",
    "        valid_train_data = train_part.loc[train_part[col] > 0]\n",
    "        \n",
    "        if valid_train_data.empty:\n",
    "            fill_global = final_fallback_value\n",
    "            median_map_l1 = pd.Series(dtype=float) # 空地图\n",
    "            median_map_l2 = pd.Series(dtype=float) # 空地图\n",
    "        else:\n",
    "            # L1 地图: '板块' -> 中位数\n",
    "            median_map_l1 = valid_train_data.groupby(group_col_l1)[col].median()\n",
    "            # L2 地图: '区域' -> 中位数\n",
    "            median_map_l2 = valid_train_data.groupby(group_col_l2)[col].median()\n",
    "            # L3 (Global) 回退值\n",
    "            global_median = valid_train_data[col].median()\n",
    "            fill_global = global_median if pd.notna(global_median) else final_fallback_value\n",
    "\n",
    "        # 应用填充\n",
    "        \n",
    "        # L1 填充: 使用 '板块' 中位数\n",
    "        nan_mask_l1 = df_processed[col].isnull()\n",
    "        fill_values_l1 = df_processed.loc[nan_mask_l1, group_col_l1].map(median_map_l1)\n",
    "        df_processed[col].fillna(fill_values_l1, inplace=True)\n",
    "\n",
    "        # L2 填充: (只填充 L1 没填上的)\n",
    "        nan_mask_l2 = df_processed[col].isnull()\n",
    "        if nan_mask_l2.any():\n",
    "            fill_values_l2 = df_processed.loc[nan_mask_l2, group_col_l2].map(median_map_l2)\n",
    "            df_processed[col].fillna(fill_values_l2, inplace=True)\n",
    "        \n",
    "        # L3 (Global) 填充: (填充 L2 没填上的)\n",
    "        nan_mask_l3 = df_processed[col].isnull()\n",
    "        if nan_mask_l3.any():\n",
    "            df_processed[col].fillna(fill_global, inplace=True)\n",
    "            \n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# 处理燃气费、水费、停车费用\n",
    "fee_columns = ['燃气费', '供热费', '停车费用','物 业 费', '绿 化 率', '容 积 率']\n",
    "df_price = process_numerical_impute(\n",
    "    df_price, \n",
    "    n_train_price,\n",
    "    cols_to_process=fee_columns,\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域',\n",
    "    final_fallback_value=0.0\n",
    ")\n",
    "\n",
    "\n",
    "df_price.loc[(df_price['供暖_自采暖'] == True) |(df_price['供暖_无供暖'] == True) | (df_price['供暖_自采暖/无供暖'] == False), '供热费'] = 0\n",
    "\n",
    "\n",
    "\n",
    "fee_columns_rent = ['燃气费','物 业 费', '绿 化 率', '容 积 率','供热费','停车费用']\n",
    "df_rent = process_numerical_impute(\n",
    "    df_rent, \n",
    "    n_train_rent,\n",
    "    cols_to_process=fee_columns_rent,\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县',\n",
    "    final_fallback_value=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa4f50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... L1/L2 填充后剩余 23243 个缺失值。使用全局回退 '普通住宅'...\n",
      "... L1/L2 填充后剩余 20827 个缺失值。使用全局回退 '普通住宅'...\n",
      "... L1/L2 填充后剩余 23243 个缺失值。使用全局回退 '商品房'...\n",
      "... L1/L2 填充后剩余 20827 个缺失值。使用全局回退 '商品房'...\n",
      "... L1/L2 填充后剩余 23283 个缺失值。使用全局回退 '塔楼'...\n",
      "... L1/L2 填充后剩余 21902 个缺失值。使用全局回退 '塔楼'...\n",
      "... L1/L2 填充后剩余 3010 个缺失值。使用全局回退 '洗衣机、空调、衣柜、电视、冰箱、热水器、床、天然气'...\n"
     ]
    }
   ],
   "source": [
    "def process_multi_hot_impute_and_encode(df, n_train, \n",
    "                                        col_to_process,      # 原始列名 (无空格)\n",
    "                                        keywords_list,       # 要提取的关键词列表\n",
    "                                        prefix,              # 新列的前缀\n",
    "                                        group_col_l1='板块', \n",
    "                                        group_col_l2='区域'):\n",
    "    \"\"\"\n",
    "    (通用函数)\n",
    "    处理具有组合关键词的类别型特征 (如 物业类别, 产权描述):\n",
    "    1. 使用 L1(板块)/L2(区域)/全局 众数填充 NaN。\n",
    "    2. 执行多标签独热编码 (Multi-hot)，根据 keywords_list 拆分属性。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    col = col_to_process \n",
    "            \n",
    "    # --- 步骤 1: 填充缺失值 (L1/L2/L3 众数) ---\n",
    "    \n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    \n",
    "    mode_map_l1 = train_part.groupby(group_col_l1)[col].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "    mode_map_l2 = train_part.groupby(group_col_l2)[col].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "    \n",
    "    # 全局回退值 (使用列表中的第一个关键词，或 '其他')\n",
    "    global_mode_series = train_part[col].mode()\n",
    "    fallback = keywords_list[0] if keywords_list else '其他'\n",
    "    fill_global = global_mode_series[0] if not global_mode_series.empty else fallback\n",
    "    \n",
    "    # 应用填充\n",
    "    nan_mask = df_processed[col].isnull()\n",
    "    if nan_mask.any():\n",
    "        \n",
    "        fill_values_l1 = df_processed.loc[nan_mask, group_col_l1].map(mode_map_l1)\n",
    "        df_processed[col].fillna(fill_values_l1, inplace=True)\n",
    "        \n",
    "        nan_mask_l2 = df_processed[col].isnull()\n",
    "        if nan_mask_l2.any():\n",
    "            fill_values_l2 = df_processed.loc[nan_mask_l2, group_col_l2].map(mode_map_l2)\n",
    "            df_processed[col].fillna(fill_values_l2, inplace=True)\n",
    "            \n",
    "        nan_mask_l3 = df_processed[col].isnull()\n",
    "        if nan_mask_l3.any():\n",
    "            print(f\"... L1/L2 填充后剩余 {nan_mask_l3.sum()} 个缺失值。使用全局回退 '{fill_global}'...\")\n",
    "            df_processed[col].fillna(fill_global, inplace=True)\n",
    "            \n",
    "\n",
    "    # --- 步骤 2: 拆分与多标签独热编码 ---\n",
    "    \n",
    "    df_processed[col] = df_processed[col].astype(str)\n",
    "\n",
    "    new_cols = []\n",
    "    for keyword in keywords_list:\n",
    "        # e.g., \"物业_is_住宅\"\n",
    "        col_name = f\"{prefix}_is_{keyword}\" \n",
    "        df_processed[col_name] = df_processed[col].str.contains(keyword).astype(int)\n",
    "        new_cols.append(col_name)\n",
    "\n",
    "    # --- 步骤 3: 创建 '其他' 列 ---\n",
    "    other_col_name = f\"{prefix}_is_其他\"\n",
    "    is_known_type = df_processed[new_cols].sum(axis=1)\n",
    "    df_processed[other_col_name] = (is_known_type == 0).astype(int)\n",
    "    \n",
    "    # --- 步骤 4: 清理 ---\n",
    "    df_processed = df_processed.drop(columns=[col], errors='ignore')\n",
    "\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# 1. 定义 '物业类别' 的关键词 (基于您的透视)\n",
    "keywords_type = ['住宅', '商业', '底商', '别墅', '办公', '公寓']\n",
    "\n",
    "# 2. 调用通用函数\n",
    "df_price = process_multi_hot_impute_and_encode(\n",
    "    df_price,\n",
    "    n_train_price,\n",
    "    col_to_process='物业类别',\n",
    "    keywords_list=keywords_type,\n",
    "    prefix='物业',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "df_rent = process_multi_hot_impute_and_encode(\n",
    "    df_rent,\n",
    "    n_train_rent,\n",
    "    col_to_process='物业类别',\n",
    "    keywords_list=keywords_type,\n",
    "    prefix='物业',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")\n",
    "# 1. 定义 '产权描述' 的关键词\n",
    "keywords_rights = ['商品房', '使用权', '已购公房', '私产']\n",
    "\n",
    "# 复用函数\n",
    "df_price = process_multi_hot_impute_and_encode(\n",
    "    df_price,\n",
    "    n_train_price,\n",
    "    col_to_process='产权描述',\n",
    "    keywords_list=keywords_rights,\n",
    "    prefix='产权',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "df_rent = process_multi_hot_impute_and_encode(\n",
    "    df_rent,\n",
    "    n_train_rent,\n",
    "    col_to_process='产权描述', \n",
    "    keywords_list=keywords_rights,\n",
    "    prefix='建构',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")\n",
    "\n",
    "keywords_structure = ['板楼', '塔楼', '塔板结合', '平房']\n",
    "df_price = process_multi_hot_impute_and_encode(\n",
    "    df_price,\n",
    "    n_train_price,\n",
    "    col_to_process='建筑结构_comm', \n",
    "    keywords_list=keywords_structure,\n",
    "    prefix='建构',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "df_rent = process_multi_hot_impute_and_encode(\n",
    "    df_rent,\n",
    "    n_train_rent,\n",
    "    col_to_process='建筑结构', \n",
    "    keywords_list=keywords_structure,\n",
    "    prefix='建构',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")\n",
    "\n",
    "keywords_furniture = ['洗衣机', '冰箱', '空调', '衣柜', '热水器', '电视', '天然气']\n",
    "df_rent = process_multi_hot_impute_and_encode(\n",
    "    df_rent,\n",
    "    n_train_rent,\n",
    "    col_to_process='配套设施',\n",
    "    keywords_list=keywords_furniture,\n",
    "    prefix='物业',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec1224",
   "metadata": {},
   "source": [
    "#### c. 填充环线位置等缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f667f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ring_road_impute_and_encode(df, n_train, col_to_process='环线', group_col_l1='板块', group_col_l2='区域'):\n",
    "    \"\"\"\n",
    "    处理环线特征：\n",
    "    1. 仅使用训练集 (n_train) 数据计算填充地图 (mode)。\n",
    "    2. 优先使用 ('区域', '板块') 级别的众数进行填充。\n",
    "    3. 回退使用 '城市' 级别的众数进行填充。\n",
    "    4. 将所有剩余的 NaN (包括结构性缺失) 填充为 \"无环线\"。\n",
    "    5. 对处理干净的 '环线' 列进行独热编码。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    \n",
    "    # 地图: ('区域', '板块') -> 环线众数\n",
    "    mode_map_plate = train_part.groupby([group_col_l2, group_col_l1])[col_to_process].agg(\n",
    "        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
    "    )\n",
    "\n",
    "    # 地图: ('区域') -> 环线众数\n",
    "    mode_map_city = train_part.groupby(group_col_l2)[col_to_process].agg(\n",
    "        lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
    "    )\n",
    "\n",
    "    # 应用填充 (应用到整个数据集)\n",
    "    \n",
    "    mapped_values_l1 = df_processed.set_index([group_col_l2, group_col_l1]).index.map(mode_map_plate)\n",
    "    fill_values_plate = pd.Series(mapped_values_l1, index=df_processed.index)\n",
    "\n",
    "    fill_values_city = df_processed[group_col_l2].map(mode_map_city)\n",
    "\n",
    "    df_processed[col_to_process].fillna(fill_values_plate, inplace=True)\n",
    "    df_processed[col_to_process].fillna(fill_values_city, inplace=True)\n",
    "\n",
    "    # 处理结构性缺失    \n",
    "    fill_label = \"无环线\"\n",
    "    df_processed[col_to_process].fillna(fill_label, inplace=True)\n",
    "\n",
    "    # 独热编码\n",
    "    df_processed = pd.get_dummies(df_processed, \n",
    "                                columns=[col_to_process], \n",
    "                                prefix=col_to_process, \n",
    "                                dummy_na=False)\n",
    "    \n",
    "\n",
    "    # 打印新生成的列以供检查\n",
    "    new_cols = [col for col in df_processed.columns if col.startswith(f\"{col_to_process}_\")]\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "df_price = process_ring_road_impute_and_encode(\n",
    "     df_price, \n",
    "     n_train_price, \n",
    "     col_to_process='环线', \n",
    "     group_col_l1='板块', \n",
    "     group_col_l2='区域'\n",
    ")\n",
    "\n",
    "community_cols = ['房屋总数', '楼栋总数', '停车位']\n",
    "df_price = process_numerical_impute(\n",
    "    df_price, \n",
    "    n_train_price,\n",
    "    cols_to_process=community_cols,\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域',\n",
    "    final_fallback_value=0.0\n",
    ")\n",
    "\n",
    "df_rent = process_ring_road_impute_and_encode(\n",
    "     df_rent, \n",
    "     n_train_rent, \n",
    "     col_to_process='环线位置', \n",
    "     group_col_l1='板块', \n",
    "     group_col_l2='区县'\n",
    ")\n",
    "\n",
    "community_cols = ['房屋总数', '楼栋总数', '停车位']\n",
    "df_price = process_numerical_impute(\n",
    "    df_price, \n",
    "    n_train_price,\n",
    "    cols_to_process=community_cols,\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域',\n",
    "    final_fallback_value=0.0\n",
    ")\n",
    "df_rent = process_numerical_impute(\n",
    "    df_rent, \n",
    "    n_train_rent,\n",
    "    cols_to_process=community_cols,\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县',\n",
    "    final_fallback_value=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48885d",
   "metadata": {},
   "source": [
    "### 3. 提取时间相关数据\n",
    "\n",
    "#### a. 提取交易时间的年代和月份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9dc736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_and_fill_age_col(df, time_col='交易时间', last_time_col='上次交易', age_col='房屋年限'):\n",
    "    \"\"\"\n",
    "    处理时间特征并填充 '房屋年限' 列，然后进行独热编码：\n",
    "    1. 提取 '交易时间' 的年份和月份。\n",
    "    2. 对 'age_col' (房屋年限) 中的 NaN 值进行填充：\n",
    "       a. 如果 '上次交易' 有效，根据 '交易时间'-'上次交易' 的间隔计算并填充 \n",
    "          '未满两年', '满两年', '满五年'。\n",
    "       b. 如果 '上次交易' 无效 (NaN)，填充为 '其他'。\n",
    "    3. 对填充完毕的 'age_col' 进行独热编码。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 统一日期格式：将 '/' 替换为 '-'，以便 pandas 能正确解析\n",
    "    # 例如：'2024/11/28' -> '2024-11-28'\n",
    "    df_processed[time_col] = df_processed[time_col].astype(str).str.replace('/', '-')\n",
    "    \n",
    "    # 提取年份和月份\n",
    "    t_trans = pd.to_datetime(df_processed[time_col], errors='coerce')\n",
    "    \n",
    "    # 如果存在 last_time_col，也需要统一格式\n",
    "    if last_time_col in df_processed.columns:\n",
    "        df_processed[last_time_col] = df_processed[last_time_col].astype(str).str.replace('/', '-')\n",
    "        t_last = pd.to_datetime(df_processed[last_time_col], errors='coerce')\n",
    "    else:\n",
    "        t_last = None\n",
    "\n",
    "    # 提取年份和月份\n",
    "    df_processed['交易年份'] = t_trans.dt.year\n",
    "    df_processed['交易月份'] = t_trans.dt.month\n",
    "\n",
    "    # 检查是否有解析失败的情况\n",
    "    failed_count = t_trans.isna().sum()\n",
    "    if failed_count > 0:\n",
    "        print(f\"警告: 有 {failed_count} 条交易时间数据无法解析\")\n",
    "\n",
    "    if last_time_col not in df_processed.columns or t_last is None:\n",
    "        # 租赁数据集没有 '上次交易' 列，直接返回\n",
    "        df_processed = df_processed.drop(columns=[time_col], errors='ignore')\n",
    "        return df_processed\n",
    "\n",
    "    # 找出需要填充的行\n",
    "    mask_calc = (df_processed[age_col].isna()) & (t_trans.notna()) & (t_last.notna())\n",
    "    \n",
    "    # 找出无法填充的行：\n",
    "    mask_fill_other = (df_processed[age_col].isna()) & (~mask_calc)\n",
    "    \n",
    "    # 执行填充\n",
    "    # 根据计算结果填充\n",
    "    if mask_calc.any():\n",
    "        \n",
    "        # 1. 计算 \"满X年\" 的布尔掩码\n",
    "        # [mask_calc] 确保只对相关行进行日期计算，提高效率\n",
    "        two_years_later = t_last[mask_calc] + pd.DateOffset(years=2)\n",
    "        five_years_later = t_last[mask_calc] + pd.DateOffset(years=5)\n",
    "        \n",
    "        t_trans_masked = t_trans[mask_calc] # 确保索引对齐\n",
    "        \n",
    "        is_g5 = (t_trans_masked >= five_years_later)\n",
    "        is_g2 = (t_trans_masked >= two_years_later) & (~is_g5)\n",
    "        is_u2 = (~is_g2) & (~is_g5) # 默认是 \"未满两年\"\n",
    "        \n",
    "        # 获取需要操作的行的实际索引\n",
    "        idx_u2 = is_u2[is_u2].index\n",
    "        idx_g2 = is_g2[is_g2].index\n",
    "        idx_g5 = is_g5[is_g5].index\n",
    "        \n",
    "        # 执行 .loc 填充\n",
    "        df_processed.loc[idx_u2, age_col] = \"未满两年\"\n",
    "        df_processed.loc[idx_g2, age_col] = \"满两年\"\n",
    "        df_processed.loc[idx_g5, age_col] = \"满五年\"\n",
    "\n",
    "    # 填充 \"其他\"\n",
    "    if mask_fill_other.any():\n",
    "\n",
    "        df_processed.loc[mask_fill_other, age_col] = \"其他\"\n",
    "\n",
    "    df_processed = pd.get_dummies(df_processed, \n",
    "                                columns=[age_col], \n",
    "                                prefix='年限', \n",
    "                                dummy_na=False) \n",
    "\n",
    "    # 删除原始时间列\n",
    "    df_processed = df_processed.drop(columns=[time_col, last_time_col], errors='ignore')\n",
    "\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "df_price = process_time_and_fill_age_col(\n",
    "    df_price,\n",
    "    time_col='交易时间',\n",
    "    last_time_col='上次交易',\n",
    "    age_col='房屋年限'\n",
    ")\n",
    "\n",
    "# 处理 df_rent 的交易时间（df_rent 没有 '上次交易' 和 '房屋年限' 列）\n",
    "df_rent = process_time_and_fill_age_col(\n",
    "    df_rent,\n",
    "    time_col='交易时间',\n",
    "    last_time_col='上次交易',  # 不存在，函数会处理\n",
    "    age_col='房屋年限'  # 不存在，函数会处理\n",
    ")\n",
    "\n",
    "# 交易月份独热编码\n",
    "df_price = process_structure(df_price, structure_col='交易月份', fillna_value='', prefix='交易月份')\n",
    "df_rent = process_structure(df_rent, structure_col='交易月份', fillna_value='', prefix='交易月份')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a13a2b",
   "metadata": {},
   "source": [
    "#### b. 提取建筑年代（房龄）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6534ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_building_year(text):\n",
    "    \"\"\"\n",
    "    解析建筑年代：\n",
    "    1. \"1955-2000年\" -> 1977.5 (区间取中点)\n",
    "    2. \"2005年\" -> 2005.0 (单个数字)\n",
    "    3. \"未知\" -> NaN\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    \n",
    "    s = str(text).strip()\n",
    "\n",
    "    # 尝试匹配区间 \n",
    "    range_match = re.search(r'(\\d{4})\\s*[-—–]\\s*(\\d{4})', s)\n",
    "    if range_match:\n",
    "        try:\n",
    "            year1 = float(range_match.group(1))\n",
    "            year2 = float(range_match.group(2))\n",
    "            return (year1 + year2) / 2.0\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # 试匹配单个4位数年份\n",
    "    single_match = re.search(r'(\\d{4})', s)\n",
    "    if single_match:\n",
    "        try:\n",
    "            return float(single_match.group(1))\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "            \n",
    "    # 匹配失败\n",
    "    return np.nan\n",
    "\n",
    "def process_building_age(df, n_train, col='建筑年代', \n",
    "                         trans_year_col='交易年份',\n",
    "                         group_col_l1='板块', group_col_l2='区域'):\n",
    "    \"\"\"\n",
    "    处理 '建筑年代' 特征：\n",
    "    1. 使用 parse_building_year 解析原始列。\n",
    "    2. 计算 '房龄' = '交易年份' - '解析后的建筑年代'。\n",
    "    3. 使用 L1/L2/L3 中位数填充 '房龄' 的缺失值。\n",
    "    4. 清理异常值 (如负房龄)。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 解析建筑年代\n",
    "    parsed_year_col = col + '_parsed'\n",
    "    df_processed[parsed_year_col] = df_processed[col].apply(parse_building_year)\n",
    "    \n",
    "    # 计算 '房龄'\n",
    "    age_col = '房龄'\n",
    "    df_processed[age_col] = df_processed[trans_year_col] - df_processed[parsed_year_col]\n",
    "\n",
    "    nan_count = df_processed[age_col].isnull().sum()\n",
    "\n",
    "\n",
    "    # 填充 '房龄' 的缺失值 (L1/L2/L3 中位数)\n",
    "    train_part = df_processed.iloc[:n_train]\n",
    "    valid_train_age = train_part.loc[train_part[age_col] >= 0]\n",
    "    \n",
    "    median_map_l1 = valid_train_age.groupby(group_col_l1)[age_col].median()\n",
    "    median_map_l2 = valid_train_age.groupby(group_col_l2)[age_col].median()\n",
    "    global_median = valid_train_age[age_col].median()\n",
    "    fill_global = global_median if pd.notna(global_median) else 20\n",
    "    \n",
    "    # L1 填充\n",
    "    nan_mask_l1 = df_processed[age_col].isnull()\n",
    "    fill_values_l1 = df_processed.loc[nan_mask_l1, group_col_l1].map(median_map_l1)\n",
    "    df_processed[age_col].fillna(fill_values_l1, inplace=True)\n",
    "\n",
    "    # L2 填充\n",
    "    nan_mask_l2 = df_processed[age_col].isnull()\n",
    "    if nan_mask_l2.any():\n",
    "        fill_values_l2 = df_processed.loc[nan_mask_l2, group_col_l2].map(median_map_l2)\n",
    "        df_processed[age_col].fillna(fill_values_l2, inplace=True)\n",
    "    \n",
    "    # L3 (Global) 填充\n",
    "    nan_mask_l3 = df_processed[age_col].isnull()\n",
    "    if nan_mask_l3.any():\n",
    "        df_processed[age_col].fillna(fill_global, inplace=True)\n",
    "\n",
    "    # 清理异常值\n",
    "    negative_age_count = (df_processed[age_col] < 0).sum()\n",
    "    if negative_age_count > 0:\n",
    "        print(f\"... 发现 {negative_age_count} 行负房龄，将其修正为 0。\")\n",
    "        df_processed[age_col] = np.maximum(0, df_processed[age_col])\n",
    "        \n",
    "    # 清理临时列 \n",
    "    df_processed = df_processed.drop(columns=[col, parsed_year_col], errors='ignore')\n",
    "\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "df_price = process_building_age(\n",
    "    df_price,\n",
    "    n_train_price,\n",
    "    col='建筑年代',\n",
    "    trans_year_col='交易年份',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区域'\n",
    ")\n",
    "df_rent = process_building_age(\n",
    "    df_rent,\n",
    "    n_train_rent,\n",
    "    col='建筑年代',\n",
    "    trans_year_col='交易年份',\n",
    "    group_col_l1='板块',\n",
    "    group_col_l2='区县'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee702c9b",
   "metadata": {},
   "source": [
    "#### c. 处理租期数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a255148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 使用训练集的中位数 12.0 月填充 '租期_月数' 的缺失值...\n"
     ]
    }
   ],
   "source": [
    "def parse_lease_term(text):\n",
    "    \"\"\"\n",
    "    解析租期字符串，转换为统一的【月数】。\n",
    "    1. \"X年以内\" -> X * 12\n",
    "    2. \"X个月\" -> X\n",
    "    3. \"X～Y月\" -> (X+Y)/2\n",
    "    4. 其他/NaN -> NaN\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    \n",
    "    s = str(text).strip()\n",
    "    \n",
    "    match_year = re.search(r'(\\d+)\\s*年以内', s)\n",
    "    if match_year:\n",
    "        try:\n",
    "            years = int(match_year.group(1))\n",
    "            return float(years * 12)\n",
    "        except ValueError: pass\n",
    "            \n",
    "    match_range = re.search(r'(\\d+)\\s*[～\\-]\\s*(\\d+)\\s*月', s)\n",
    "    if match_range:\n",
    "        try:\n",
    "            month1 = int(match_range.group(1))\n",
    "            month2 = int(match_range.group(2))\n",
    "            return float((month1 + month2) / 2.0)\n",
    "        except ValueError: pass\n",
    "\n",
    "    match_month = re.search(r'(\\d+)\\s*个?月', s)\n",
    "    if match_month:\n",
    "        try:\n",
    "            months = int(match_month.group(1))\n",
    "            return float(months)\n",
    "        except ValueError: pass\n",
    "            \n",
    "    try:\n",
    "        num = int(s)\n",
    "        if num > 0: return float(num)\n",
    "    except ValueError: pass\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# --- 主处理函数：(仅清洗+填充) ---\n",
    "def process_lease_duration_numeric(df, n_train, col='租期'):\n",
    "    \"\"\"\n",
    "    (修改版 - 仅清洗+填充)\n",
    "    处理'租期'特征：\n",
    "    1. 使用 parse_lease_term 清洗并转换为数值（月数），存入新列 '租期_月数'。\n",
    "    2. 使用训练集的中位数填充 '租期_月数' 的 NaN。\n",
    "    3. 删除原始 '租期' 列。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 清洗并转换为数值 (月数) \n",
    "    lease_col_numeric = col + '_月数' # 新列名，例如 '租期_月数'\n",
    "    df_processed[lease_col_numeric] = df_processed[col].apply(parse_lease_term)\n",
    "    \n",
    "    nan_count = df_processed[lease_col_numeric].isnull().sum()\n",
    "    \n",
    "    # 填充缺失值 (使用训练集 Median)\n",
    "    if nan_count > 0:\n",
    "        train_part = df_processed.iloc[:n_train]\n",
    "        median_lease = train_part[lease_col_numeric].median()\n",
    "        # 提供一个合理的回退值 (例如 12 个月)\n",
    "        fill_value = median_lease if pd.notna(median_lease) else 12.0 \n",
    "        \n",
    "        print(f\"... 使用训练集的中位数 {fill_value:.1f} 月填充 '{lease_col_numeric}' 的缺失值...\")\n",
    "        df_processed[lease_col_numeric].fillna(fill_value, inplace=True)\n",
    "\n",
    "    # 清理\n",
    "    df_processed = df_processed.drop(columns=[col], errors='ignore')\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "df_rent = process_lease_duration_numeric(df_rent, n_train_rent, col='租期')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660647da",
   "metadata": {},
   "source": [
    "### 4. 创建复杂特征列\n",
    "\n",
    "#### a.创建一些比率类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be80c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ratio(df_price, NumeratorValue, DenominatorValue, new_col_name):\n",
    "    \"\"\"\n",
    "    计算一些比率类特征变量，处理除以零的情况。\n",
    "    \"\"\"\n",
    "    df_price[new_col_name] = np.where(\n",
    "        df_price[DenominatorValue] > 0,\n",
    "        df_price[NumeratorValue] / df_price[DenominatorValue],\n",
    "        0\n",
    "    )\n",
    "def feature(df):\n",
    "    cal_ratio(df, '房屋总数', '楼栋总数', '平均每栋房屋数')\n",
    "    cal_ratio(df, '停车位', '房屋总数', '停车位与房屋总数比')\n",
    "    cal_ratio(df, '绿 化 率', '容 积 率', '绿化率与容积率比')\n",
    "    cal_ratio(df, '室', '厅', '室厅比')\n",
    "    cal_ratio(df, '室', '卫', '室卫比')\n",
    "    df = process_structure(df,structure_col = '卫',fillna_value='',prefix='卫')\n",
    "    df = process_structure(df,structure_col = '厅',fillna_value='',prefix='厅')\n",
    "    df = process_structure(df,structure_col = '室',fillna_value='',prefix='室')\n",
    "feature(df_price)\n",
    "feature(df_rent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c56ce",
   "metadata": {},
   "source": [
    "#### b. 计算距城市中心距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e442e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_city_center_and_distances(df):\n",
    "    \"\"\"\n",
    "    计算每个城市的中心点 (平均经纬度) 和房源到中心点的距离。\n",
    "    \"\"\"\n",
    "    city_centers = df.groupby('城市', observed=True)[['lon', 'lat']].mean().reset_index()\n",
    "    \n",
    "    # 重命名，以便合并\n",
    "    city_centers = city_centers.rename(columns={'lon': 'center_lon', 'lat': 'center_lat'})\n",
    "\n",
    "\n",
    "    # 将中心点坐标合并回主 DataFrame\n",
    "    df = pd.merge(df, city_centers, on='城市', how='left')\n",
    "\n",
    "\n",
    "    # 定义距离计算函数\n",
    "    def compute_distance(row):\n",
    "        # 检查必要的列是否存在且非 NaN\n",
    "        if pd.isna(row['lat']) or pd.isna(row['lon']) or pd.isna(row['center_lat']) or pd.isna(row['center_lon']):\n",
    "            return np.nan\n",
    "        \n",
    "        # geopy.distance.geodesic 需要 (纬度, 经度) 格式\n",
    "        center_coords = (row['center_lat'], row['center_lon'])\n",
    "        property_coords = (row['lat'], row['lon'])\n",
    "        \n",
    "        try:\n",
    "            distance_km = geodesic(center_coords, property_coords).km\n",
    "            return distance_km\n",
    "        except ValueError:\n",
    "            # 处理 geopy 可能因无效坐标抛出的错误\n",
    "            return np.nan\n",
    "\n",
    "    # 应用函数计算距离并创建新特征\n",
    "    df['距离中心_公里'] = df.apply(compute_distance, axis=1)\n",
    "    \n",
    "    # 创建距离的平方特征\n",
    "    df['距离中心_公里_平方'] = df['距离中心_公里'] ** 2\n",
    " \n",
    "    df = df.drop(columns=['center_lon', 'center_lat'], errors='ignore')\n",
    "\n",
    "    return df\n",
    "df_price = compute_city_center_and_distances(df_price)\n",
    "df_rent = compute_city_center_and_distances(df_rent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5ec4c",
   "metadata": {},
   "source": [
    "#### c. 对经纬度进行 K-means 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd831869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始为 '城市' 列创建地理聚类 (每个城市 100 个簇) ---\n",
      "--- 开始为 '城市' 列创建地理聚类 (每个城市 100 个簇) ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans # 确保已导入\n",
    "\n",
    "def create_geo_clusters(df, n_clusters=50, city_col='城市', lon_col='lon', lat_col='lat'):\n",
    "    \"\"\"\n",
    "    (完善版)\n",
    "    为每个城市计算地理聚类 (基于 lon, lat) 并进行独热编码。\n",
    "\n",
    "    参数:\n",
    "    df (pd.DataFrame): 输入 DataFrame，必须包含 city_col, lon_col, lat_col。\n",
    "    n_clusters (int): 为每个城市生成的聚类数量。\n",
    "    city_col (str): 代表城市标签的列名。\n",
    "    lon_col (str): 代表经度的列名。\n",
    "    lat_col (str): 代表纬度的列名。\n",
    "\n",
    "    返回:\n",
    "    pd.DataFrame: 增加了 GeoCluster_* 独热编码列的 DataFrame 副本。\n",
    "                 原始的 city_col, lon_col, lat_col 仍在。\n",
    "                 临时的 '地理聚类_temp' 列会被删除。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy() # 在副本上操作\n",
    "\n",
    "    # --- 1. 检查必需的列 ---\n",
    "    required_cols = [city_col, lon_col, lat_col]\n",
    "    if not all(col in df_processed.columns for col in required_cols):\n",
    "        print(f\"错误: DataFrame 缺少必需的列: {required_cols}。跳过聚类...\")\n",
    "        return df # 返回原始 DataFrame\n",
    "\n",
    "    print(f\"--- 开始为 '{city_col}' 列创建地理聚类 (每个城市 {n_clusters} 个簇) ---\")\n",
    "\n",
    "    # --- 2. 处理坐标 NaN ---\n",
    "    coord_nan_mask = df_processed[[lon_col, lat_col]].isnull().any(axis=1)\n",
    "    initial_nan_count = coord_nan_mask.sum()\n",
    "    if initial_nan_count > 0:\n",
    "        print(f\"    (警告: 发现 {initial_nan_count} 行的经纬度坐标存在 NaN，这些行将不会被分配聚类)\")\n",
    "\n",
    "    # --- 3. 初始化聚类列 ---\n",
    "    cluster_col_temp = '地理聚类_temp'\n",
    "    df_processed[cluster_col_temp] = -1 # 初始化为 -1 (未分配)\n",
    "\n",
    "    all_city_labels = df_processed[city_col].unique()\n",
    "    \n",
    "    # --- 4. 循环为每个城市进行聚类 ---\n",
    "    for city_label in all_city_labels:\n",
    "        # 筛选出当前城市的数据，并且坐标有效\n",
    "        city_mask = (df_processed[city_col] == city_label) & (~coord_nan_mask)\n",
    "        city_data = df_processed.loc[city_mask, [lon_col, lat_col]]\n",
    "        \n",
    "        # 确保该城市有足够的数据点进行聚类\n",
    "        if len(city_data) >= n_clusters:\n",
    "            try:\n",
    "                # 初始化并拟合 KMeans\n",
    "                kmeans = KMeans(n_clusters=n_clusters, \n",
    "                                random_state=42, \n",
    "                                n_init='auto') # n_init='auto' 抑制未来警告\n",
    "                # 预测聚类标签 (0, 1, 2...)\n",
    "                clusters = kmeans.fit_predict(city_data)\n",
    "                \n",
    "                # 将聚类标签写回主 DataFrame (使用 .loc 和 city_data 的索引确保准确)\n",
    "                df_processed.loc[city_data.index, cluster_col_temp] = clusters\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    (警告: 城市 {city_label} KMeans 失败: {e})\")\n",
    "        elif len(city_data) > 0: # 如果城市有数据但太少\n",
    "             print(f\"    (警告: 城市 {city_label} 有效数据点不足 ({len(city_data)})，跳过聚类)\")\n",
    "        # 如果城市完全没有有效坐标数据，则不做任何事，保持 -1\n",
    "\n",
    "    # --- 5. 创建组合标签 ---\n",
    "    #    例如：城市0的簇1 -> \"C0_1\", 城市1的簇1 -> \"C1_1\"\n",
    "    #    对于未分配聚类的行 (包括 NaN 坐标或小城市)，创建一个特定标签\n",
    "    combined_label_col = '地理聚类_带城市'\n",
    "    df_processed[combined_label_col] = np.where(\n",
    "         (df_processed[cluster_col_temp] != -1) & df_processed[city_col].notna(), # 仅组合有效聚类和城市\n",
    "         'C' + df_processed[city_col].astype(str) + '_' + df_processed[cluster_col_temp].astype(str),\n",
    "         'GeoCluster_Unknown' # 为所有未聚类的行指定一个标签\n",
    "    )\n",
    "    \n",
    "    # --- 6. 独热编码 ---\n",
    "\n",
    "    df_processed = pd.get_dummies(df_processed, \n",
    "                                columns=[combined_label_col], \n",
    "                                prefix='GeoCluster', \n",
    "                                drop_first=False) # drop_first=False 对聚类标签通常更安全\n",
    "    \n",
    "    # --- 7. 清理临时列 ---\n",
    "    df_processed = df_processed.drop(columns=[cluster_col_temp], errors='ignore')\n",
    "\n",
    "    return df_processed # 返回修改后的副本\n",
    "\n",
    "\n",
    "df_price = create_geo_clusters(df_price, n_clusters=100) \n",
    "df_rent = create_geo_clusters(df_rent, n_clusters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8bdb6",
   "metadata": {},
   "source": [
    "### 5. 清理与最终准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1182f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "区域",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "建筑面积",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "套内面积",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "房屋总数",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "楼栋总数",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "绿 化 率",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "容 积 率",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物 业 费",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "燃气费",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "供热费",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "停车位",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "停车费用",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "室",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "厅",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "卫",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "得房率",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "总楼层",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_东",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_东北",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_东南",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_北",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_南",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_西",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_西北",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "朝向_西南",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "有电梯",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "梯户比",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_住宅",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_商业",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_底商",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_别墅",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_办公",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_公寓",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "物业_is_其他",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "产权_is_商品房",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "产权_is_使用权",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "产权_is_已购公房",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "产权_is_私产",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "产权_is_其他",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "建构_is_板楼",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "建构_is_塔楼",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "建构_is_塔板结合",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "建构_is_平房",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "建构_is_其他",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "交易年份",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "房龄",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "平均每栋房屋数",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "停车位与房屋总数比",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "绿化率与容积率比",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "室厅比",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "室卫比",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "距离中心_公里",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "距离中心_公里_平方",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b1e331bd-8530-43b9-9828-3bcbdaace893",
       "rows": [
        [
         "count",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0",
         "137888.0"
        ],
        [
         "mean",
         "65.87703063355767",
         "98.12844105360873",
         "80.65704675120709",
         "115.04158017237366",
         "32.39072178690783",
         "1915.8493233638894",
         "31.23294267811557",
         "33.21871366616384",
         "2.657228257716407",
         "2.4032036326583897",
         "2.6856946942446047",
         "0.005215646031561848",
         "1074.0763119343235",
         "304.12436781580334",
         "2.599022394987236",
         "1.5239542237177999",
         "1.41843380134602",
         "0.8201427463897348",
         "18.075351009514968",
         "0.07852750058018101",
         "0.020988048271060572",
         "0.13030140403805987",
         "0.3152631120909724",
         "0.6715667788349965",
         "0.0456167324205152",
         "0.02044412856811325",
         "0.04487700162450685",
         "0.7626769552100255",
         "0.2514582488224867",
         "0.9942634601995822",
         "0.2937674054304943",
         "0.3230375377117661",
         "0.11101038524019494",
         "0.07032519145973544",
         "0.02021930842422836",
         "0.001029821304246925",
         "0.9997171617544673",
         "0.13941749825945696",
         "0.07438645857507542",
         "0.16218235089347877",
         "0.0002755859828266419",
         "0.5223079600835461",
         "0.6510573799025295",
         "0.315139823624971",
         "0.05041047806915758",
         "0.0",
         "2024.042374970991",
         "16.647612555117195",
         "128.73305522480496",
         "2.363184703724254",
         "15.813693055865398",
         "1.7439923464454243",
         "1.9393708506696947",
         "17.46798754515931",
         "477.3157864471738"
        ],
        [
         "std",
         "34.223416352454215",
         "44.855142843344666",
         "37.47438914712078",
         "5.815359202129368",
         "6.010348243077014",
         "1839.7959853588454",
         "61.627978762628956",
         "8.243372617033359",
         "1.3910966621922176",
         "3.6436123128912525",
         "0.5596663527380465",
         "0.2798340555736975",
         "1230.291599465642",
         "212.15340254899576",
         "0.957190485633178",
         "0.5710258069405058",
         "0.6405669579142559",
         "0.030392015825784928",
         "10.94347464295547",
         "0.26900084947275477",
         "0.14334468639647552",
         "0.336635960642326",
         "0.4646222635857018",
         "0.46964501488463184",
         "0.2086532096073125",
         "0.14151435054895709",
         "0.20703470048775288",
         "0.4254434508565852",
         "0.2698137697691997",
         "0.07552266729991208",
         "0.455488333068878",
         "0.4676386135774292",
         "0.3141461368834362",
         "0.25569519561250853",
         "0.14075024569441122",
         "0.03207441711437929",
         "0.016815477950667877",
         "0.3463829233279976",
         "0.2623997193219948",
         "0.36861934484073855",
         "0.016598555156310316",
         "0.49950391828988405",
         "0.4766375096123602",
         "0.464573223974919",
         "0.21879124510145564",
         "0.0",
         "0.8510063905248729",
         "8.002008126679488",
         "126.66000855828453",
         "33.1585966580699",
         "21.424876068662588",
         "0.7294435988669229",
         "0.658076709980721",
         "13.121983322298382",
         "1244.364781463252"
        ],
        [
         "min",
         "1.0",
         "11.7",
         "9.647826191342675",
         "103.48262373034508",
         "23.02585259872269",
         "1.0",
         "1.0",
         "0.01",
         "0.02",
         "0.02",
         "0.4",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.6",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2018.0",
         "2.0",
         "0.6666666666666666",
         "0.0002855782960495002",
         "0.0033333333333333335",
         "0.0",
         "0.0",
         "0.20485405302358412",
         "0.04196518304018941"
        ],
        [
         "25%",
         "35.0",
         "70.74",
         "58.03",
         "107.6479876970323",
         "30.428380096619517",
         "714.0",
         "8.0",
         "30.0",
         "2.0",
         "1.3",
         "2.33",
         "0.0",
         "380.0",
         "150.0",
         "2.0",
         "1.0",
         "1.0",
         "0.8246005291745876",
         "7.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.08333333333333333",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "2024.0",
         "11.5",
         "40.833333333333336",
         "0.27419354838709675",
         "10.801393728222996",
         "1.5",
         "1.5",
         "8.538431982862217",
         "72.90482072882706"
        ],
        [
         "50%",
         "68.0",
         "90.77",
         "74.46967378975702",
         "116.1245740809776",
         "32.18720952475596",
         "1372.0",
         "15.0",
         "34.0",
         "2.5",
         "1.9",
         "2.61",
         "0.0",
         "734.0",
         "300.0",
         "3.0",
         "2.0",
         "1.0",
         "0.8246005291745877",
         "18.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.25",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "2024.0",
         "15.0",
         "91.77777777777777",
         "0.5349854227405247",
         "13.6",
         "1.5",
         "2.0",
         "14.5370743704703",
         "211.3265312545039"
        ],
        [
         "75%",
         "89.0",
         "117.44",
         "96.47826191342676",
         "121.53414642641874",
         "40.47925069413861",
         "2477.0",
         "29.0",
         "35.0",
         "3.0",
         "2.65",
         "3.0",
         "0.0",
         "1300.0",
         "380.0",
         "3.0",
         "2.0",
         "2.0",
         "0.8246005291745877",
         "28.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.3333333333333333",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "0.0",
         "2025.0",
         "19.0",
         "175.42857142857142",
         "1.0135635018495683",
         "17.6",
         "2.0",
         "2.0",
         "22.579846822448655",
         "509.84948253717334"
        ],
        [
         "max",
         "131.0",
         "508.11",
         "466.36",
         "122.96666899547684",
         "42.6936762390023",
         "12669.0",
         "734.0",
         "80.0",
         "30.0",
         "76.45",
         "5.0",
         "25.0",
         "8700.0",
         "2300.0",
         "17.0",
         "9.0",
         "12.0",
         "0.9499214013024927",
         "70.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "20.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "2025.0",
         "90.0",
         "2175.0",
         "4350.0",
         "2250.0",
         "17.0",
         "8.0",
         "227.4590460885672",
         "51737.617647520936"
        ]
       ],
       "shape": {
        "columns": 55,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>区域</th>\n",
       "      <th>建筑面积</th>\n",
       "      <th>套内面积</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>房屋总数</th>\n",
       "      <th>楼栋总数</th>\n",
       "      <th>绿 化 率</th>\n",
       "      <th>容 积 率</th>\n",
       "      <th>物 业 费</th>\n",
       "      <th>...</th>\n",
       "      <th>建构_is_其他</th>\n",
       "      <th>交易年份</th>\n",
       "      <th>房龄</th>\n",
       "      <th>平均每栋房屋数</th>\n",
       "      <th>停车位与房屋总数比</th>\n",
       "      <th>绿化率与容积率比</th>\n",
       "      <th>室厅比</th>\n",
       "      <th>室卫比</th>\n",
       "      <th>距离中心_公里</th>\n",
       "      <th>距离中心_公里_平方</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>137888.0</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "      <td>137888.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.877031</td>\n",
       "      <td>98.128441</td>\n",
       "      <td>80.657047</td>\n",
       "      <td>115.041580</td>\n",
       "      <td>32.390722</td>\n",
       "      <td>1915.849323</td>\n",
       "      <td>31.232943</td>\n",
       "      <td>33.218714</td>\n",
       "      <td>2.657228</td>\n",
       "      <td>2.403204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024.042375</td>\n",
       "      <td>16.647613</td>\n",
       "      <td>128.733055</td>\n",
       "      <td>2.363185</td>\n",
       "      <td>15.813693</td>\n",
       "      <td>1.743992</td>\n",
       "      <td>1.939371</td>\n",
       "      <td>17.467988</td>\n",
       "      <td>477.315786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34.223416</td>\n",
       "      <td>44.855143</td>\n",
       "      <td>37.474389</td>\n",
       "      <td>5.815359</td>\n",
       "      <td>6.010348</td>\n",
       "      <td>1839.795985</td>\n",
       "      <td>61.627979</td>\n",
       "      <td>8.243373</td>\n",
       "      <td>1.391097</td>\n",
       "      <td>3.643612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.851006</td>\n",
       "      <td>8.002008</td>\n",
       "      <td>126.660009</td>\n",
       "      <td>33.158597</td>\n",
       "      <td>21.424876</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>0.658077</td>\n",
       "      <td>13.121983</td>\n",
       "      <td>1244.364781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>9.647826</td>\n",
       "      <td>103.482624</td>\n",
       "      <td>23.025853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204854</td>\n",
       "      <td>0.041965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>70.740000</td>\n",
       "      <td>58.030000</td>\n",
       "      <td>107.647988</td>\n",
       "      <td>30.428380</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>40.833333</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>10.801394</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>8.538432</td>\n",
       "      <td>72.904821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>90.770000</td>\n",
       "      <td>74.469674</td>\n",
       "      <td>116.124574</td>\n",
       "      <td>32.187210</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>91.777778</td>\n",
       "      <td>0.534985</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.537074</td>\n",
       "      <td>211.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>117.440000</td>\n",
       "      <td>96.478262</td>\n",
       "      <td>121.534146</td>\n",
       "      <td>40.479251</td>\n",
       "      <td>2477.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>175.428571</td>\n",
       "      <td>1.013564</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.579847</td>\n",
       "      <td>509.849483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>131.000000</td>\n",
       "      <td>508.110000</td>\n",
       "      <td>466.360000</td>\n",
       "      <td>122.966669</td>\n",
       "      <td>42.693676</td>\n",
       "      <td>12669.000000</td>\n",
       "      <td>734.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>76.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>2175.000000</td>\n",
       "      <td>4350.000000</td>\n",
       "      <td>2250.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>227.459046</td>\n",
       "      <td>51737.617648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  区域           建筑面积           套内面积            lon  \\\n",
       "count  137888.000000  137888.000000  137888.000000  137888.000000   \n",
       "mean       65.877031      98.128441      80.657047     115.041580   \n",
       "std        34.223416      44.855143      37.474389       5.815359   \n",
       "min         1.000000      11.700000       9.647826     103.482624   \n",
       "25%        35.000000      70.740000      58.030000     107.647988   \n",
       "50%        68.000000      90.770000      74.469674     116.124574   \n",
       "75%        89.000000     117.440000      96.478262     121.534146   \n",
       "max       131.000000     508.110000     466.360000     122.966669   \n",
       "\n",
       "                 lat           房屋总数           楼栋总数          绿 化 率  \\\n",
       "count  137888.000000  137888.000000  137888.000000  137888.000000   \n",
       "mean       32.390722    1915.849323      31.232943      33.218714   \n",
       "std         6.010348    1839.795985      61.627979       8.243373   \n",
       "min        23.025853       1.000000       1.000000       0.010000   \n",
       "25%        30.428380     714.000000       8.000000      30.000000   \n",
       "50%        32.187210    1372.000000      15.000000      34.000000   \n",
       "75%        40.479251    2477.000000      29.000000      35.000000   \n",
       "max        42.693676   12669.000000     734.000000      80.000000   \n",
       "\n",
       "               容 积 率          物 业 费  ...  建构_is_其他           交易年份  \\\n",
       "count  137888.000000  137888.000000  ...  137888.0  137888.000000   \n",
       "mean        2.657228       2.403204  ...       0.0    2024.042375   \n",
       "std         1.391097       3.643612  ...       0.0       0.851006   \n",
       "min         0.020000       0.020000  ...       0.0    2018.000000   \n",
       "25%         2.000000       1.300000  ...       0.0    2024.000000   \n",
       "50%         2.500000       1.900000  ...       0.0    2024.000000   \n",
       "75%         3.000000       2.650000  ...       0.0    2025.000000   \n",
       "max        30.000000      76.450000  ...       0.0    2025.000000   \n",
       "\n",
       "                  房龄        平均每栋房屋数      停车位与房屋总数比       绿化率与容积率比  \\\n",
       "count  137888.000000  137888.000000  137888.000000  137888.000000   \n",
       "mean       16.647613     128.733055       2.363185      15.813693   \n",
       "std         8.002008     126.660009      33.158597      21.424876   \n",
       "min         2.000000       0.666667       0.000286       0.003333   \n",
       "25%        11.500000      40.833333       0.274194      10.801394   \n",
       "50%        15.000000      91.777778       0.534985      13.600000   \n",
       "75%        19.000000     175.428571       1.013564      17.600000   \n",
       "max        90.000000    2175.000000    4350.000000    2250.000000   \n",
       "\n",
       "                 室厅比            室卫比        距离中心_公里     距离中心_公里_平方  \n",
       "count  137888.000000  137888.000000  137888.000000  137888.000000  \n",
       "mean        1.743992       1.939371      17.467988     477.315786  \n",
       "std         0.729444       0.658077      13.121983    1244.364781  \n",
       "min         0.000000       0.000000       0.204854       0.041965  \n",
       "25%         1.500000       1.500000       8.538432      72.904821  \n",
       "50%         1.500000       2.000000      14.537074     211.326531  \n",
       "75%         2.000000       2.000000      22.579847     509.849483  \n",
       "max        17.000000       8.000000     227.459046   51737.617648  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_price = process_structure(df_price,structure_col = '城市',fillna_value='',prefix='城市')\n",
    "df_rent = process_structure(df_rent,structure_col = '城市',fillna_value='',prefix='城市')\n",
    "\n",
    "df_price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035676d",
   "metadata": {},
   "source": [
    "#### a. 清除作为数据填充依据的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67faf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_final = ['区域', 'lon', 'lat', '板块', '区县', '建筑结构_comm']\n",
    "df_price = df_price.drop(columns=cols_to_drop_final, errors='ignore')\n",
    "df_rent = df_rent.drop(columns=cols_to_drop_final, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53b9b9",
   "metadata": {},
   "source": [
    "#### b. 重新分离训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9766bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ln_price = np.log1p(y_train_price)\n",
    "X_train_price = df_price.iloc[:n_train_price]\n",
    "X_test = df_price.iloc[n_train_price:]\n",
    "\n",
    "y_train_ln_rent = np.log1p(y_train_rent)\n",
    "X_train_rent = df_rent.iloc[:n_train_rent]\n",
    "X_test_rent = df_rent.iloc[n_train_rent:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0923da",
   "metadata": {},
   "source": [
    "#### c. 保存数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1449c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_price.to_parquet('X_train_price.parquet')\n",
    "X_test.to_parquet('X_test_price.parquet')\n",
    "y_train_ln_price.to_frame().to_parquet('y_train_price.parquet')\n",
    "df_price.to_parquet('df_price.parquet')\n",
    "\n",
    "X_train_rent.to_parquet('X_train_rent.parquet')\n",
    "X_test_rent.to_parquet('X_test_rent.parquet')\n",
    "y_train_ln_rent.to_frame().to_parquet('y_train_rent.parquet')\n",
    "df_rent.to_parquet('df_rent.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfdcf98",
   "metadata": {},
   "source": [
    "## Part3：特征工程\n",
    "\n",
    "### 1. 异常值检测与处理\n",
    "#### a. 识别特征类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7f5a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载上一步保存的数据\n",
    "X_train_price = pd.read_parquet('X_train_price.parquet')\n",
    "X_test_price = pd.read_parquet('X_test_price.parquet')\n",
    "y_train_ln_price = pd.read_parquet('y_train_price.parquet')\n",
    "\n",
    "X_train_rent = pd.read_parquet('X_train_rent.parquet')\n",
    "X_test_rent = pd.read_parquet('X_test_rent.parquet')\n",
    "y_train_ln_rent = pd.read_parquet('y_train_rent.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d356cd0",
   "metadata": {},
   "source": [
    "#### b. 识别并清除异常值\n",
    "接下来使用 Q1 和 Q99 分位数识别异常值并清除（对于数值型变量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68eea61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    (修正版)\n",
    "    使用Q1和Q99分位数对数值特征进行“缩尾处理”(Capping)。\n",
    "    \"\"\"\n",
    "    X_train = X_train.copy() # 使用 .copy() 避免 SettingWithCopyWarning\n",
    "    X_test = X_test.copy()\n",
    "    y_train = y_train.copy() # 对 y_train 也使用 .copy()\n",
    "\n",
    "    # --- (*** 修正点：更精确地识别数值列 ***) ---\n",
    "    numeric_cols = []\n",
    "    binary_cols = []\n",
    "    other_cols = [] \n",
    "\n",
    "    for col in X_train.columns:\n",
    "        dtype = X_train[col].dtype\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(dtype) and not pd.api.types.is_bool_dtype(dtype):\n",
    "\n",
    "            if X_train[col].nunique(dropna=False) == 2 and X_train[col].min() == 0 and X_train[col].max() == 1:\n",
    "                binary_cols.append(col)\n",
    "            else:\n",
    "                numeric_cols.append(col) # 这是真正的数值列\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            binary_cols.append(col)\n",
    "        else:\n",
    "            other_cols.append(col)\n",
    "    \n",
    "    numeric_cols.remove('供热费')\n",
    "    quantiles = X_train[numeric_cols].quantile([0.01, 0.99])\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        # 从训练集获取 Q1 和 Q99\n",
    "        Q1 = quantiles.loc[0.01, col]\n",
    "        Q99 = quantiles.loc[0.99, col]\n",
    "        \n",
    "        X_train[col] = X_train[col].clip(Q1, Q99)\n",
    "        if col in X_test.columns:\n",
    "                X_test[col] = X_test[col].clip(Q1, Q99)\n",
    "    \n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "X_train_price, X_test_price, y_train_ln_price = detect_outliers(X_train_price, X_test_price, y_train_ln_price)\n",
    "X_train_rent, X_test_rent, y_train_ln_rent = detect_outliers(X_train_rent, X_test_rent, y_train_ln_rent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b72c3",
   "metadata": {},
   "source": [
    "### 2. 创建非线性特征\n",
    "\n",
    "#### a. 对数转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "421fc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Skew (偏度)  Kurtosis (峰度)\n",
      "供热费               89.462816    8391.021565\n",
      "停车位与房屋总数比          6.267866      44.141269\n",
      "楼栋总数               4.426539      23.101704\n",
      "物 业 费              3.480153      15.804279\n",
      "距离中心_公里_平方         3.002323      10.329502\n",
      "...                     ...            ...\n",
      "GeoCluster_C2_5    0.000000       0.000000\n",
      "GeoCluster_C5_56   0.000000       0.000000\n",
      "GeoCluster_C5_61   0.000000       0.000000\n",
      "GeoCluster_C5_82   0.000000       0.000000\n",
      "GeoCluster_C9_99   0.000000       0.000000\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "                   Skew (偏度)  Kurtosis (峰度)\n",
      "楼栋总数                4.463038      23.762716\n",
      "交易年份                3.793161      12.388322\n",
      "停车位与房屋总数比           3.533512      16.618094\n",
      "物 业 费               3.013789      11.841948\n",
      "卫                   2.936996       6.626078\n",
      "停车位                 2.928420       9.504336\n",
      "距离中心_公里_平方          2.583159       7.585866\n",
      "租期_月数               2.540422       5.104637\n",
      "容 积 率               1.995659       5.375790\n",
      "停车费用                1.936715       4.884821\n",
      "平均每栋房屋数             1.674671       2.804404\n",
      "房屋总数                1.568431       2.025561\n",
      "绿化率与容积率比            1.452545       2.979152\n",
      "距离中心_公里             1.132043       1.137688\n",
      "房龄                  1.128693       1.408974\n",
      "面积                  0.868420       1.447621\n",
      "室                   0.483129      -0.135346\n",
      "室卫比                 0.457392      -0.492119\n",
      "室厅比                 0.438491       1.102869\n",
      "厅                  -0.238681      -0.662136\n",
      "总楼层                 0.191463      -1.081333\n",
      "绿 化 率              -0.142973       2.298467\n",
      "燃气费                -0.056258      -0.163406\n",
      "供热费                 0.043335      -1.704406\n",
      "建构_is_其他            0.000000       0.000000\n",
      "GeoCluster_C8_41    0.000000       0.000000\n",
      "GeoCluster_C5_38    0.000000       0.000000\n",
      "GeoCluster_C6_5     0.000000       0.000000\n",
      "GeoCluster_C6_90    0.000000       0.000000\n",
      "GeoCluster_C9_87    0.000000       0.000000\n",
      "GeoCluster_C8_64    0.000000       0.000000\n",
      "GeoCluster_C9_10    0.000000       0.000000\n",
      "GeoCluster_C9_53    0.000000       0.000000\n",
      "GeoCluster_C11_90   0.000000       0.000000\n",
      "GeoCluster_C4_23    0.000000       0.000000\n",
      "GeoCluster_C11_57   0.000000       0.000000\n",
      "GeoCluster_C11_84   0.000000       0.000000\n",
      "GeoCluster_C11_74   0.000000       0.000000\n",
      "年份_2023.0           0.000000       0.000000\n",
      "GeoCluster_C11_53   0.000000       0.000000\n",
      "GeoCluster_C11_5    0.000000       0.000000\n",
      "GeoCluster_C11_32   0.000000       0.000000\n",
      "GeoCluster_C11_30   0.000000       0.000000\n",
      "GeoCluster_C11_15   0.000000       0.000000\n",
      "GeoCluster_C0_24    0.000000       0.000000\n",
      "GeoCluster_C9_89    0.000000       0.000000\n"
     ]
    }
   ],
   "source": [
    "def cal_skew(df):\n",
    "    # 重新识别 0/1 哑变量列\n",
    "    binary_cols = [col for col in df.columns\n",
    "                   if df[col].nunique(dropna=False) == 2 and\n",
    "                   df[col].min() == 0 and\n",
    "                   df[col].max() == 1]\n",
    "\n",
    "    # 识别连续型数值列\n",
    "    continuous_numeric_cols = [col for col in df.columns if col not in binary_cols]\n",
    "\n",
    "    X_train_numeric = df[continuous_numeric_cols]\n",
    "\n",
    "    skewness = X_train_numeric.skew()\n",
    "    kurtosis = X_train_numeric.kurtosis()\n",
    "\n",
    "    # 创建一个汇总 DataFrame\n",
    "    distribution_summary = pd.DataFrame({\n",
    "        'Skew (偏度)': skewness,\n",
    "        'Kurtosis (峰度)': kurtosis\n",
    "    })\n",
    "\n",
    "    # 按偏度的绝对值降序排列\n",
    "    distribution_summary_sorted = distribution_summary.reindex(\n",
    "        distribution_summary['Skew (偏度)'].abs().sort_values(ascending=False).index\n",
    "    )\n",
    "\n",
    "    print(distribution_summary_sorted)\n",
    "\n",
    "cal_skew(X_train_price)\n",
    "cal_skew(X_train_rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c5724b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(X_train, X_test, skewed_cols):\n",
    "    \"\"\"\n",
    "    对训练集和测试集中指定的偏斜数值特征应用 log1p 转换。\n",
    "    会检查列是否在各自的DataFrame中存在。\n",
    "    转换后会删除原始列。\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): 训练集特征.\n",
    "        X_test (pd.DataFrame): 测试集特征.\n",
    "        skewed_cols (list): 需要进行对数转换的列名列表.\n",
    "\n",
    "    Returns:\n",
    "        tuple: 包含转换后的 X_train 和 X_test 的元组.\n",
    "    \"\"\"\n",
    "    # 创建副本以避免修改原始传入的DataFrame\n",
    "    X_train_transformed = X_train.copy()\n",
    "    X_test_transformed = X_test.copy()\n",
    "    \n",
    "    # 记录实际转换的列和跳过的列\n",
    "    transformed_in_train = []\n",
    "    transformed_in_test = []\n",
    "    skipped = []\n",
    "\n",
    "    for col in skewed_cols:\n",
    "        col_log = f'log_{col}' # 新列名\n",
    "\n",
    "        # 处理训练集\n",
    "        if col in X_train_transformed.columns:\n",
    "            # 检查数据类型是否为数值型，以防万一\n",
    "            if pd.api.types.is_numeric_dtype(X_train_transformed[col]):\n",
    "                X_train_transformed[col_log] = np.log1p(X_train_transformed[col])\n",
    "                X_train_transformed = X_train_transformed.drop(columns=[col], errors='ignore')\n",
    "                transformed_in_train.append(col)\n",
    "\n",
    "        # 处理测试集\n",
    "        if col in X_test_transformed.columns:\n",
    "             # 检查数据类型是否为数值型\n",
    "            if pd.api.types.is_numeric_dtype(X_test_transformed[col]):\n",
    "                X_test_transformed[col_log] = np.log1p(X_test_transformed[col])\n",
    "                X_test_transformed = X_test_transformed.drop(columns=[col], errors='ignore')\n",
    "                transformed_in_test.append(col)\n",
    "\n",
    "                if col not in transformed_in_train and (col + \" (train, 非数值)\") not in skipped:\n",
    "                     skipped.append(col + \" (test, 非数值)\")\n",
    "\n",
    "\n",
    "        # 记录那些根本不存在于任一集合的列（或只存在于一个集合中）\n",
    "        if col not in transformed_in_train and col not in transformed_in_test and \\\n",
    "           (col + \" (train, 非数值)\") not in skipped and (col + \" (test, 非数值)\") not in skipped:\n",
    "             if col not in X_train.columns and col not in X_test.columns:\n",
    "                  skipped.append(col + \" (train&test 均不存在)\")\n",
    "             elif col not in X_train.columns:\n",
    "                  skipped.append(col + \" (train 不存在)\")\n",
    "             elif col not in X_test.columns:\n",
    "                  skipped.append(col + \" (test 不存在)\")\n",
    "\n",
    "    return X_train_transformed, X_test_transformed\n",
    "\n",
    "skewed_cols = ['容 积 率', '套内面积', '建筑面积', '燃气费', '楼栋总数',\n",
    "               '供热费', '停车位', '房屋总数', '物 业 费','停车费用']\n",
    "\n",
    "X_train_price, X_test_price = log_transform(X_train_price, X_test_price, skewed_cols)\n",
    "X_train_rent, X_test_rent = log_transform(X_train_rent, X_test_rent, skewed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9b99c",
   "metadata": {},
   "source": [
    "#### b.分箱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ee0d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_and_encode(X_train, X_test, feature='房龄'):\n",
    "    \"\"\"\n",
    "    对 '房龄' 特征进行分箱和独热编码\n",
    "    \"\"\"\n",
    "    # 检查特征是否同时存在于训练集和测试集中\n",
    "    if feature not in X_train.columns or feature not in X_test.columns:\n",
    "        return X_train, X_test\n",
    "    \n",
    "    # 创建 KBinsDiscretizer 实例\n",
    "    binner = KBinsDiscretizer(n_bins=5,   # 分为 5 个箱\n",
    "                              encode='ordinal', # 输出 0, 1, 2, 3, 4\n",
    "                              strategy='kmeans', \n",
    "                              subsample=None) # 使用所有数据\n",
    "\n",
    "    # 只在 X_train 上 .fit()\n",
    "    binner.fit(X_train[[feature]])\n",
    "\n",
    "    # 在 X_train 和 X_test 上 .transform()\n",
    "    X_train[f'{feature}_分箱'] = binner.transform(X_train[[feature]])\n",
    "    X_test[f'{feature}_分箱'] = binner.transform(X_test[[feature]])\n",
    "\n",
    "    # 将新的分箱列转换为独热编码\n",
    "    X_train = pd.get_dummies(X_train, columns=[f'{feature}_分箱'], prefix=f'{feature}段', drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, columns=[f'{feature}_分箱'], prefix=f'{feature}段', drop_first=True)\n",
    "\n",
    "    # 删除原始列\n",
    "    X_train = X_train.drop(columns=[feature], errors='ignore')\n",
    "    X_test = X_test.drop(columns=[feature], errors='ignore')\n",
    "\n",
    "    # 确保 X_test 和 X_train 有完全相同的\"房龄段\"列\n",
    "    X_train_cols = set(X_train.columns)\n",
    "    X_test_cols = set(X_test.columns)\n",
    "\n",
    "    missing_in_test = list(X_train_cols - X_test_cols)\n",
    "    for col in missing_in_test:\n",
    "        X_test[col] = 0 # 在 X_test 中添加缺失的哑变量列，并设为 0\n",
    "\n",
    "    missing_in_train = list(X_test_cols - X_train_cols)\n",
    "    for col in missing_in_train:\n",
    "        X_train[col] = 0 # (以防万一)\n",
    "\n",
    "    X_test = X_test[X_train.columns] # 保证顺序一致\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "feature_list = ['log_供热费', 'log_物 业 费', 'log_停车费用',\n",
    "                'log_房屋总数', 'log_停车位', '交易年份','房龄']\n",
    "for feature in feature_list:\n",
    "    X_train_price, X_test_price = bin_and_encode(X_train_price, X_test_price, feature=feature)\n",
    "    X_train_rent, X_test_rent = bin_and_encode(X_train_rent, X_test_rent, feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d699af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_price.to_csv('X_train_price_pre_featured.csv')\n",
    "X_test_price.to_csv('X_test_price_pre_featured.csv')\n",
    "y_train_ln_price.to_csv('y_train_price.csv')\n",
    "\n",
    "X_train_rent.to_csv('X_train_rent_pre_featured.csv')\n",
    "X_test_rent.to_csv('X_test_rent_pre_featured.csv')\n",
    "y_train_ln_rent.to_csv('y_train_rent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dfcd2",
   "metadata": {},
   "source": [
    "### 3. 创建交互项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Price dataset: (103871, 233)\n",
      "  自动识别: 19 个数值特征, 214 个二元特征。\n",
      "  成功生成 190 个新的非线性特征。\n",
      "Saved augmented Price data (train shape=(103871, 423), test shape=(34017, 423))\n",
      "\n",
      "Processing Rent dataset: (98899, 196)\n",
      "  自动识别: 17 个数值特征, 179 个二元特征。\n",
      "  成功生成 153 个新的非线性特征。\n",
      "Saved augmented Rent data (train shape=(98899, 349), test shape=(9773, 349))\n",
      "\n",
      "--- Feature generation complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. 定义特征生成函数\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def generate_nonlinear_features(df, verbose=True):\n",
    "    \"\"\"\n",
    "    根据明确的规则生成非线性特征：\n",
    "    1. 数值特征的平方项\n",
    "    2. 数值特征之间的二元交互项\n",
    "    3. 数值特征与二元(虚拟)特征的交互项\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 合并后的 (train + test) DataFrame.\n",
    "        verbose (bool): 是否打印日志.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 增加了新特征的 DataFrame.\n",
    "    \"\"\"\n",
    "    df_aug = df.copy()\n",
    "    \n",
    "    # --- 步骤 A: 自动识别列类型 ---\n",
    "    # 假设二元/虚拟变量是那些只包含 0 和 1 的列\n",
    "    binary_cols = [col for col in df.columns if set(df[col].unique()) <= {0, 1}]\n",
    "    \n",
    "    # 假设数值变量是所有 'number' 类型的列，且不是二元变量\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=np.number).columns if col not in binary_cols]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  自动识别: {len(numeric_cols)} 个数值特征, {len(binary_cols)} 个二元特征。\")\n",
    "\n",
    "    new_features_count = 0\n",
    "\n",
    "    # --- 步骤 B: (要求 1) 创建数值类平方项 ---\n",
    "    for col in numeric_cols:\n",
    "        new_col_name = f'{col}_sq'\n",
    "        if new_col_name not in df_aug.columns:\n",
    "            df_aug[new_col_name] = df_aug[col] ** 2\n",
    "            new_features_count += 1\n",
    "\n",
    "    # --- 步骤 C: (要求 2) 创建数值类内部交互项 ---\n",
    "    for col1, col2 in combinations(numeric_cols, 2):\n",
    "        new_col_name = f'{col1}_x_{col2}'\n",
    "        if new_col_name not in df_aug.columns:\n",
    "            df_aug[new_col_name] = df_aug[col1] * df_aug[col2]\n",
    "            new_features_count += 1\n",
    "            \n",
    "    if verbose:\n",
    "        print(f\"  成功生成 {new_features_count} 个新的非线性特征。\")\n",
    "\n",
    "    return df_aug\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. 执行特征工程 (Price 数据集)\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    print(f\"Processing Price dataset: {X_train_price.shape}\")\n",
    "    # 合并训练/测试，保证交互特征一致性\n",
    "    combined_price = pd.concat([X_train_price, X_test_price], axis=0)\n",
    "    \n",
    "    # 生成特征\n",
    "    X_aug_combined_price = generate_nonlinear_features(combined_price)\n",
    "    \n",
    "    # 拆回训练/测试\n",
    "    X_aug_price = X_aug_combined_price.loc[X_train_price.index]\n",
    "    X_aug_test_price = X_aug_combined_price.loc[X_test_price.index]\n",
    "    \n",
    "    # 保存生成的特征文件 (使用你之前的 'pre_filtering' 命名)\n",
    "    X_aug_price.to_parquet('X_train_price_pre_filtering.parquet', index=False)\n",
    "    X_aug_test_price.to_parquet('X_test_price_pre_filtering.parquet', index=False)\n",
    "    \n",
    "    print(f\"Saved augmented Price data (train shape={X_aug_price.shape}, test shape={X_aug_test_price.shape})\")\n",
    "    \n",
    "    # 更新变量以供后续单元使用\n",
    "    X_train_price = X_aug_price\n",
    "    X_test_price = X_aug_test_price\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing Price dataset augmentation: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. 执行特征工程 (Rent 数据集)\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    print(f\"\\nProcessing Rent dataset: {X_train_rent.shape}\")\n",
    "    # 合并\n",
    "    combined_rent = pd.concat([X_train_rent, X_test_rent], axis=0)\n",
    "    \n",
    "    # 生成特征\n",
    "    X_aug_combined_rent = generate_nonlinear_features(combined_rent)\n",
    "    \n",
    "    # 拆分\n",
    "    X_aug_rent = X_aug_combined_rent.loc[X_train_rent.index]\n",
    "    X_aug_test_rent = X_aug_combined_rent.loc[X_test_rent.index]\n",
    "\n",
    "    # 保存生成的特征文件\n",
    "    X_aug_rent.to_parquet('X_train_rent_pre_filtering.parquet', index=False)\n",
    "    X_aug_test_rent.to_parquet('X_test_rent_pre_filtering.parquet', index=False)\n",
    "    \n",
    "    print(f\"Saved augmented Rent data (train shape={X_aug_rent.shape}, test shape={X_aug_test_rent.shape})\")\n",
    "    \n",
    "    # 更新变量以供后续单元使用\n",
    "    X_train_rent = X_aug_rent\n",
    "    X_test_rent = X_aug_test_rent\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing Rent dataset augmentation: {e}\")\n",
    "\n",
    "print(\"\\n--- Feature generation complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6b666",
   "metadata": {},
   "source": [
    "## Part 4: 模型训练与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f61b9b",
   "metadata": {},
   "source": [
    "### 1. 导入Z-score 后的数据\n",
    "\n",
    "#### a. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_data(suffix='final'):\n",
    "    \"\"\"\n",
    "    加载特征数据\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train_price = pd.read_parquet(f'X_train_price{suffix}.parquet')\n",
    "    X_test_price = pd.read_parquet(f'X_test_price{suffix}.parquet')\n",
    "    y_train_ln_price = pd.read_parquet('y_train_price.parquet')\n",
    "\n",
    "    X_train_rent = pd.read_parquet(f'X_train_rent{suffix}.parquet')\n",
    "    X_test_rent = pd.read_parquet(f'X_test_rent{suffix}.parquet')\n",
    "    y_train_ln_rent = pd.read_parquet('y_train_rent.parquet')\n",
    "    \n",
    "    return X_train_price, X_test_price, y_train_ln_price, \\\n",
    "           X_train_rent, X_test_rent, y_train_ln_rent\n",
    "\n",
    "# 加载数据\n",
    "X_train_price, X_test_price, y_train_ln_price, \\\n",
    "X_train_rent, X_test_rent, y_train_ln_rent = load_feature_data('_pre_featured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ac071",
   "metadata": {},
   "source": [
    "#### b. Z-score 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test):\n",
    "    binary_cols = [col for col in X_train.columns \n",
    "                    if X_train[col].nunique(dropna=False) == 2 and \n",
    "                        X_train[col].min() == 0 and \n",
    "                        X_train[col].max() == 1]\n",
    "    numeric_cols = [col for col in X_train.columns if col not in binary_cols]\n",
    "\n",
    "    numeric_cols = [col for col in numeric_cols if col in X_train.columns]\n",
    "\n",
    "    # 初始化缩放器\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train_price, X_test_price = standardize_data(X_train_price, X_test_price)\n",
    "X_train_rent, X_test_rent = standardize_data(X_train_rent, X_test_rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 复制当前的训练与测试特征，后续建模直接使用这些变量\n",
    "train_price_features = X_train_price.copy()\n",
    "test_price_features = X_test_price.copy()\n",
    "train_rent_features = X_train_rent.copy()\n",
    "test_rent_features = X_test_rent.copy()\n",
    "\n",
    "# 确保测试集列顺序与训练集一致，缺失列用 0 填充\n",
    "test_price_features = test_price_features.reindex(columns=train_price_features.columns, fill_value=0)\n",
    "test_rent_features = test_rent_features.reindex(columns=train_rent_features.columns, fill_value=0)\n",
    "\n",
    "# 将对数目标转换为 Series 形式\n",
    "y_price_log = y_train_ln_price.squeeze()\n",
    "if isinstance(y_price_log, pd.DataFrame):\n",
    "    y_price_log = y_price_log.iloc[:, 0]\n",
    "y_rent_log = y_train_ln_rent.squeeze()\n",
    "if isinstance(y_rent_log, pd.DataFrame):\n",
    "    y_rent_log = y_rent_log.iloc[:, 0]\n",
    "\n",
    "# 重新加载测试集 ID，确保提交文件行顺序正确\n",
    "test_ids_price = pd.read_csv('ruc_Class25Q2_test_price.csv')['ID']\n",
    "test_ids_rent = pd.read_csv('ruc_Class25Q2_test_rent.csv')['ID']\n",
    "\n",
    "# 建立容器以便后续记录模型表现和提交路径\n",
    "price_metrics = {}\n",
    "rent_metrics = {}\n",
    "submission_registry = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a72d706",
   "metadata": {},
   "source": [
    "#### c. 初始化 Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "# Removed mean_squared_error import if not needed elsewhere\n",
    "import warnings\n",
    "import time # Import time for optional timing info\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "#统一设置 6 折交叉验证\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "# --- Scoring Functions (MAE and RMAE, positive and negative versions) ---\n",
    "def mae_original_scale_neg(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    # Handle potential overflows or NaNs after expm1, fallback to median/sensible value might be better if frequent\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred = np.clip(y_pred, 0, None) # Ensure non-negative predictions\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return -mae\n",
    "\n",
    "def mae_original_scale(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def rmae_original_scale_neg(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmae = np.sqrt(mae) if mae >= 0 else 0\n",
    "    return -rmae\n",
    "\n",
    "def rmae_original_scale(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=np.finfo(np.float64).max, neginf=0.0)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return np.sqrt(mae) if mae >= 0 else 0\n",
    "\n",
    "# Scorers for Scikit-learn (expect negative values as higher score = better)\n",
    "mae_neg_scorer = make_scorer(mae_original_scale_neg, greater_is_better=True)\n",
    "rmae_neg_scorer = make_scorer(rmae_original_scale_neg, greater_is_better=True)\n",
    "\n",
    "\n",
    "# --- Speed-Optimized Evaluation Function ---\n",
    "def evaluate_and_submit(model_pipeline, X, y_log, X_test, test_ids, target_tag, model_tag, param_grid=None, output_path=None):\n",
    "    \"\"\"\n",
    "    (Speed-Optimized Version using MAE and RMAE)\n",
    "    - Assumes model_pipeline includes preprocessing.\n",
    "    - Runs GridSearchCV on full training data (X) for CV scores.\n",
    "    - Trains a separate model on 80% split ONLY for IS/OOS metrics.\n",
    "    - Uses GridSearchCV's refit estimator for final predictions.\n",
    "    \"\"\"\n",
    "    start_time = time.time() # Start timing\n",
    "\n",
    "    # 1. Train/Validation Split (ONLY for calculating non-CV In-Sample and Holdout metrics)\n",
    "    X_train_split, X_valid, y_train_split, y_valid = train_test_split(\n",
    "        X, y_log, test_size=0.2, random_state=111\n",
    "    )\n",
    "\n",
    "    base_pipeline = clone(model_pipeline)\n",
    "    best_params = None\n",
    "    final_estimator_trained_on_full_X = None # Will hold the model for submission\n",
    "    cv_mae_mean, cv_mae_std = np.nan, np.nan\n",
    "    cv_rmae_mean, cv_rmae_std = np.nan, np.nan\n",
    "\n",
    "    if param_grid:\n",
    "        print(f\"--- Running GridSearchCV for {model_tag} ({target_tag}) on FULL training data ---\")\n",
    "        # 2a. GridSearch on the *entire* training dataset (X, y_log)\n",
    "        # refit='mae' ensures the best estimator (by MAE) is trained on X\n",
    "        search = GridSearchCV(\n",
    "            estimator=base_pipeline,\n",
    "            param_grid=param_grid,\n",
    "            scoring={'mae': mae_neg_scorer, 'rmae': rmae_neg_scorer},\n",
    "            refit='mae', # Refit the best model based on MAE score on the full data X\n",
    "            cv=kf,\n",
    "            n_jobs=8,\n",
    "            verbose=0,\n",
    "            return_train_score=False\n",
    "        )\n",
    "        search.fit(X, y_log) # Fit on the complete training data\n",
    "\n",
    "        best_params = search.best_params_\n",
    "        # This estimator is trained on FULL X with best params - ready for submission\n",
    "        final_estimator_trained_on_full_X = search.best_estimator_\n",
    "\n",
    "        # 3a. Extract CV scores directly from GridSearchCV results\n",
    "        best_idx = search.best_index_\n",
    "        # Scores are negative, take negative to get positive MAE/RMAE\n",
    "        cv_mae_mean = -search.cv_results_['mean_test_mae'][best_idx]\n",
    "        cv_mae_std = search.cv_results_['std_test_mae'][best_idx]\n",
    "        cv_rmae_mean = -search.cv_results_['mean_test_rmae'][best_idx] # Changed key\n",
    "        cv_rmae_std = search.cv_results_['std_test_rmae'][best_idx] # Changed key\n",
    "\n",
    "        # 4a. Train a *separate* model on the 80% split JUST for IS/OOS metrics\n",
    "        print(f\"--- Fitting model on 80% split for IS/OOS metrics ({model_tag}, {target_tag}) ---\")\n",
    "        estimator_for_metrics = clone(base_pipeline)\n",
    "        estimator_for_metrics.set_params(**best_params)\n",
    "        estimator_for_metrics.fit(X_train_split, y_train_split)\n",
    "\n",
    "    else:\n",
    "        # No parameter tuning\n",
    "        print(f\"--- Running Cross-Validation on FULL data for {model_tag} ({target_tag}) ---\")\n",
    "        # 2b. Run cross_validate on the *full* training set to get CV scores\n",
    "        cv_estimator = clone(base_pipeline)\n",
    "        cv_results = cross_validate(\n",
    "            cv_estimator,\n",
    "            X,\n",
    "            y_log,\n",
    "            cv=kf,\n",
    "            scoring={'mae': mae_neg_scorer, 'rmae': rmae_neg_scorer}, # Changed key\n",
    "            n_jobs=8,\n",
    "            return_train_score=False,\n",
    "            verbose=1\n",
    "        )\n",
    "        # Scores are negative, take negative to get positive MAE/RMAE\n",
    "        cv_mae_scores = -cv_results['test_mae']\n",
    "        cv_rmae_scores = -cv_results['test_rmae'] # Changed key\n",
    "        cv_mae_mean = cv_mae_scores.mean()\n",
    "        cv_mae_std = cv_mae_scores.std()\n",
    "        cv_rmae_mean = cv_rmae_scores.mean()\n",
    "        cv_rmae_std = cv_rmae_scores.std()\n",
    "\n",
    "        # 3b. Fit model on 80% split for IS/OOS metrics\n",
    "        print(f\"--- Fitting model on 80% split for IS/OOS metrics ({model_tag}, {target_tag}) ---\")\n",
    "        estimator_for_metrics = clone(base_pipeline)\n",
    "        estimator_for_metrics.fit(X_train_split, y_train_split)\n",
    "\n",
    "        # 4b. Fit final model on FULL data X for submission\n",
    "        print(f\"--- Fitting final model on FULL data for {model_tag} ({target_tag}) ---\")\n",
    "        final_estimator_trained_on_full_X = clone(base_pipeline)\n",
    "        final_estimator_trained_on_full_X.fit(X, y_log)\n",
    "\n",
    "\n",
    "    # 5. Calculate In-Sample and Holdout metrics using the model trained on the 80% split\n",
    "    print(f\"--- Calculating IS/OOS metrics for {model_tag} ({target_tag}) ---\")\n",
    "    y_train_pred_log = estimator_for_metrics.predict(X_train_split)\n",
    "    y_valid_pred_log = estimator_for_metrics.predict(X_valid)\n",
    "\n",
    "    metrics = {\n",
    "        'in_sample_mae': mae_original_scale(y_train_split, y_train_pred_log),\n",
    "        'in_sample_rmae': rmae_original_scale(y_train_split, y_train_pred_log), # Changed key\n",
    "        'holdout_mae': mae_original_scale(y_valid, y_valid_pred_log),\n",
    "        'holdout_rmae': rmae_original_scale(y_valid, y_valid_pred_log), # Changed key\n",
    "        'cv_mae_mean': cv_mae_mean,\n",
    "        'cv_mae_std': cv_mae_std,\n",
    "        'cv_rmae_mean': cv_rmae_mean, # Changed key\n",
    "        'cv_rmae_std': cv_rmae_std    # Changed key\n",
    "    }\n",
    "\n",
    "    # 6. Predict test set using the final model trained on FULL X\n",
    "    print(f\"--- Predicting on test data for {model_tag} ({target_tag}) ---\")\n",
    "    if final_estimator_trained_on_full_X is None:\n",
    "         # This case should ideally not happen if logic is correct\n",
    "         print(\"Error: Final estimator not trained on full data!\")\n",
    "         # Fallback: refit using best params if available, else base.\n",
    "         final_estimator_trained_on_full_X = clone(base_pipeline)\n",
    "         if best_params:\n",
    "             final_estimator_trained_on_full_X.set_params(**best_params)\n",
    "         final_estimator_trained_on_full_X.fit(X, y_log)\n",
    "\n",
    "    test_pred_log = final_estimator_trained_on_full_X.predict(X_test)\n",
    "    test_pred = np.expm1(test_pred_log)\n",
    "    # Apply safety checks after exponentiation\n",
    "    test_pred = np.nan_to_num(test_pred, nan=np.expm1(y_log.median()), posinf=np.finfo(np.float64).max, neginf=0.0) # Use median as fallback for NaN\n",
    "    test_pred = np.clip(test_pred, 0, None) # Ensure non-negative predictions\n",
    "    submission_df = pd.DataFrame({'ID': test_ids, 'Price': test_pred})\n",
    "\n",
    "    # 7. Save submission file\n",
    "    submission_path = None\n",
    "    if output_path is not None:\n",
    "        submission_path = Path(output_path)\n",
    "        submission_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        print(f\"Submission file saved to: {submission_path}\")\n",
    "\n",
    "    # 8. Store results and print summary\n",
    "    metrics['best_params'] = best_params\n",
    "    metrics['submission'] = str(submission_path) if submission_path else None\n",
    "\n",
    "    end_time = time.time() # End timing\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n=== {model_tag.upper()} - {target_tag.capitalize()} Final Results (Time: {elapsed_time:.2f}s) ===\")\n",
    "    if best_params:\n",
    "        print(f\"  Best Parameters: {best_params}\")\n",
    "    print(f\"  In-Sample -> MAE: {metrics['in_sample_mae']:.4f}, RMAE: {metrics['in_sample_rmae']:.4f}\")\n",
    "    print(f\"  Holdout   -> MAE: {metrics['holdout_mae']:.4f}, RMAE: {metrics['holdout_rmae']:.4f}\")\n",
    "    print(f\"  6-Fold CV -> MAE: {metrics['cv_mae_mean']:.4f} ± {metrics['cv_mae_std']:.4f}\")\n",
    "    print(f\"             RMAE: {metrics['cv_rmae_mean']:.4f} ± {metrics['cv_rmae_std']:.4f}\")\n",
    "    if submission_path:\n",
    "        print(f\"  Submission File: {submission_path}\")\n",
    "    print(\"=\" * (len(model_tag) + len(target_tag) + 30)) # Adjust separator length\n",
    "\n",
    "    # Return metrics, submission DataFrame, final trained model instance (on full X), best params\n",
    "    return metrics, submission_df, final_estimator_trained_on_full_X, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076bf7d",
   "metadata": {},
   "source": [
    "#### d. OLS模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Cross-Validation on FULL data for ols (price) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of   6 | elapsed:   49.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fitting model on 80% split for IS/OOS metrics (ols, price) ---\n",
      "--- Fitting final model on FULL data for ols (price) ---\n",
      "--- Calculating IS/OOS metrics for ols (price) ---\n",
      "--- Predicting on test data for ols (price) ---\n",
      "\n",
      "=== OLS - Price Final Results (Time: 76.11s) ===\n",
      "  In-Sample -> MAE: 349859.7959, RMAE: 591.4895\n",
      "  Holdout   -> MAE: 347307.4272, RMAE: 589.3279\n",
      "  6-Fold CV -> MAE: 353300.2521 ± 5359.2686\n",
      "             RMAE: 594.3736 ± 4.5037\n",
      "======================================\n",
      "--- Running Cross-Validation on FULL data for ols (rent) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of   6 | elapsed:   47.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fitting model on 80% split for IS/OOS metrics (ols, rent) ---\n",
      "--- Fitting final model on FULL data for ols (rent) ---\n",
      "--- Calculating IS/OOS metrics for ols (rent) ---\n",
      "--- Predicting on test data for ols (rent) ---\n",
      "\n",
      "=== OLS - Rent Final Results (Time: 72.00s) ===\n",
      "  In-Sample -> MAE: 89231.9096, RMAE: 298.7171\n",
      "  Holdout   -> MAE: 90369.2374, RMAE: 300.6148\n",
      "  6-Fold CV -> MAE: 90431.1436 ± 1406.2312\n",
      "             RMAE: 300.7086 ± 2.3380\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x108785bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x107ef1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def build_ols_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "\n",
    "price_metrics['OLS'], price_submission_df, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_ols_pipeline(),\n",
    "    X=train_price_features,\n",
    "    y_log=y_price_log,\n",
    "    X_test=test_price_features,\n",
    "    test_ids=test_ids_price,\n",
    "    target_tag='price',\n",
    "    model_tag='ols'\n",
    " )\n",
    "\n",
    "rent_metrics['OLS'], rent_submission_df, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_ols_pipeline(),\n",
    "    X=train_rent_features,\n",
    "    y_log=y_rent_log,\n",
    "    X_test=test_rent_features,\n",
    "    test_ids=test_ids_rent,\n",
    "    target_tag='rent',\n",
    "    model_tag='ols'\n",
    " )\n",
    "\n",
    "submission_ols_df = pd.concat([price_submission_df, rent_submission_df], ignore_index=True)\n",
    "submission_ols_path = Path('submission_ols_filtered.csv')\n",
    "submission_ols_df.to_csv(submission_ols_path, index=False)\n",
    "\n",
    "price_metrics['OLS']['submission'] = str(submission_ols_path)\n",
    "rent_metrics['OLS']['submission'] = str(submission_ols_path)\n",
    "submission_registry['OLS'] = str(submission_ols_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0f127",
   "metadata": {},
   "source": [
    "#### e. Ridge模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ba135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running GridSearchCV for ridge (price) on FULL training data ---\n",
      "--- Fitting model on 80% split for IS/OOS metrics (ridge, price) ---\n",
      "--- Calculating IS/OOS metrics for ridge (price) ---\n",
      "--- Predicting on test data for ridge (price) ---\n",
      "\n",
      "=== RIDGE - Price Final Results (Time: 1936.95s) ===\n",
      "  Best Parameters: {'model__alpha': np.float64(0.0001)}\n",
      "  In-Sample -> MAE: 349859.7974, RMAE: 591.4895\n",
      "  Holdout   -> MAE: 347221.3134, RMAE: 589.2549\n",
      "  6-Fold CV -> MAE: 353359.2267 ± 5300.8601\n",
      "             RMAE: 594.4236 ± 4.4550\n",
      "========================================\n",
      "--- Running GridSearchCV for ridge (rent) on FULL training data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x105051bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fitting model on 80% split for IS/OOS metrics (ridge, rent) ---\n",
      "--- Calculating IS/OOS metrics for ridge (rent) ---\n",
      "--- Predicting on test data for ridge (rent) ---\n",
      "\n",
      "=== RIDGE - Rent Final Results (Time: 1447.34s) ===\n",
      "  Best Parameters: {'model__alpha': np.float64(0.0001)}\n",
      "  In-Sample -> MAE: 89231.9100, RMAE: 298.7171\n",
      "  Holdout   -> MAE: 90397.7409, RMAE: 300.6622\n",
      "  6-Fold CV -> MAE: 90445.4053 ± 1407.7163\n",
      "             RMAE: 300.7323 ± 2.3399\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1036d1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106129bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10406dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1034d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104eadbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106231bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x102b1dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def build_ridge_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge())  # 注意：Ridge 通常不需要 max_iter\n",
    "    ])\n",
    "\n",
    "# Ridge 的 alpha 参数网格，可以沿用 Lasso 的范围\n",
    "ridge_param_grid = {\n",
    "    'model__alpha': np.logspace(-4, 2, 10)\n",
    "}\n",
    "\n",
    "# --- 训练和评估 'price' 模型 ---\n",
    "price_metrics['Ridge'], price_ridge_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_ridge_pipeline(),\n",
    "    X=train_price_features,\n",
    "    y_log=y_price_log,\n",
    "    X_test=test_price_features,\n",
    "    test_ids=test_ids_price,\n",
    "    target_tag='price',\n",
    "    model_tag='ridge',  # 标签改为 'ridge'\n",
    "    param_grid=ridge_param_grid\n",
    " )\n",
    "\n",
    "# --- 训练和评估 'rent' 模型 ---\n",
    "rent_metrics['Ridge'], rent_ridge_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_ridge_pipeline(),\n",
    "    X=train_rent_features,\n",
    "    y_log=y_rent_log,\n",
    "    X_test=test_rent_features,\n",
    "    test_ids=test_ids_rent,\n",
    "    target_tag='rent',\n",
    "    model_tag='ridge',  # 标签改为 'ridge'\n",
    "    param_grid=ridge_param_grid\n",
    " )\n",
    "\n",
    "# --- 合并并保存 Ridge 的提交文件 ---\n",
    "submission_ridge_df = pd.concat([price_ridge_submission, rent_ridge_submission], ignore_index=True)\n",
    "submission_ridge_path = Path('submission_ridge_50K.csv') # 文件名\n",
    "submission_ridge_df.to_csv(submission_ridge_path, index=False)\n",
    "\n",
    "# --- 更新 metrics 和 registry ---\n",
    "price_metrics['Ridge']['submission'] = str(submission_ridge_path)\n",
    "rent_metrics['Ridge']['submission'] = str(submission_ridge_path)\n",
    "submission_registry['Ridge'] = str(submission_ridge_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70008468",
   "metadata": {},
   "source": [
    "#### f. LASSO模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running GridSearchCV for lasso (price) on FULL training data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x11006dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x102cf1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fitting model on 80% split for IS/OOS metrics (lasso, price) ---\n",
      "--- Calculating IS/OOS metrics for lasso (price) ---\n",
      "--- Predicting on test data for lasso (price) ---\n",
      "\n",
      "=== LASSO - Price Final Results (Time: 815.10s) ===\n",
      "  Best Parameters: {'model__alpha': np.float64(0.01)}\n",
      "  In-Sample -> MAE: 505194.8406, RMAE: 710.7706\n",
      "  Holdout   -> MAE: 499899.4255, RMAE: 707.0357\n",
      "  6-Fold CV -> MAE: 504794.9683 ± 7785.1124\n",
      "             RMAE: 710.4682 ± 5.4695\n",
      "========================================\n",
      "--- Running GridSearchCV for lasso (rent) on FULL training data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10461dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fitting model on 80% split for IS/OOS metrics (lasso, rent) ---\n",
      "--- Calculating IS/OOS metrics for lasso (rent) ---\n",
      "--- Predicting on test data for lasso (rent) ---\n",
      "\n",
      "=== LASSO - Rent Final Results (Time: 525.86s) ===\n",
      "  Best Parameters: {'model__alpha': np.float64(0.01)}\n",
      "  In-Sample -> MAE: 125104.5866, RMAE: 353.7013\n",
      "  Holdout   -> MAE: 123339.3151, RMAE: 351.1970\n",
      "  6-Fold CV -> MAE: 125154.9308 ± 2055.9627\n",
      "             RMAE: 353.7605 ± 2.9015\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1056d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104fd9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10716dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x105855bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1056d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x11006dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106b41bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def build_lasso_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Lasso(max_iter=10000))\n",
    "    ])\n",
    "\n",
    "lasso_param_grid = {\n",
    "    'model__alpha': np.logspace(-2, 3, 11)\n",
    "}\n",
    "\n",
    "price_metrics['Lasso'], price_lasso_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_lasso_pipeline(),\n",
    "    X=train_price_features,\n",
    "    y_log=y_price_log,\n",
    "    X_test=test_price_features,\n",
    "    test_ids=test_ids_price,\n",
    "    target_tag='price',\n",
    "    model_tag='lasso',\n",
    "    param_grid=lasso_param_grid\n",
    " )\n",
    "\n",
    "rent_metrics['Lasso'], rent_lasso_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_lasso_pipeline(),\n",
    "    X=train_rent_features,\n",
    "    y_log=y_rent_log,\n",
    "    X_test=test_rent_features,\n",
    "    test_ids=test_ids_rent,\n",
    "    target_tag='rent',\n",
    "    model_tag='lasso',\n",
    "    param_grid=lasso_param_grid\n",
    " )\n",
    "\n",
    "submission_lasso_df = pd.concat([price_lasso_submission, rent_lasso_submission], ignore_index=True)\n",
    "submission_lasso_path = Path('submission_lasso_100K.csv')\n",
    "submission_lasso_df.to_csv(submission_lasso_path, index=False)\n",
    "\n",
    "price_metrics['Lasso']['submission'] = str(submission_lasso_path)\n",
    "rent_metrics['Lasso']['submission'] = str(submission_lasso_path)\n",
    "\n",
    "submission_registry['Lasso'] = str(submission_lasso_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running GridSearchCV for elasticnet (price) on FULL training data ---\n",
      "--- Fitting model on 80% split for IS/OOS metrics (elasticnet, price) ---\n",
      "--- Calculating IS/OOS metrics for elasticnet (price) ---\n",
      "--- Predicting on test data for elasticnet (price) ---\n",
      "\n",
      "=== ELASTICNET - Price Final Results (Time: 163.88s) ===\n",
      "  Best Parameters: {'model__alpha': np.float64(0.01), 'model__l1_ratio': np.float64(0.1)}\n",
      "  In-Sample -> MAE: 498152.9856, RMAE: 705.7995\n",
      "  Holdout   -> MAE: 492792.2497, RMAE: 701.9916\n",
      "  6-Fold CV -> MAE: 497664.1799 ± 7110.0579\n",
      "             RMAE: 705.4351 ± 5.0441\n",
      "=============================================\n",
      "--- Running GridSearchCV for elasticnet (rent) on FULL training data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x103155bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104a09bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104f29bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104a6dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x107bf9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1077d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1046d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10757dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x105455bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fitting model on 80% split for IS/OOS metrics (elasticnet, rent) ---\n",
      "--- Calculating IS/OOS metrics for elasticnet (rent) ---\n",
      "--- Predicting on test data for elasticnet (rent) ---\n",
      "\n",
      "=== ELASTICNET - Rent Final Results (Time: 105.71s) ===\n",
      "  Best Parameters: {'model__alpha': np.float64(0.01), 'model__l1_ratio': np.float64(0.1)}\n",
      "  In-Sample -> MAE: 113592.6136, RMAE: 337.0350\n",
      "  Holdout   -> MAE: 113207.2482, RMAE: 336.4628\n",
      "  6-Fold CV -> MAE: 113685.7270 ± 1678.8931\n",
      "             RMAE: 337.1639 ± 2.4972\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "def build_elasticnet_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', ElasticNet(max_iter=10000, l1_ratio=0.5))\n",
    "    ])\n",
    "\n",
    "elastic_param_grid = {\n",
    "    'model__alpha': np.logspace(-2, 0, 5),\n",
    "    'model__l1_ratio': np.linspace(0.1, 0.9, 9)\n",
    "}\n",
    "\n",
    "price_metrics['ElasticNet'], price_elastic_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_elasticnet_pipeline(),\n",
    "    X=train_price_features,\n",
    "    y_log=y_price_log,\n",
    "    X_test=test_price_features,\n",
    "    test_ids=test_ids_price,\n",
    "    target_tag='price',\n",
    "    model_tag='elasticnet',\n",
    "    param_grid=elastic_param_grid\n",
    " )\n",
    "\n",
    "rent_metrics['ElasticNet'], rent_elastic_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_elasticnet_pipeline(),\n",
    "    X=train_rent_features,\n",
    "    y_log=y_rent_log,\n",
    "    X_test=test_rent_features,\n",
    "    test_ids=test_ids_rent,\n",
    "    target_tag='rent',\n",
    "    model_tag='elasticnet',\n",
    "    param_grid=elastic_param_grid\n",
    " )\n",
    "\n",
    "submission_elastic_df = pd.concat([price_elastic_submission, rent_elastic_submission], ignore_index=True)\n",
    "submission_elastic_path = Path('submission_elasticnet.csv')\n",
    "submission_elastic_df.to_csv(submission_elastic_path, index=False)\n",
    "\n",
    "price_metrics['ElasticNet']['submission'] = str(submission_elastic_path)\n",
    "rent_metrics['ElasticNet']['submission'] = str(submission_elastic_path)\n",
    "submission_registry['ElasticNet'] = str(submission_elastic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5fd2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price 任务模型表现一览：\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "in_sample_mae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "in_sample_rmae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "holdout_mae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "holdout_rmae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_mae_mean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_mae_std",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_rmae_mean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_rmae_std",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "best_params",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ac25e810-b9ae-4abc-beb4-01569e6d089b",
       "rows": [
        [
         "OLS",
         "494872.7516888546",
         "703.4719267240553",
         "488144.08981626306",
         "698.6730922371801",
         "494495.675190924",
         "6838.155124606542",
         "703.1870276551612",
         "4.866141019739115",
         null,
         "submission_ols.csv"
        ],
        [
         "Ridge",
         "494877.27622207685",
         "703.4751425758246",
         "488191.1993532184",
         "698.7068049999359",
         "494499.74520559376",
         "6828.967144630039",
         "703.189967115396",
         "4.859563132831477",
         "{'model__alpha': np.float64(1.0)}",
         "submission_ridge.csv"
        ],
        [
         "Lasso",
         "532365.7144463232",
         "729.6339592195002",
         "529917.3159841638",
         "727.9541990978305",
         "531458.6004560258",
         "7199.91849466159",
         "728.9953153880779",
         "4.942731862314899",
         "{'model__alpha': np.float64(0.01)}",
         "submission_lasso.csv"
        ],
        [
         "ElasticNet",
         "498152.98564679734",
         "705.7995364455812",
         "492792.24974014005",
         "701.9916308191573",
         "497664.17994007416",
         "7110.057909902265",
         "705.4351399912927",
         "5.044125844934689",
         "{'model__alpha': np.float64(0.01), 'model__l1_ratio': np.float64(0.1)}",
         "submission_elasticnet.csv"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_sample_mae</th>\n",
       "      <th>in_sample_rmae</th>\n",
       "      <th>holdout_mae</th>\n",
       "      <th>holdout_rmae</th>\n",
       "      <th>cv_mae_mean</th>\n",
       "      <th>cv_mae_std</th>\n",
       "      <th>cv_rmae_mean</th>\n",
       "      <th>cv_rmae_std</th>\n",
       "      <th>best_params</th>\n",
       "      <th>submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>494872.751689</td>\n",
       "      <td>703.471927</td>\n",
       "      <td>488144.089816</td>\n",
       "      <td>698.673092</td>\n",
       "      <td>494495.675191</td>\n",
       "      <td>6838.155125</td>\n",
       "      <td>703.187028</td>\n",
       "      <td>4.866141</td>\n",
       "      <td>None</td>\n",
       "      <td>submission_ols.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>494877.276222</td>\n",
       "      <td>703.475143</td>\n",
       "      <td>488191.199353</td>\n",
       "      <td>698.706805</td>\n",
       "      <td>494499.745206</td>\n",
       "      <td>6828.967145</td>\n",
       "      <td>703.189967</td>\n",
       "      <td>4.859563</td>\n",
       "      <td>{'model__alpha': 1.0}</td>\n",
       "      <td>submission_ridge.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>532365.714446</td>\n",
       "      <td>729.633959</td>\n",
       "      <td>529917.315984</td>\n",
       "      <td>727.954199</td>\n",
       "      <td>531458.600456</td>\n",
       "      <td>7199.918495</td>\n",
       "      <td>728.995315</td>\n",
       "      <td>4.942732</td>\n",
       "      <td>{'model__alpha': 0.01}</td>\n",
       "      <td>submission_lasso.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>498152.985647</td>\n",
       "      <td>705.799536</td>\n",
       "      <td>492792.24974</td>\n",
       "      <td>701.991631</td>\n",
       "      <td>497664.17994</td>\n",
       "      <td>7110.05791</td>\n",
       "      <td>705.43514</td>\n",
       "      <td>5.044126</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__l1_ratio': 0.1}</td>\n",
       "      <td>submission_elasticnet.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            in_sample_mae in_sample_rmae    holdout_mae holdout_rmae  \\\n",
       "OLS         494872.751689     703.471927  488144.089816   698.673092   \n",
       "Ridge       494877.276222     703.475143  488191.199353   698.706805   \n",
       "Lasso       532365.714446     729.633959  529917.315984   727.954199   \n",
       "ElasticNet  498152.985647     705.799536   492792.24974   701.991631   \n",
       "\n",
       "              cv_mae_mean   cv_mae_std cv_rmae_mean cv_rmae_std  \\\n",
       "OLS         494495.675191  6838.155125   703.187028    4.866141   \n",
       "Ridge       494499.745206  6828.967145   703.189967    4.859563   \n",
       "Lasso       531458.600456  7199.918495   728.995315    4.942732   \n",
       "ElasticNet   497664.17994   7110.05791    705.43514    5.044126   \n",
       "\n",
       "                                               best_params  \\\n",
       "OLS                                                   None   \n",
       "Ridge                                {'model__alpha': 1.0}   \n",
       "Lasso                               {'model__alpha': 0.01}   \n",
       "ElasticNet  {'model__alpha': 0.01, 'model__l1_ratio': 0.1}   \n",
       "\n",
       "                           submission  \n",
       "OLS                submission_ols.csv  \n",
       "Ridge            submission_ridge.csv  \n",
       "Lasso            submission_lasso.csv  \n",
       "ElasticNet  submission_elasticnet.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rent 任务模型表现一览：\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "in_sample_mae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "in_sample_rmae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "holdout_mae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "holdout_rmae",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_mae_mean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_mae_std",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_rmae_mean",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cv_rmae_std",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "best_params",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "806aa488-8105-4ba8-8450-0048a7c2e7b1",
       "rows": [
        [
         "OLS",
         "112642.5585146976",
         "335.62264303037955",
         "112613.88955724922",
         "335.57993020627623",
         "112801.22728819784",
         "1432.7360673580654",
         "335.8521244266522",
         "2.1395808707087336",
         null,
         "submission_ols.csv"
        ],
        [
         "Ridge",
         "112642.55851627309",
         "335.62264303272667",
         "112613.88955768781",
         "335.5799302069297",
         "112801.22728937185",
         "1432.7360693448697",
         "335.8521244283813",
         "2.139580873673586",
         "{'model__alpha': np.float64(1e-05)}",
         "submission_ridge.csv"
        ],
        [
         "Lasso",
         "122182.78356143451",
         "349.5465399076846",
         "120699.32669847218",
         "347.4180863145616",
         "122076.93151737785",
         "2026.4858156687433",
         "349.38301733901704",
         "2.904946206378164",
         "{'model__alpha': np.float64(0.01)}",
         "submission_lasso.csv"
        ],
        [
         "ElasticNet",
         "113592.6136306749",
         "337.0350332393873",
         "113207.24817056704",
         "336.4628481282399",
         "113685.72696037136",
         "1678.8930841986898",
         "337.16389363870303",
         "2.49715573416859",
         "{'model__alpha': np.float64(0.01), 'model__l1_ratio': np.float64(0.1)}",
         "submission_elasticnet.csv"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_sample_mae</th>\n",
       "      <th>in_sample_rmae</th>\n",
       "      <th>holdout_mae</th>\n",
       "      <th>holdout_rmae</th>\n",
       "      <th>cv_mae_mean</th>\n",
       "      <th>cv_mae_std</th>\n",
       "      <th>cv_rmae_mean</th>\n",
       "      <th>cv_rmae_std</th>\n",
       "      <th>best_params</th>\n",
       "      <th>submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>112642.558515</td>\n",
       "      <td>335.622643</td>\n",
       "      <td>112613.889557</td>\n",
       "      <td>335.57993</td>\n",
       "      <td>112801.227288</td>\n",
       "      <td>1432.736067</td>\n",
       "      <td>335.852124</td>\n",
       "      <td>2.139581</td>\n",
       "      <td>None</td>\n",
       "      <td>submission_ols.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>112642.558516</td>\n",
       "      <td>335.622643</td>\n",
       "      <td>112613.889558</td>\n",
       "      <td>335.57993</td>\n",
       "      <td>112801.227289</td>\n",
       "      <td>1432.736069</td>\n",
       "      <td>335.852124</td>\n",
       "      <td>2.139581</td>\n",
       "      <td>{'model__alpha': 1e-05}</td>\n",
       "      <td>submission_ridge.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>122182.783561</td>\n",
       "      <td>349.54654</td>\n",
       "      <td>120699.326698</td>\n",
       "      <td>347.418086</td>\n",
       "      <td>122076.931517</td>\n",
       "      <td>2026.485816</td>\n",
       "      <td>349.383017</td>\n",
       "      <td>2.904946</td>\n",
       "      <td>{'model__alpha': 0.01}</td>\n",
       "      <td>submission_lasso.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>113592.613631</td>\n",
       "      <td>337.035033</td>\n",
       "      <td>113207.248171</td>\n",
       "      <td>336.462848</td>\n",
       "      <td>113685.72696</td>\n",
       "      <td>1678.893084</td>\n",
       "      <td>337.163894</td>\n",
       "      <td>2.497156</td>\n",
       "      <td>{'model__alpha': 0.01, 'model__l1_ratio': 0.1}</td>\n",
       "      <td>submission_elasticnet.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            in_sample_mae in_sample_rmae    holdout_mae holdout_rmae  \\\n",
       "OLS         112642.558515     335.622643  112613.889557    335.57993   \n",
       "Ridge       112642.558516     335.622643  112613.889558    335.57993   \n",
       "Lasso       122182.783561      349.54654  120699.326698   347.418086   \n",
       "ElasticNet  113592.613631     337.035033  113207.248171   336.462848   \n",
       "\n",
       "              cv_mae_mean   cv_mae_std cv_rmae_mean cv_rmae_std  \\\n",
       "OLS         112801.227288  1432.736067   335.852124    2.139581   \n",
       "Ridge       112801.227289  1432.736069   335.852124    2.139581   \n",
       "Lasso       122076.931517  2026.485816   349.383017    2.904946   \n",
       "ElasticNet   113685.72696  1678.893084   337.163894    2.497156   \n",
       "\n",
       "                                               best_params  \\\n",
       "OLS                                                   None   \n",
       "Ridge                              {'model__alpha': 1e-05}   \n",
       "Lasso                               {'model__alpha': 0.01}   \n",
       "ElasticNet  {'model__alpha': 0.01, 'model__l1_ratio': 0.1}   \n",
       "\n",
       "                           submission  \n",
       "OLS                submission_ols.csv  \n",
       "Ridge            submission_ridge.csv  \n",
       "Lasso            submission_lasso.csv  \n",
       "ElasticNet  submission_elasticnet.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_metrics(metrics_map):\n",
    "    df = pd.DataFrame(metrics_map).T\n",
    "    ordered_cols = [\n",
    "        'in_sample_mae',\n",
    "        'in_sample_rmae',\n",
    "        'holdout_mae',\n",
    "        'holdout_rmae',\n",
    "        'cv_mae_mean',\n",
    "        'cv_mae_std',\n",
    "        'cv_rmae_mean',\n",
    "        'cv_rmae_std',\n",
    "        'best_params',\n",
    "        'submission'\n",
    "    ]\n",
    "    return df[ordered_cols]\n",
    "\n",
    "price_summary = summarize_metrics(price_metrics)\n",
    "rent_summary = summarize_metrics(rent_metrics)\n",
    "\n",
    "print(\"Price 任务模型表现一览：\")\n",
    "display(price_summary)\n",
    "\n",
    "print(\"Rent 任务模型表现一览：\")\n",
    "display(rent_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b236504",
   "metadata": {},
   "source": [
    "#### g.非线性模型（附）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe120881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running GridSearchCV for random_forest (price) on FULL training data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104f2dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x111a6dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x111a6dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x107ef1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x107ef1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10576dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10576dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest model cell ---\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def build_rf_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(random_state=111))\n",
    "    ])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Run for price\n",
    "price_metrics['RandomForest'], price_rf_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_rf_pipeline(),\n",
    "    X=train_price_features,\n",
    "    y_log=y_price_log,\n",
    "    X_test=test_price_features,\n",
    "    test_ids=test_ids_price,\n",
    "    target_tag='price',\n",
    "    model_tag='random_forest',\n",
    "    param_grid=rf_param_grid,\n",
    "    output_path='submission_rf_price.csv'\n",
    ")\n",
    "\n",
    "# Run for rent\n",
    "rent_metrics['RandomForest'], rent_rf_submission, _, _ = evaluate_and_submit(\n",
    "    model_pipeline=build_rf_pipeline(),\n",
    "    X=train_rent_features,\n",
    "    y_log=y_rent_log,\n",
    "    X_test=test_rent_features,\n",
    "    test_ids=test_ids_rent,\n",
    "    target_tag='rent',\n",
    "    model_tag='random_forest',\n",
    "    param_grid=rf_param_grid,\n",
    "    output_path='submission_rf_rent.csv'\n",
    ")\n",
    "\n",
    "# Consolidate and save a single csv\n",
    "submission_rf_df = pd.concat([price_rf_submission, rent_rf_submission], ignore_index=True)\n",
    "submission_rf_path = Path('submission_rf.csv')\n",
    "submission_rf_df.to_csv(submission_rf_path, index=False)\n",
    "\n",
    "price_metrics['RandomForest']['submission'] = str(submission_rf_path)\n",
    "rent_metrics['RandomForest']['submission'] = str(submission_rf_path)\n",
    "submission_registry['RandomForest'] = str(submission_rf_path)\n",
    "\n",
    "print('RandomForest done — saved to', submission_rf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c41a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running GridSearchCV for lightgbm (price) on FULL training data ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Start training from score 14.264765\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n",
      "[LightGBM] [Info] Start training from score 14.262384\n",
      "[LightGBM] [Info] Start training from score 14.261743\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x11060dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3524\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3521\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.260149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3528\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.263147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3534\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.261743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3526\n",
      "[LightGBM] [Info] Number of data points in the train set: 86559, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.262384\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3529\n",
      "[LightGBM] [Info] Number of data points in the train set: 86560, number of used features: 218\n",
      "[LightGBM] [Info] Start training from score 14.264708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3541\n",
      "[LightGBM] [Info] Number of data points in the train set: 103871, number of used features: 219\n",
      "[LightGBM] [Info] Start training from score 14.262816\n",
      "--- Fitting model on 80% split for IS/OOS metrics (lightgbm, price) ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3527\n",
      "[LightGBM] [Info] Number of data points in the train set: 83096, number of used features: 217\n",
      "[LightGBM] [Info] Start training from score 14.264576\n",
      "--- Calculating IS/OOS metrics for lightgbm (price) ---\n",
      "--- Predicting on test data for lightgbm (price) ---\n",
      "Submission file saved to: submission_lgb_price.csv\n",
      "\n",
      "=== LIGHTGBM - Price Final Results (Time: 113.15s) ===\n",
      "  Best Parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 300, 'model__num_leaves': 64}\n",
      "  In-Sample -> MAE: 209269.4957, RMAE: 457.4598\n",
      "  Holdout   -> MAE: 244776.0835, RMAE: 494.7485\n",
      "  6-Fold CV -> MAE: 246662.3154 ± 1969.4221\n",
      "             RMAE: 496.6471 ± 1.9850\n",
      "  Submission File: submission_lgb_price.csv\n",
      "===========================================\n",
      "--- Running GridSearchCV for lightgbm (rent) on FULL training data ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1050bdbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106ec1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109445 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2879\n",
      "[LightGBM] [Info] Number of data points in the train set: 82415, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2872\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.960063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2875\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.957929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2881\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.960619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 82416, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.959197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1058d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1069b5bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2889\n",
      "[LightGBM] [Info] Number of data points in the train set: 98899, number of used features: 189\n",
      "[LightGBM] [Info] Start training from score 12.959108\n",
      "--- Fitting model on 80% split for IS/OOS metrics (lightgbm, rent) ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2876\n",
      "[LightGBM] [Info] Number of data points in the train set: 79119, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score 12.958874\n",
      "--- Calculating IS/OOS metrics for lightgbm (rent) ---\n",
      "--- Predicting on test data for lightgbm (rent) ---\n",
      "Submission file saved to: submission_lgb_rent.csv\n",
      "\n",
      "=== LIGHTGBM - Rent Final Results (Time: 97.30s) ===\n",
      "  Best Parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 300, 'model__num_leaves': 64}\n",
      "  In-Sample -> MAE: 61013.0525, RMAE: 247.0082\n",
      "  Holdout   -> MAE: 72429.9959, RMAE: 269.1282\n",
      "  6-Fold CV -> MAE: 71922.4660 ± 608.3112\n",
      "             RMAE: 268.1812 ± 1.1330\n",
      "  Submission File: submission_lgb_rent.csv\n",
      "==========================================\n",
      "LightGBM done — saved to submission_lightgbm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x107981bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x109259bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1040d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1059d9bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1023bdbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10697dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "# --- LightGBM model cell ---\n",
    "# This cell checks for lightgbm and, if available, builds a pipeline and runs evaluate_and_submit_faster\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception as _e:\n",
    "    LGBMRegressor = None\n",
    "    print('LightGBM not available in this environment:', _e)\n",
    "\n",
    "if LGBMRegressor is None:\n",
    "    print('\\n跳过 LightGBM：未检测到 lightgbm 包。要启用，请运行 `pip install lightgbm` 并重启 kernel，然后重新运行此单元。')\n",
    "else:\n",
    "    def build_lgb_pipeline():\n",
    "        return Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LGBMRegressor(random_state=111, n_jobs=8))\n",
    "        ])\n",
    "\n",
    "    lgb_param_grid = {\n",
    "        'model__n_estimators': [100, 300],\n",
    "        'model__num_leaves': [31, 64],\n",
    "        'model__learning_rate': [0.05, 0.1]\n",
    "    }\n",
    "\n",
    "    price_metrics['LightGBM'], price_lgb_submission, _, _ = evaluate_and_submit(\n",
    "        model_pipeline=build_lgb_pipeline(),\n",
    "        X=train_price_features,\n",
    "        y_log=y_price_log,\n",
    "        X_test=test_price_features,\n",
    "        test_ids=test_ids_price,\n",
    "        target_tag='price',\n",
    "        model_tag='lightgbm',\n",
    "        param_grid=lgb_param_grid,\n",
    "        output_path='submission_lgb_price.csv'\n",
    "    )\n",
    "\n",
    "    rent_metrics['LightGBM'], rent_lgb_submission, _, _ = evaluate_and_submit(\n",
    "        model_pipeline=build_lgb_pipeline(),\n",
    "        X=train_rent_features,\n",
    "        y_log=y_rent_log,\n",
    "        X_test=test_rent_features,\n",
    "        test_ids=test_ids_rent,\n",
    "        target_tag='rent',\n",
    "        model_tag='lightgbm',\n",
    "        param_grid=lgb_param_grid,\n",
    "        output_path='submission_lgb_rent.csv'\n",
    "    )\n",
    "\n",
    "    submission_lgb_df = pd.concat([price_lgb_submission, rent_lgb_submission], ignore_index=True)\n",
    "    submission_lgb_path = Path('submission_lightgbm.csv')\n",
    "    submission_lgb_df.to_csv(submission_lgb_path, index=False)\n",
    "\n",
    "    price_metrics['LightGBM']['submission'] = str(submission_lgb_path)\n",
    "    rent_metrics['LightGBM']['submission'] = str(submission_lgb_path)\n",
    "    submission_registry['LightGBM'] = str(submission_lgb_path)\n",
    "\n",
    "    print('LightGBM done — saved to', submission_lgb_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyKernel)",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
