{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab3e2a4",
   "metadata": {},
   "source": [
    "# 3-2 Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25fa4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- start navigate and filter ---\n",
      "click on: /html/body/div[4]/div[3]/div[2]/div[1]/ul/li[1]/ul/li[11]/a\n",
      "click on: /html/body/div[4]/div[3]/div[2]/div[1]/ul/li[2]/ul/li[34]/a\n",
      "filter accomplished\n",
      "\n",
      "--- working on page 1 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 2 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 3 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 4 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 5 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 6 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 7 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 8 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 9 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 10 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 11 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 12 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 13 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 14 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 15 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 16 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 17 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 18 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 19 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 20 ---\n",
      "已达到最大爬取页数 20。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# 二手房数据爬取\n",
    "DRIVER_PATH = r'C:\\Users\\lshte\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe'\n",
    "MAX_PAGES = 20\n",
    "\n",
    "all_house_data = []\n",
    "\n",
    "def initialize_driver():\n",
    "    Opt = webdriver.ChromeOptions()\n",
    "    Opt.add_argument('start-maximized')\n",
    "    s = Service(DRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service = s, options=Opt)\n",
    "    driver.implicitly_wait(10) # 隐式等待，增加元素加载的容错性\n",
    "    return driver\n",
    "\n",
    "def click_element(driver, by, value, timeout=20):\n",
    "    try:\n",
    "        # 显式等待：直到元素可点击\n",
    "        element = WebDriverWait(driver, timeout).until(\n",
    "            EC.element_to_be_clickable((by, value))\n",
    "        )\n",
    "        element.click()\n",
    "        time.sleep(1)\n",
    "        print(f\"click on: {value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "\n",
    "def navigate_and_filter(driver):\n",
    "    print(\"--- start navigate and filter ---\")\n",
    "    \n",
    "    city_pinyin = 'tj' # 天津\n",
    "    target_url = f'https://{city_pinyin}.esf.fang.com/'\n",
    "    driver.get(target_url)\n",
    "    # 滨海新区\n",
    "    area_selector = '/html/body/div[4]/div[3]/div[2]/div[1]/ul/li[1]/ul/li[11]/a'\n",
    "    click_element(driver, By.XPATH, area_selector)\n",
    "    time.sleep(2)\n",
    "    # 中新生态城\n",
    "    sub_area_selector = '/html/body/div[4]/div[3]/div[2]/div[1]/ul/li[2]/ul/li[34]/a'\n",
    "    click_element(driver, By.XPATH, sub_area_selector)\n",
    "\n",
    "    print(\"filter accomplished\")\n",
    "    time.sleep(2)\n",
    "\n",
    "def extract_house_data(driver, page_num):\n",
    "    print(f\"\\n--- working on page {page_num} ---\")\n",
    "    house_list_items = driver.find_elements(By.CSS_SELECTOR, '.shop_list_4 > dl')\n",
    "    \n",
    "    if not house_list_items:\n",
    "        print(\"no house found on this page.\")\n",
    "        return False\n",
    "\n",
    "    for item in house_list_items:\n",
    "        try:\n",
    "            # 提取面积\n",
    "            p_element = item.find_element(By.CSS_SELECTOR, 'p.tel_shop')\n",
    "            full_info_text = p_element.text\n",
    "            index_end = full_info_text.rfind('㎡')\n",
    "            index_end -= 1\n",
    "            area_text = ''\n",
    "            for i in range(index_end, -1, -1):\n",
    "                char = full_info_text[i]\n",
    "                if char.isspace() or char == '|':\n",
    "                    break\n",
    "                elif char.isdigit() or char == '.':\n",
    "                    area_text = char + area_text\n",
    "                else :\n",
    "                    break\n",
    "            area_num = float(area_text)\n",
    "\n",
    "            # 提取总价\n",
    "            total_price_element = item.find_element(By.CSS_SELECTOR, 'span.red')\n",
    "            total_price = total_price_element.text.replace('万', '').strip()\n",
    "            total_price = float(total_price)\n",
    "            \n",
    "            # 提取单价\n",
    "            unit_price_element = item.find_element(By.CSS_SELECTOR, 'dd.price_right > span:nth-child(2)')\n",
    "            unit_price = unit_price_element.text.replace('元/㎡', '').strip()\n",
    "            unit_price = float(unit_price)\n",
    "            \n",
    "            all_house_data.append({\n",
    "                '面积(㎡)': area_num,\n",
    "                '总价(万)': total_price,\n",
    "                '单价(元/㎡)': unit_price\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"error occurs at extracting house data: {e}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def find_next_page(driver):\n",
    "    try:\n",
    "        # 查找 class 为 'next' 的链接，并且它必须是可点击的\n",
    "        next_page_link = driver.find_element(By.XPATH, '//a[text()=\"下一页\"]')\n",
    "        \n",
    "        if 'nohref' in next_page_link.get_attribute('class'):\n",
    "            print(\"已到达最后一页或下一页链接不可用。\")\n",
    "            return False\n",
    "            \n",
    "        # 使用点击函数\n",
    "        click_element(driver, By.XPATH, '//a[text()=\"下一页\"]')\n",
    "        return True\n",
    "    except:\n",
    "        print(\"未找到下一页按钮，爬取结束。\")\n",
    "        return False\n",
    "\n",
    "\n",
    "driver = initialize_driver()\n",
    "\n",
    "navigate_and_filter(driver)\n",
    "\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    # 提取当前页数据\n",
    "    success = extract_house_data(driver, page)\n",
    "    if not success:\n",
    "        break # 如果当前页提取失败，则退出\n",
    "\n",
    "    # 检查是否需要翻页\n",
    "    if page < MAX_PAGES:\n",
    "        if not find_next_page(driver):\n",
    "            break # 如果找不到下一页，则退出\n",
    "    else:\n",
    "        print(f\"已达到最大爬取页数 {MAX_PAGES}。\")\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10a65bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- result display ---\n",
      "   面积(㎡)  总价(万)  单价(元/㎡)\n",
      "0   80.0   60.0   7500.0\n",
      "1   89.0   70.0   7865.0\n",
      "2  125.0  100.0   8000.0\n",
      "3   87.0   56.0   6436.0\n",
      "4  108.0   70.0   6481.0\n",
      "\n",
      "data has been saved to : Tianjin_Zhongxin_esf_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 输出文件\n",
    "OUTPUT_FILE = 'Tianjin_Zhongxin_esf_data.xlsx'\n",
    "if all_house_data:\n",
    "    df = pd.DataFrame(all_house_data)\n",
    "    print(\"\\n--- result display ---\")\n",
    "    print(df.head())\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\ndata has been saved to : {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"no data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6161bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- start navigate and filter ---\n",
      "click on: /html/body/div[4]/div[2]/div[2]/div/dl[1]/dd/a[12]\n",
      "click on: /html/body/div[4]/div[2]/div[2]/div[1]/div/a[35]\n",
      "filter accomplished\n",
      "\n",
      "--- working on page 1 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 2 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 3 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 4 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 5 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 6 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 7 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 8 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 9 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 10 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 11 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 12 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 13 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 14 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 15 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 16 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 17 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 18 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 19 ---\n",
      "click on: //a[text()=\"下一页\"]\n",
      "\n",
      "--- working on page 20 ---\n",
      "已达到最大爬取页数 20。\n"
     ]
    }
   ],
   "source": [
    "# 租房数据爬取\n",
    "def navigate_and_filter_zu(driver):\n",
    "    print(\"--- start navigate and filter ---\")\n",
    "    \n",
    "    city_pinyin = 'tj' # 天津\n",
    "    target_url = f'https://{city_pinyin}.zu.fang.com/'\n",
    "    driver.get(target_url)\n",
    "    # 滨海新区\n",
    "    area_selector = '/html/body/div[4]/div[2]/div[2]/div/dl[1]/dd/a[12]'\n",
    "    click_element(driver, By.XPATH, area_selector)\n",
    "    time.sleep(2)\n",
    "    # 中新生态城\n",
    "    sub_area_selector = '/html/body/div[4]/div[2]/div[2]/div[1]/div/a[35]'\n",
    "    click_element(driver, By.XPATH, sub_area_selector)\n",
    "\n",
    "    print(\"filter accomplished\")\n",
    "    time.sleep(2)\n",
    "\n",
    "def extract_house_data_zu(driver, page_num):\n",
    "    print(f\"\\n--- working on page {page_num} ---\")\n",
    "    house_list_items = driver.find_elements(By.CSS_SELECTOR, '.houseList > dl')\n",
    "    \n",
    "    if not house_list_items:\n",
    "        print(\"no house found on this page.\")\n",
    "        return False\n",
    "\n",
    "    for item in house_list_items:\n",
    "        try:\n",
    "            # 提取面积\n",
    "            p_element = item.find_element(By.CSS_SELECTOR, 'p.font15.mt12.bold')\n",
    "            full_info_text = p_element.text\n",
    "            index_end = full_info_text.rfind('㎡')\n",
    "            index_end -= 1\n",
    "            area_text = ''\n",
    "            for i in range(index_end, -1, -1):\n",
    "                char = full_info_text[i]\n",
    "                if char.isspace() or char == '|':\n",
    "                    break\n",
    "                elif char.isdigit() or char == '.':\n",
    "                    area_text = char + area_text\n",
    "                else :\n",
    "                    break\n",
    "            area_num = float(area_text)\n",
    "            \n",
    "            # 提取单价\n",
    "            unit_price_element = item.find_element(By.CSS_SELECTOR, 'p.mt5.alingC > span.price')\n",
    "            unit_price = unit_price_element.text.strip()\n",
    "            unit_price = float(unit_price)\n",
    "            \n",
    "            all_house_data.append({\n",
    "                '面积(㎡)': area_num,\n",
    "                '租房单价(元/月)': unit_price\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"error occurs at extracting house data: {e}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "driver = initialize_driver()\n",
    "\n",
    "navigate_and_filter_zu(driver)\n",
    "\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    # 提取当前页数据\n",
    "    success = extract_house_data_zu(driver, page)\n",
    "    if not success:\n",
    "        break # 如果当前页提取失败，则退出\n",
    "\n",
    "    # 检查是否需要翻页\n",
    "    if page < MAX_PAGES:\n",
    "        if not find_next_page(driver):\n",
    "            break # 如果找不到下一页，则退出\n",
    "    else:\n",
    "        print(f\"已达到最大爬取页数 {MAX_PAGES}。\")\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d5a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- result display ---\n",
      "   面积(㎡)  总价(万)  单价(元/㎡)  租房单价(元/月)\n",
      "0   80.0   60.0   7500.0        NaN\n",
      "1   89.0   70.0   7865.0        NaN\n",
      "2  125.0  100.0   8000.0        NaN\n",
      "3   87.0   56.0   6436.0        NaN\n",
      "4  108.0   70.0   6481.0        NaN\n",
      "\n",
      "data has been saved to : Tianjin_Zhongxin_zu_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "#输出文件\n",
    "OUTPUT_FILE = 'Tianjin_Zhongxin_zu_data.xlsx'\n",
    "\n",
    "if all_house_data:\n",
    "    df = pd.DataFrame(all_house_data)\n",
    "    print(\"\\n--- result display ---\")\n",
    "    print(df.head())\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\ndata has been saved to : {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"no data to save.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Selenium_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
