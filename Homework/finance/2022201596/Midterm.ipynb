{"cells":[{"cell_type":"markdown","metadata":{"id":"BAEE51CAC7804597B5D0E06FB209A34B","trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":true,"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"6901f9297a0e19b0ec3ce988"},"source":"## 欢迎进入 Notebook  \n\n这里你可以编写代码，文档  \n\n### 关于文件目录  \n\n\n**project**：project 目录是本项目的工作空间，可以把将项目运行有关的所有文件放在这里，目录中文件的增、删、改操作都会被保留  \n\n\n**input**：input 目录是数据集的挂载位置，所有挂载进项目的数据集都在这里，未挂载数据集时 input 目录被隐藏  \n\n\n**temp**：temp 目录是临时磁盘空间，训练或分析过程中产生的不必要文件可以存放在这里，目录中的文件不会保存  \n"},{"cell_type":"code","metadata":{"id":"3999AF60F2754394BA5F1A3C9AD8354B","trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"scrolled":false,"notebookId":"6901f9297a0e19b0ec3ce988"},"source":"# 查看个人持久化工作区文件\n!ls /home/mw/project/","outputs":[{"output_type":"stream","name":"stdout","text":"outputs\r\n"}],"execution_count":19},{"cell_type":"code","metadata":{"id":"F8B70CD7BC87457F9CB6221D1CE5EDBB","trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"scrolled":false,"notebookId":"6901f9297a0e19b0ec3ce988"},"source":"# 查看当前挂载的数据集目录\n!ls /home/mw/input/","outputs":[{"output_type":"stream","name":"stdout","text":"folder1003\r\n"}],"execution_count":20},{"cell_type":"code","metadata":{"id":"73321AC4AB2643129B0C3A08F8D405F4","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell1 环境与依赖\n\"\"\"\n统一设置并行线程数与临时目录，避免小机器/在线环境爆线程或 /tmp 爆掉。\n定义一个小工具函数 ensure()：检测→若缺失则静默安装→再导入依赖包，保证运行环境自\n洽。\n载入建模所需的科学计算栈与 scikit-learn 模块（含 Pipeline/预处理/线性模型/KMeans/网格\n搜索等）。\n（可选）导入文本特征用到的 TF-IDF 与 SVD。\n关闭警告以净化日志，并定义一个简单的 log() 打印函数（带时间戳，立刻刷新）。\n\"\"\"\n# ================== Cell1 环境与依赖 ==================\nimport os, sys, time, json, warnings, re, importlib, subprocess\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Dict, Optional, Set\n\n# 线程与临时目录\nos.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\nos.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\nos.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\nos.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\nos.makedirs(\"/home/mw/project/outputs/joblib_tmp\", exist_ok=True)\nos.environ.setdefault(\"JOBLIB_TEMP_FOLDER\", \"/home/mw/project/outputs/joblib_tmp\")\n\n# 依赖自检与按需安装\ndef ensure(pkg, pip_name=None, version=None, optional=False):\n    name = pip_name or pkg\n    try:\n        importlib.import_module(pkg)\n        print(f\"[deps] {pkg} OK\")\n    except Exception as e:\n        if optional:\n            print(f\"[deps] {pkg} optional: {e}\")\n            return\n        nv = f\"{name}=={version}\" if version else name\n        print(f\"[deps] installing {nv} ...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", nv])\n        importlib.import_module(pkg)\n        print(f\"[deps] {pkg} installed.\")\n\nensure(\"numpy\"); ensure(\"pandas\"); ensure(\"joblib\")\nensure(\"sklearn\", pip_name=\"scikit-learn\")\nensure(\"scipy\")\n\nimport numpy as np\nimport pandas as pd\nfrom joblib import dump\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, MaxAbsScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances\n\n# 文本模块\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nwarnings.filterwarnings(\"ignore\")\nlog = lambda m: print(time.strftime(\"[%H:%M:%S]\"), m, flush=True)\n","outputs":[{"output_type":"stream","name":"stdout","text":"[deps] numpy OK\n[deps] pandas OK\n[deps] joblib OK\n[deps] sklearn OK\n[deps] scipy OK\n"}],"execution_count":21},{"cell_type":"code","metadata":{"id":"87C3980023334AF681EF6A0ADC0A2855","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell2 基础配置\n@dataclass\nclass CFG:\n    TRACK: str = \"price\"  # \"price\" or \"rent\"\n    TARGET: str = \"Price\"\n    TRAIN_PATH: str = \"\"\n    TEST_PATH: str = \"\"\n    TEST_SIZE: float = 0.2\n    SEEDS: Tuple[int, ...] = (111, 222, 333)\n\n    # 目标与裁剪\n    USE_LOG_TARGET: bool = True\n    WINSOR_LOWER: float = 0.01\n    WINSOR_UPPER: float = 0.99\n\n    # 类别/TE\n    RARE_MIN_COUNT: int = 100\n    HIGH_CARD_THRESH: int = 20\n    TE_SMOOTH: int = 90\n\n    # 地理聚类\n    GEO_K: int = 80\n    GEO_DIST_TOPK: int = 2\n\n    # 文本（默认开）\n    USE_TEXT_FEATURES: bool = True\n    TEXT_COLS: Tuple[str, ...] = (\"房屋优势\",\"核心卖点\",\"户型介绍\",\"周边配套\",\"交通出行\",\"客户反馈\")\n    TEXT_SVD_N: int = 5\n    TEXT_MAX_FEATURES: int = 10000\n    TEXT_MIN_DF: int = 3\n\n    # 交互\n    ENABLE_INTERACTION_TE_AGE: bool = True\n    ENABLE_INTERACTION_DIST_AREA: bool = True\n\n    # 数值特征筛选（log域）\n    FS_BY_CORR: bool = True\n    MIN_ABS_CORR_Y: float = 0.02\n    FS_BY_CORR_PAIR: bool = True\n    MAX_ABS_CORR_FEAT: float = 0.999\n\n    OUT_DIR: str = \"/home/mw/project/outputs\"\n    SAVE_MODEL_PATH: str = \"\"\n    PREDICT_CSV: str = \"\"\n    METRICS_JSON: str = \"\"\n\ndef set_paths(cfg: CFG) -> CFG:\n    if cfg.TRACK == \"rent\":\n        cfg.TRAIN_PATH = \"/home/mw/input/folder1003/ruc_Class25Q2_train_rent.csv\"\n        cfg.TEST_PATH  = \"/home/mw/input/folder1003/ruc_Class25Q2_test_rent.csv\"\n    else:\n        cfg.TRAIN_PATH = \"/home/mw/input/folder1003/ruc_Class25Q2_train_price.csv\"\n        cfg.TEST_PATH  = \"/home/mw/input/folder1003/ruc_Class25Q2_test_price.csv\"\n    os.makedirs(cfg.OUT_DIR, exist_ok=True)\n    s = f\"_{cfg.TRACK}\"\n    cfg.SAVE_MODEL_PATH = os.path.join(cfg.OUT_DIR, f\"best_model{s}.joblib\")\n    cfg.PREDICT_CSV     = os.path.join(cfg.OUT_DIR, f\"predictions{s}.csv\")\n    cfg.METRICS_JSON    = os.path.join(cfg.OUT_DIR, f\"metrics{s}.json\")\n    print(\"[paths] OUT_DIR =\", cfg.OUT_DIR)\n    return cfg","outputs":[],"execution_count":22},{"cell_type":"code","metadata":{"id":"7C2D2B3E590B45DF927AAB1F10ABEDFC","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell3 工具\n_num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n\ndef extract_numeric(s):\n    if pd.isna(s): return np.nan\n    m = _num_pat.findall(str(s))\n    return float(m[0]) if m else np.nan\n\ndef winsorize_y(y, lower_q=0.01, upper_q=0.99):\n    lo, hi = np.quantile(y, [lower_q, upper_q])\n    return np.clip(y, lo, hi)\n\ndef _to_target(y, use_log):\n    return np.log1p(y) if use_log else y\n\ndef _from_target(y, use_log):\n    return np.expm1(y) if use_log else y\n\ndef numeric_postprocess(df: pd.DataFrame):\n    # 物业费/面积\n    if {\"物 业 费_num\",\"面积_num\"}.issubset(df.columns):\n        a = pd.to_numeric(df[\"面积_num\"], errors=\"coerce\").replace(0, np.nan)\n        fee = pd.to_numeric(df[\"物 业 费_num\"], errors=\"coerce\")\n        df[\"propfee_per_m2\"] = (fee / a).replace([np.inf, -np.inf], np.nan)\n    # 房龄\n    if \"年份\" in df.columns:\n        now_y = pd.Timestamp.now().year\n        yrs = pd.to_numeric(df[\"年份\"], errors=\"coerce\")\n        df[\"house_age\"] = (now_y - yrs).clip(lower=0)\n    return df\n\ndef load_raw(cfg: CFG):\n    log(\"[1/10] 读取数据\")\n    tr = pd.read_csv(cfg.TRAIN_PATH, low_memory=False)\n    te = pd.read_csv(cfg.TEST_PATH, low_memory=False)\n    # 强转为数值\n    for c in [\"lon\",\"lat\",\"年份\",\"容 积 率\",\"停车位\"]:\n        if c in tr.columns: tr[c] = pd.to_numeric(tr[c], errors=\"coerce\")\n        if c in te.columns: te[c] = pd.to_numeric(te[c], errors=\"coerce\")\n    # 抽取字符串里的数值\n    for c in [\"建筑面积\",\"套内面积\",\"物 业 费\",\"绿 化 率\",\"面积\",\"停车费用\"]:\n        if c in tr.columns: tr[c+\"_num\"] = tr[c].apply(extract_numeric)\n        if c in te.columns: te[c+\"_num\"] = te[c].apply(extract_numeric)\n    tr = numeric_postprocess(tr); te = numeric_postprocess(te)\n    print(f\"[train] shape={tr.shape}, dtypes summary: {tr.dtypes.value_counts().to_dict()}\")\n    print(f\"[test]  shape={te.shape}, dtypes summary: {te.dtypes.value_counts().to_dict()}\")\n    return tr, te\n\n\n# ================== 常用派生封装 ==================\ndef apply_common_feature_enrichment(df: pd.DataFrame) -> pd.DataFrame:\n    df = parse_layout(df, \"房屋户型\")\n    df = add_orientation_features(df, \"房屋朝向\")\n    df = add_time_features(df, \"交易时间\", \"上次交易\")\n    # 楼层解析（支持关键词+总层数）\n    for floor_col in [\"所在楼层\", \"楼层\", \"楼层情况\"]:\n        if floor_col in df.columns:\n            df = add_floor_features_simple(df, col=floor_col)\n            break\n    return df\n\n# ================== 户型 & 朝向 ==================\ndef parse_layout(df: pd.DataFrame, col=\"房屋户型\"):\n    if col not in df.columns: return df\n    s = df[col].astype(str).fillna(\"\")\n    beds, halls, baths = [], [], []\n    pats_bed = [r\"(\\d+)\\s*(?:室|房|卧)\"]\n    pats_hall = [r\"(\\d+)\\s*(?:厅)\"]\n    pats_bath = [r\"(\\d+)\\s*(?:卫|厕|bath)\"]\n    for s_i in s:\n        def _get(pats):\n            for p in pats:\n                m = re.search(p, s_i)\n                if m: return int(m.group(1))\n            return np.nan\n        beds.append(_get(pats_bed)); halls.append(_get(pats_hall)); baths.append(_get(pats_bath))\n    df[\"bedrooms\"] = pd.to_numeric(beds, errors=\"coerce\")\n    df[\"livingrooms\"] = pd.to_numeric(halls, errors=\"coerce\")\n    df[\"bathrooms\"] = pd.to_numeric(baths, errors=\"coerce\")\n    return df\n\nDIR2ANG = {\"东\":0,\"东北\":45,\"北\":90,\"西北\":135,\"西\":180,\"西南\":225,\"南\":270,\"东南\":315}\n\ndef orientation_sincos(text: str):\n    if pd.isna(text) or not str(text).strip():\n        return (np.nan, np.nan)\n    t = str(text); hits=[]\n    for k in [\"东北\",\"西北\",\"西南\",\"东南\"]:\n        if k in t: hits.append(DIR2ANG[k])\n    for k in [\"东\",\"南\",\"西\",\"北\"]:\n        if k in t: hits.append(DIR2ANG[k])\n    if not hits:\n        if \"南北\" in t: hits = [DIR2ANG[\"南\"], DIR2ANG[\"北\"]]\n        elif \"东西\" in t: hits = [DIR2ANG[\"东\"], DIR2ANG[\"西\"]]\n        else: return (np.nan, np.nan)\n    angs = np.deg2rad(np.array(hits, dtype=float))\n    return (np.sin(angs).mean(), np.cos(angs).mean())\n\ndef add_orientation_features(df: pd.DataFrame, col=\"房屋朝向\"):\n    if col not in df.columns: return df\n    sin_list, cos_list = [], []\n    for v in df[col].astype(str).fillna(\"\"):\n        s, c = orientation_sincos(v)\n        sin_list.append(s); cos_list.append(c)\n    df[\"ori_sin\"] = pd.to_numeric(sin_list, errors=\"coerce\")\n    df[\"ori_cos\"] = pd.to_numeric(cos_list, errors=\"coerce\")\n    return df\n\n# ================== 时间差 ==================\ndef add_time_features(df: pd.DataFrame, trade_col=\"交易时间\", last_trade_col=\"上次交易\"):\n    today = pd.Timestamp.today().normalize()\n    if trade_col in df.columns:\n        dt = pd.to_datetime(df[trade_col], errors=\"coerce\")\n        df[\"days_since_trade\"] = (today - dt).dt.days.astype(\"float\")\n        df[\"trade_month\"] = dt.dt.month.astype(\"float\")\n    if last_trade_col in df.columns and \"days_since_trade\" in df.columns:\n        dt2 = pd.to_datetime(df[last_trade_col], errors=\"coerce\")\n        df[\"days_since_last_trade\"] = (today - dt2).dt.days.astype(\"float\")\n        df[\"interval_trade_last\"] = (pd.to_numeric(df[\"days_since_last_trade\"], errors=\"coerce\")\n                                     - pd.to_numeric(df[\"days_since_trade\"], errors=\"coerce\"))\n    return df\n\n# ================== 楼层解析（简化版） ==================\n_re_total_simple = re.compile(r\"共\\s*(\\d+)\\s*层\")\n\ndef _parse_total_simple(text: str):\n    if not isinstance(text, str): return np.nan\n    m = _re_total_simple.search(text)\n    if not m: return np.nan\n    tot = int(m.group(1))\n    return tot if tot > 0 else np.nan\n\ndef _estimate_level_by_keyword(text: str, total: float):\n    t = str(text)\n    if '地下室' in t: return 0\n    if '底层' in t: return 1\n    if '顶层' in t and pd.notna(total): return int(total)\n    if pd.notna(total):\n        if '低楼层' in t: return max(1, int(round(0.2 * total)))\n        if '中楼层' in t: return max(1, int(round(0.5 * total)))\n        if '高楼层' in t: return max(1, int(round(0.8 * total)))\n    return np.nan\n\ndef add_floor_features_simple(df: pd.DataFrame, col=\"所在楼层\"):\n    if col not in df.columns: return df\n    s = df[col].astype(str)\n    total = s.apply(_parse_total_simple)\n    level = [_estimate_level_by_keyword(txt, tot) for txt, tot in zip(s, total)]\n    level = pd.to_numeric(level, errors=\"coerce\")\n    total = pd.to_numeric(total, errors=\"coerce\")\n    level = np.where(np.isnan(level) | np.isnan(total), level, np.minimum(level, total))\n    is_basement = s.str.contains(\"地下室\", regex=False)\n    is_top = (s.str.contains(\"顶层\", regex=False)) | ((pd.Series(level)==pd.Series(total)) & pd.notna(level) & pd.notna(total))\n    df[\"floor_total\"] = total\n    df[\"floor_level\"] = level\n    df[\"is_top_floor\"] = is_top.astype(int)\n    df[\"is_basement\"] = is_basement.astype(int)\n    return df\n","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{"id":"EBF089B62C724028AE210E28F2117D65","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell4\n\n# ================== 分段线性 ==================\ndef add_piecewise(df: pd.DataFrame, col: str, cuts: List[float]):\n    if col not in df.columns: return\n    x = pd.to_numeric(df[col], errors=\"coerce\")\n    for t in cuts:\n        df[f\"{col}_gt_{t}\"] = np.clip(x - t, 0, None)\n\ndef add_piecewise_map(df_list, piecewise_map: Dict[str, List[float]]):\n    for df in df_list:\n        for col, cuts in (piecewise_map or {}).items():\n            if col in df.columns:\n                add_piecewise(df, col, cuts)\n\n# ================== 地理 KMeans ==================\ndef fit_geo_kmeans(full_train_df: pd.DataFrame, n_clusters=80, seed=111):\n    if not {\"lon\",\"lat\"}.issubset(full_train_df.columns): return None\n    good = full_train_df[[\"lon\",\"lat\"]].dropna()\n    if len(good) < max(2, n_clusters): return None\n    km = KMeans(n_clusters=n_clusters, random_state=seed, n_init=10)\n    km.fit(good.values)\n    return km\n\ndef assign_geo_cluster(df: pd.DataFrame, km: Optional[KMeans]):\n    if km is None or not {\"lon\",\"lat\"}.issubset(df.columns): return df\n    filler = df[[\"lon\",\"lat\"]].mean()\n    lab = km.predict(df[[\"lon\",\"lat\"]].fillna(filler).values)\n    df[\"geo_cluster\"] = lab.astype(\"int64\")\n    return df\n\ndef add_geo_center_dists(df: pd.DataFrame, km: Optional[KMeans], topk=2):\n    if km is None or not {\"lon\",\"lat\"}.issubset(df.columns): return df\n    centers = km.cluster_centers_\n    pts = df[[\"lon\",\"lat\"]].fillna(df[[\"lon\",\"lat\"]].mean()).values\n    d = pairwise_distances(pts, centers, metric=\"euclidean\")\n    ktop = min(topk, d.shape[1])\n    idx = np.argpartition(d, kth=list(range(ktop)), axis=1)[:, :ktop]\n    for k in range(idx.shape[1]):\n        df[f\"geo_dist_{k+1}\"] = d[np.arange(len(d)), idx[:,k]]\n    return df\n\n# ================== 组内归一 & 中心化 ==================\ndef _clip_series(s: pd.Series, lo=None, hi=None):\n    if lo is not None or hi is not None:\n        s = s.clip(lower=lo, upper=hi)\n    s = s.replace([np.inf, -np.inf], np.nan)\n    return s.astype(float)\n\ndef add_group_norm(df_list: List[pd.DataFrame], base_df: pd.DataFrame, group_cols: List[str], num_cols: List[str], min_group_size:int=20):\n    RATIO_CLIP = 30.0\n    Z_CLIP = 15.0\n    EPS_M = 1e-6; EPS_A = 1e-9\n    for gcol in group_cols:\n        if gcol not in base_df.columns: continue\n        gsize = base_df.groupby(gcol).size()\n        valid_groups = set(gsize[gsize >= min_group_size].index)\n        if not valid_groups: continue\n        g = base_df[base_df[gcol].isin(valid_groups)].groupby(gcol)\n        for c in num_cols:\n            if c not in base_df.columns: continue\n            med = g[c].median()\n            mad = g[c].apply(lambda x: np.nanmedian(np.abs(x - np.nanmedian(x))) + EPS_A)\n            for df in df_list:\n                if c in df.columns:\n                    m = df[gcol].map(med)\n                    a = df[gcol].map(mad)\n                    m = m.where(df[gcol].isin(valid_groups), np.nan)\n                    a = a.where(df[gcol].isin(valid_groups), np.nan)\n                    denom_m = m.where(m.abs() >= EPS_M, np.nan)\n                    ratio = df[c] / denom_m\n                    z = (df[c] - m) / a\n                    df[f\"{c}_gmed_{gcol}\"] = _clip_series(ratio, -RATIO_CLIP, RATIO_CLIP)\n                    df[f\"{c}_gz_{gcol}\"] = _clip_series(z, -Z_CLIP, Z_CLIP)\n\ndef add_cat_num_centered(df_list: List[pd.DataFrame], base_df: pd.DataFrame, pairs: List[Tuple[str,str]], min_group_size:int=20):\n    for cat, num in pairs:\n        if cat not in base_df.columns or num not in base_df.columns: continue\n        gsize = base_df.groupby(cat).size()\n        valid = set(gsize[gsize >= min_group_size].index)\n        means = base_df[base_df[cat].isin(valid)].groupby(cat)[num].mean()\n        for df in df_list:\n            if (cat in df.columns) and (num in df.columns):\n                m = df[cat].map(means)\n                df[f\"{num}_by_{cat}_centered\"] = (df[num] - m).astype(float)\n\n","outputs":[],"execution_count":24},{"cell_type":"code","metadata":{"id":"C1EE79B19E2145E19CD00C17263645FC","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell5\n\n# ================== 目标编码（含分层） ==================\ndef add_target_encoding_triplet(X_tr: pd.DataFrame, y_tr: np.ndarray, X_val: pd.DataFrame, X_te: pd.DataFrame, cols: List[str], n_splits=5, smooth=90, seed=111):\n    X_tr = X_tr.copy(); X_val = X_val.copy(); X_te = X_te.copy()\n    global_mean = float(np.nanmean(y_tr))\n    kf = KFold(n_splits=max(2, n_splits), shuffle=True, random_state=seed)\n    for col in cols:\n        if col not in X_tr.columns: continue\n        oof = pd.Series(index=X_tr.index, dtype=float)\n        for tr_idx, vd_idx in kf.split(X_tr):\n            tr_fold = X_tr.iloc[tr_idx]; y_fold = y_tr[tr_idx]\n            dfm = pd.DataFrame({col: tr_fold[col].values, \"y\": y_fold})\n            means = dfm.groupby(col)[\"y\"].mean()\n            cnts  = tr_fold[col].value_counts()\n            sm = ((means * cnts) + global_mean * smooth) / (cnts + smooth)\n            oof.iloc[vd_idx] = X_tr[col].iloc[vd_idx].map(sm).fillna(global_mean).values\n        df_full = pd.DataFrame({col: X_tr[col].values, \"y\": y_tr})\n        means_full = df_full.groupby(col)[\"y\"].mean()\n        cnts_full  = X_tr[col].value_counts()\n        sm_full = ((means_full * cnts_full) + global_mean * smooth) / (cnts_full + smooth)\n        X_tr[f\"{col}__te\"]  = oof.values\n        X_val[f\"{col}__te\"] = X_val[col].map(sm_full).fillna(global_mean).values\n        X_te[f\"{col}__te\"]  = X_te[col].map(sm_full).fillna(global_mean).values\n    return X_tr, X_val, X_te\n\ndef _mode_parent_map(df: pd.DataFrame, child: str, parent: str) -> pd.Series:\n    grp = df.groupby(child)[parent].agg(lambda s: s.mode().iat[0] if len(s.mode()) else s.iloc[0])\n    return grp\n\ndef add_hier_target_encoding_triplet(X_tr: pd.DataFrame, y_tr: np.ndarray, X_val: pd.DataFrame, X_te: pd.DataFrame, pairs: List[Tuple[str,str]], smooth=90, seed=111):\n    X_tr = X_tr.copy(); X_val = X_val.copy(); X_te = X_te.copy()\n    global_mean = float(np.nanmean(y_tr))\n    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n    for child, parent in pairs:\n        if child not in X_tr.columns or parent not in X_tr.columns: continue\n        # OOF 编码\n        oof = pd.Series(index=X_tr.index, dtype=float)\n        for tr_idx, vd_idx in kf.split(X_tr):\n            tr_fold = X_tr.iloc[tr_idx]; y_fold = y_tr[tr_idx]\n            pm = _mode_parent_map(tr_fold[[child, parent]], child, parent)  # child -> parent\n            prior_parent_mean = pd.DataFrame({parent: tr_fold[parent], \"y\": y_fold}).groupby(parent)[\"y\"].mean()\n            prior_by_child = pm.map(prior_parent_mean).fillna(global_mean)  # child -> prior\n            dfm = pd.DataFrame({child: tr_fold[child], parent: tr_fold[parent], \"y\": y_fold})\n            child_means = dfm.groupby(child)[\"y\"].mean()\n            child_cnts  = tr_fold[child].value_counts()\n            prior_idxed = prior_by_child.reindex(child_means.index).fillna(global_mean)\n            sm = ((child_means * child_cnts) + prior_idxed * smooth) / (child_cnts + smooth)\n            oof.iloc[vd_idx] = X_tr[child].iloc[vd_idx].map(sm).fillna(global_mean).values\n        # 全量映射\n        pm_full = _mode_parent_map(X_tr[[child, parent]], child, parent)\n        prior_parent_full = pd.DataFrame({parent: X_tr[parent], \"y\": y_tr}).groupby(parent)[\"y\"].mean()\n        prior_by_child_full = pm_full.map(prior_parent_full).fillna(global_mean)\n        df_full = pd.DataFrame({child: X_tr[child], \"y\": y_tr})\n        means_full = df_full.groupby(child)[\"y\"].mean()\n        cnts_full  = X_tr[child].value_counts()\n        prior_full_idxed = prior_by_child_full.reindex(means_full.index).fillna(global_mean)\n        sm_full = ((means_full * cnts_full) + prior_full_idxed * smooth) / (cnts_full + smooth)\n        X_tr[f\"{child}__te\"]  = oof.values\n        X_val[f\"{child}__te\"] = X_val[child].map(sm_full).fillna(global_mean).values\n        X_te[f\"{child}__te\"]  = X_te[child].map(sm_full).fillna(global_mean).values\n        log(f\"[feat] hier-TE added: {child} (parent={parent})\")\n    return X_tr, X_val, X_te\n\n# ================== 文本小容量特征 ==================\ndef build_text_svd_features(X_tr: pd.DataFrame, X_val: pd.DataFrame, X_te: pd.DataFrame, cfg: CFG):\n    cols = [c for c in cfg.TEXT_COLS if c in X_tr.columns]\n    if not cols: return X_tr, X_val, X_te\n\n    def cat_text(df):\n        arr = []\n        for c in cols:\n            s = df[c].astype(str).fillna(\"\")\n            arr.append(s)\n        return [\" \".join(x) for x in zip(*arr)]\n\n    vect = TfidfVectorizer(ngram_range=(1,2), max_features=cfg.TEXT_MAX_FEATURES, min_df=cfg.TEXT_MIN_DF)\n    tr_docs  = cat_text(X_tr);  val_docs = cat_text(X_val);  te_docs  = cat_text(X_te)\n    Xtr_txt  = vect.fit_transform(tr_docs)\n    n_feat   = Xtr_txt.shape[1]\n    if n_feat == 0:\n        # 回退\n        X_tr[\"txt_len\"] = pd.Series([len(d) for d in tr_docs], index=X_tr.index).astype(float)\n        X_tr[\"txt_wc\"]  = pd.Series([len(d.split()) for d in tr_docs], index=X_tr.index).astype(float)\n        X_val[\"txt_len\"]= pd.Series([len(d) for d in val_docs], index=X_val.index).astype(float)\n        X_val[\"txt_wc\"] = pd.Series([len(d.split()) for d in val_docs], index=X_val.index).astype(float)\n        X_te[\"txt_len\"] = pd.Series([len(d) for d in te_docs], index=X_te.index).astype(float)\n        X_te[\"txt_wc\"]  = pd.Series([len(d.split()) for d in te_docs], index=X_te.index).astype(float)\n        log(\"[text] fallback -> added txt_len/txt_wc (no vocab)\")\n        return X_tr, X_val, X_te\n\n    if n_feat == 1:\n        X_tr[\"txt_tf1\"]  = np.asarray(Xtr_txt.todense()).ravel().astype(float)\n        X_val[\"txt_tf1\"] = np.asarray(vect.transform(val_docs).todense()).ravel().astype(float)\n        X_te[\"txt_tf1\"]  = np.asarray(vect.transform(te_docs).todense()).ravel().astype(float)\n        log(\"[text] fallback -> single TF-IDF feature (n_feat=1)\")\n        return X_tr, X_val, X_te\n\n    n_comp = min(max(1, cfg.TEXT_SVD_N), n_feat - 1)\n    svd = TruncatedSVD(n_components=n_comp, random_state=0)\n    Xval_txt = vect.transform(val_docs)\n    Xte_txt  = vect.transform(te_docs)\n    Xtr_svd  = svd.fit_transform(Xtr_txt)\n    Xval_svd = svd.transform(Xval_txt)\n    Xte_svd  = svd.transform(Xte_txt)\n    for i in range(n_comp):\n        X_tr[f\"txt_svd_{i+1}\"]  = Xtr_svd[:, i].astype(float)\n        X_val[f\"txt_svd_{i+1}\"] = Xval_svd[:, i].astype(float)\n        X_te[f\"txt_svd_{i+1}\"]  = Xte_svd[:, i].astype(float)\n    log(f\"[text] added {n_comp} SVD dims from {n_feat} tfidf feats\")\n    return X_tr, X_val, X_te\n\n# ================== 数值筛选 / 预处理 ==================\ndef corr_with_target(df: pd.DataFrame, y: np.ndarray, num_cols: List[str]) -> pd.Series:\n    tmp = df[num_cols].copy(); tmp[\"_y\"] = y\n    return tmp.corr(numeric_only=True)[\"_y\"].drop(index=\"_y\")\n\ndef drop_low_target_corr(num_cols: List[str], corr_s: pd.Series, min_abs_corr=0.02) -> List[str]:\n    keep = [c for c in num_cols if abs(corr_s.get(c, 0.0)) >= min_abs_corr]\n    return keep if keep else num_cols\n\ndef drop_high_pair_corr(df: pd.DataFrame, cols: List[str], max_abs_corr=0.999) -> List[str]:\n    if len(cols) <= 1: return cols\n    cmat = df[cols].corr(numeric_only=True).abs()\n    upper = cmat.where(np.triu(np.ones(cmat.shape), k=1).astype(bool))\n    drop = set()\n    for c in upper.columns:\n        highs = [r for r in upper.index if (upper.loc[r, c] >= max_abs_corr) and (r not in drop)]\n        if highs:\n            drop.add(c)\n    return [c for c in cols if c not in drop]\n\ndef numeric_fs_by_corr_pair(df_tr: pd.DataFrame, y_tr_log: np.ndarray, cfg: CFG) -> List[str]:\n    num_cols = [c for c in df_tr.columns if pd.api.types.is_numeric_dtype(df_tr[c])]\n    if not num_cols: return []\n    log(\"[3/10] 数值特征筛选（log-y）\")\n    corr_s = corr_with_target(df_tr, y_tr_log, num_cols)\n    keep = drop_low_target_corr(num_cols, corr_s, min_abs_corr=cfg.MIN_ABS_CORR_Y) if cfg.FS_BY_CORR else num_cols\n    keep2 = drop_high_pair_corr(df_tr, keep, max_abs_corr=cfg.MAX_ABS_CORR_FEAT) if cfg.FS_BY_CORR_PAIR else keep\n    log(f\" kept: {len(keep2)} / {len(num_cols)}\")\n    return keep2\n\n","outputs":[],"execution_count":25},{"cell_type":"code","metadata":{"id":"64C367C40FFA47049A23470A24333F2D","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell6\n# --- to-string 变换器与 OHE 封装 ---\ndef _to_str_df(X):\n    if hasattr(X, \"astype\"):\n        try: return X.astype(str)\n        except Exception: pass\n    return pd.DataFrame(X).astype(str)\n\ndef _make_tostr_transformer():\n    try:\n        return FunctionTransformer(_to_str_df, validate=False, feature_names_out=\"one-to-one\")\n    except TypeError:\n        return FunctionTransformer(_to_str_df, validate=False)\n\ndef _make_ohe(drop=\"if_binary\"):\n    try:\n        return OneHotEncoder(handle_unknown=\"ignore\", drop=drop, sparse_output=True)\n    except TypeError:\n        return OneHotEncoder(handle_unknown=\"ignore\", drop=drop, sparse=True)\n\ndef build_preprocessor(df_tr, y_tr_log, cfg, num_keep, ohe_cols) -> ColumnTransformer:\n    log(\"[4/10] 构建预处理器（稀疏友好 & 可并行pickle）\")\n    num_pipe = Pipeline([\n        (\"imp\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler(with_mean=False)),\n    ])\n    tostr = _make_tostr_transformer()\n    cat_pipe = Pipeline([\n        (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"UNK\")),\n        (\"tostr\", tostr),\n        (\"ohe\", _make_ohe(\"if_binary\")),\n    ])\n    transformers = []\n    if num_keep: transformers.append((\"num\", num_pipe, num_keep))\n    if ohe_cols: transformers.append((\"cat\", cat_pipe, ohe_cols))\n    pre = ColumnTransformer(\n        transformers=transformers,\n        remainder=\"drop\",\n        sparse_threshold=0.3,\n        verbose_feature_names_out=False\n    )\n    log(f\" blocks: num={len(num_keep)}, cats={len(ohe_cols)}\")\n    return pre\n\n# ================== 类别列拆分 ==================\ndef split_categoricals_for_encoding(train_df: pd.DataFrame, high_card_thresh=20) -> Tuple[List[str], List[str]]:\n    cats = [c for c in train_df.columns if train_df[c].dtype == \"object\"]\n    exclude = set([\n        \"房屋优势\",\"核心卖点\",\"户型介绍\",\"周边配套\",\"交通出行\",\"客户反馈\",\n        \"物业办公电话\",\"coord_x\",\"coord_y\",\"交易时间\",\"上次交易\"\n    ])\n    num_mapped = {c[:-4] for c in train_df.columns if c.endswith(\"_num\")}\n    exclude |= num_mapped\n    cats = [c for c in cats if c not in exclude]\n    high, low = [], []\n    for c in cats:\n        k = train_df[c].nunique(dropna=False)\n        (high if k > high_card_thresh else low).append(c)\n    for c in [\"geo_cluster\",\"环线位置\",\"区县\",\"环线\"]:\n        if (c in cats) and (c not in high):\n            high.append(c)\n        if c in low:\n            low.remove(c)\n    return high, low\n\n# ================== winsorize 单列（按 train 分位） ==================\ndef winsorize_triplet_by_train_quantile(X_tr, X_val, X_te, col, lower=0.01, upper=0.99):\n    if col not in X_tr.columns: return X_tr, X_val, X_te\n    s = pd.to_numeric(X_tr[col], errors=\"coerce\")\n    lo, hi = s.quantile([lower, upper])\n    for df in (X_tr, X_val, X_te):\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\").clip(lower=lo, upper=hi)\n    return X_tr, X_val, X_te\n# ================== 轻量交互 ==================\ndef add_interactions(df_list: List[pd.DataFrame], cfg: CFG):\n    for df in df_list:\n        if cfg.ENABLE_INTERACTION_TE_AGE and (\"环线位置__te\" in df.columns) and (\"house_age\" in df.columns):\n            df[\"ix_teXage\"] = pd.to_numeric(df[\"环线位置__te\"], errors=\"coerce\") * pd.to_numeric(df[\"house_age\"], errors=\"coerce\")\n        if cfg.ENABLE_INTERACTION_DIST_AREA and (\"geo_dist_1\" in df.columns) and (\"面积_num\" in df.columns):\n            df[\"ix_distXarea\"] = pd.to_numeric(df[\"geo_dist_1\"], errors=\"coerce\") * pd.to_numeric(df[\"面积_num\"], errors=\"coerce\")\n\n# ================== 罕见类别合并（兜底实现） ==================\ndef build_rare_maps(df: pd.DataFrame, min_cnt: int = 100) -> Dict[str, Set[str]]:\n    rare_map = {}\n    for c in df.columns:\n        if df[c].dtype == \"object\":\n            vc = df[c].value_counts(dropna=False)\n            rares = set(vc[vc < min_cnt].index.astype(str))\n            if rares:\n                rare_map[c] = rares\n    return rare_map\n\ndef apply_rare_maps(df: pd.DataFrame, rare_map: Dict[str, Set[str]]) -> pd.DataFrame:\n    if not rare_map: return df\n    df = df.copy()\n    for c, rares in rare_map.items():\n        if c in df.columns:\n            s = df[c].astype(str)\n            df[c] = np.where(s.isin(rares), \"OTHER\", s)\n    return df\n\n# ================== 6折交叉验证（MAE） ==================\ndef compute_cv6_mae_log(estimator, X, y_log, seed=111):\n    \"\"\"\n    log 空间 MAE（neg_mean_absolute_error），仅用于诊断/参考。\n    \"\"\"\n    cv = KFold(n_splits=6, shuffle=True, random_state=seed)\n    scores = cross_val_score(\n        estimator, X, y_log,\n        cv=cv,\n        scoring=\"neg_mean_absolute_error\",\n        n_jobs=-1,\n        error_score=\"raise\"\n    )\n    return float(-scores.mean())\n\ndef compute_cv6_mae_orig(estimator, X, y_log, use_log=True, seed=111):\n    \"\"\"\n    原始单位（价格）的 MAE：将 y_log / y_pred_log 反变换后计算 MAE。\n    \"\"\"\n    def _mae_orig(y_true_log, y_pred_log):\n        y_true = np.expm1(y_true_log) if use_log else y_true_log\n        y_pred = np.expm1(y_pred_log) if use_log else y_pred_log\n        y_med  = float(np.nanmedian(y_true))\n        y_pred = np.where(np.isfinite(y_pred), y_pred, y_med)\n        return mean_absolute_error(y_true, y_pred)\n    scorer = make_scorer(_mae_orig, greater_is_better=False)\n    cv = KFold(n_splits=6, shuffle=True, random_state=seed)\n    scores = cross_val_score(estimator, X, y_log, cv=cv, scoring=scorer, n_jobs=-1, error_score=\"raise\")\n    return float(-scores.mean())","outputs":[],"execution_count":26},{"cell_type":"code","metadata":{"id":"E342F759570942348F44BD817F7F8033","notebookId":"6901f9297a0e19b0ec3ce988","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Cell7\n# ================== 单 seed 训练 ==================\ndef run_one_seed(cfg: CFG, seed: int):\n    # 读取与基础加工\n    tr, te = load_raw(cfg)\n    rare_maps = build_rare_maps(tr, min_cnt=cfg.RARE_MIN_COUNT)\n    tr = apply_rare_maps(tr, rare_maps); te = apply_rare_maps(te, rare_maps)\n    tr = apply_common_feature_enrichment(tr); te = apply_common_feature_enrichment(te)\n\n    km = fit_geo_kmeans(tr, n_clusters=cfg.GEO_K, seed=seed)\n    if km is not None:\n        log(f\"[feat] geo KMeans fitted (k={cfg.GEO_K})\")\n\n    # 目标与切分\n    y_raw = pd.to_numeric(tr[cfg.TARGET], errors=\"coerce\").values\n    y_raw = winsorize_y(y_raw, cfg.WINSOR_LOWER, cfg.WINSOR_UPPER)\n    y_t   = _to_target(y_raw, cfg.USE_LOG_TARGET)\n    X     = tr.drop(columns=[cfg.TARGET])\n    X_te  = te.drop(columns=[cfg.TARGET], errors=\"ignore\")\n\n    log(\"[2/10] 训练/验证 80/20 划分\")\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y_t, test_size=cfg.TEST_SIZE, random_state=seed)\n    y_tr_orig = _from_target(y_tr, cfg.USE_LOG_TARGET)\n    y_val_orig = _from_target(y_val, cfg.USE_LOG_TARGET)\n\n    # 地理特征\n    for df in (X_tr, X_val, X_te):\n        assign_geo_cluster(df, km)\n        add_geo_center_dists(df, km, topk=cfg.GEO_DIST_TOPK)\n\n    # 分段、组内归一等增强\n    add_piecewise_map([X_tr, X_val, X_te], {\"面积_num\":[60,90,120,150], \"house_age\":[10,20,30]})\n    grp_cols = [c for c in [\"geo_cluster\",\"环线位置\",\"区县\"] if c in X_tr.columns]\n    num_cols_for_group = [c for c in [\"面积_num\",\"建筑面积_num\",\"套内面积_num\",\"house_age\",\"propfee_per_m2\",\"days_since_trade\"] if c in X_tr.columns]\n    add_group_norm([X_tr, X_val, X_te], X_tr, grp_cols, num_cols_for_group, min_group_size=20)\n    pairs = [(c,n) for (c,n) in [(\"环线位置\",\"面积_num\"), (\"环线位置\",\"propfee_per_m2\")] if (c in X_tr.columns and n in X_tr.columns)]\n    add_cat_num_centered([X_tr, X_val, X_te], X_tr, pairs, min_group_size=20)\n    X_tr, X_val, X_te = winsorize_triplet_by_train_quantile(X_tr, X_val, X_te, \"propfee_per_m2\", 0.01, 0.99)\n\n    # 目标编码（层级 + 普通）\n    te_high, ohe_low = split_categoricals_for_encoding(X_tr, high_card_thresh=cfg.HIGH_CARD_THRESH)\n    hier_pairs = [(c,p) for (c,p) in [(\"环线位置\",\"环线\"), (\"区县\",\"城市\")] if (c in X_tr.columns and p in X_tr.columns)]\n    if hier_pairs:\n        X_tr, X_val, X_te = add_hier_target_encoding_triplet(X_tr, y_tr, X_val, X_te, hier_pairs, smooth=cfg.TE_SMOOTH, seed=seed)\n    te_cols = [c for c in te_high if f\"{c}__te\" not in X_tr.columns]\n    if te_cols:\n        X_tr, X_val, X_te = add_target_encoding_triplet(X_tr, y_tr, X_val, X_te, cols=te_cols, n_splits=5, smooth=cfg.TE_SMOOTH, seed=seed)\n    log(f\"[feat] simple-TE added: {te_cols}\")\n\n    # 文本特征（若开启）\n    if cfg.USE_TEXT_FEATURES:\n        X_tr, X_val, X_te = build_text_svd_features(X_tr, X_val, X_te, cfg)\n\n    # 轻交互 & 数值列筛选\n    add_interactions([X_tr, X_val, X_te], cfg)\n    num_keep = numeric_fs_by_corr_pair(X_tr, y_tr, cfg)\n    ohe_cols = [c for c in ohe_low if c in X_tr.columns]\n\n    # 预处理器\n    preproc = build_preprocessor(X_tr, y_tr, cfg, num_keep, ohe_cols)\n\n    # 模型 & 超参（L1/EN 范围下探）\n    models = {\n        \"OLS\": LinearRegression(),\n        \"Ridge\": Ridge(),\n        # 用 EN 退化为 Lasso（l1_ratio=1.0），统一接口\n        \"Lasso\": ElasticNet(alpha=1e-5, l1_ratio=1.0, max_iter=8000, tol=1e-3, selection=\"cyclic\", random_state=seed),\n        \"ElasticNet\": ElasticNet(max_iter=8000, tol=1e-3, selection=\"cyclic\", random_state=seed),\n    }\n    param_grids = {\n        \"Ridge\": {\"model__alpha\": [1,10,30,60,120,200,300,450]},\n        \"Lasso\": {\"model__alpha\": [1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 3e-4, 5e-4, 8e-4, 1e-3],\n                  \"model__l1_ratio\": [1.0]},\n        \"ElasticNet\": {\n            \"model__alpha\": [1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 3e-4, 5e-4, 8e-4, 1e-3],\n            \"model__l1_ratio\": [0.05, 0.1, 0.2, 0.35, 0.5],\n        },\n    }\n\n    results, best_name, best_pipe, best_out_mae = {}, None, None, np.inf\n    log_lo, log_hi = np.nanquantile(y_tr, [0.01, 0.99])\n    y_med = float(np.nanmedian(y_tr_orig))\n    log(\"[5/10] 训练与评估各模型\")\n    for name, mdl in models.items():\n        log(f\" -> {name}\")\n        base_pipe = Pipeline([\n            (\"pre\", preproc),\n            (\"post_scale\", MaxAbsScaler()),\n            (\"model\", mdl)\n        ])\n\n        tuned = None\n        pipe = None\n        pipe_for_cv = None\n\n        if name in param_grids:\n            try:\n                cv_folds = 3 if name in (\"Lasso\", \"ElasticNet\") else 5\n                gs = GridSearchCV(\n                    base_pipe, param_grids[name],\n                    scoring=\"neg_mean_absolute_error\",\n                    cv=KFold(n_splits=cv_folds, shuffle=True, random_state=seed),\n                    n_jobs=-1, refit=True, error_score=\"raise\"\n                )\n                gs.fit(X_tr, y_tr)\n                pipe = gs.best_estimator_\n                tuned = gs.best_params_\n                pipe_for_cv = gs.best_estimator_\n            except Exception as e:\n                log(f\" [warn] gridsearch failed: {e}\")\n                pipe = base_pipe\n                pipe.fit(X_tr, y_tr)\n                pipe_for_cv = base_pipe\n        else:\n            pipe = base_pipe\n            pipe.fit(X_tr, y_tr)\n            pipe_for_cv = base_pipe\n\n        # 还原预测并裁剪\n        pred_log_in  = pipe.predict(X_tr)\n        pred_log_out = pipe.predict(X_val)\n        pred_in  = _from_target(np.clip(pred_log_in,  log_lo, log_hi), cfg.USE_LOG_TARGET)\n        pred_out = _from_target(np.clip(pred_log_out, log_lo, log_hi), cfg.USE_LOG_TARGET)\n        pred_in  = np.where(np.isfinite(pred_in),  pred_in,  y_med)\n        pred_out = np.where(np.isfinite(pred_out), pred_out, y_med)\n\n        in_mae  = float(mean_absolute_error(y_tr_orig,  pred_in))\n        out_mae = float(mean_absolute_error(y_val_orig, pred_out))\n\n        # cv6：同时给出 log 空间与原空间（价格）MAE\n        cv6_mae_log  = compute_cv6_mae_log (pipe_for_cv, X_tr, y_tr, seed=seed)\n        cv6_mae_orig = compute_cv6_mae_orig(pipe_for_cv, X_tr, y_tr, use_log=cfg.USE_LOG_TARGET, seed=seed)\n\n        # 日志显示：in/out/cv6（原空间用整数），cv6_log 用小数\n        log(f\" in {in_mae:.0f} | out {out_mae:.0f} | cv6 {cv6_mae_orig:.0f} | cv6_log {cv6_mae_log:.4f} | tuned={tuned}\")\n        results[name] = {\"in\": in_mae, \"out\": out_mae, \"cv6\": cv6_mae_orig, \"cv6_log\": cv6_mae_log, \"tuned\": tuned}\n\n        if out_mae < best_out_mae:\n            best_out_mae = out_mae\n            best_name, best_pipe = name, pipe\n\n    log(\"[6/10] 保存指标(JSON)\")\n    with open(cfg.METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n\n    log(f\"[7/10] 保存最优模型：{best_name}\")\n    dump(best_pipe, cfg.SAVE_MODEL_PATH)\n\n    log(\"[8/10] 预测验证/测试集\")\n    pred_val_raw  = _from_target(np.clip(best_pipe.predict(X_val), log_lo, log_hi), cfg.USE_LOG_TARGET)\n    pred_test_raw = _from_target(np.clip(best_pipe.predict(X_te),  log_lo, log_hi), cfg.USE_LOG_TARGET)\n    pred_val_raw  = np.where(np.isfinite(pred_val_raw),  pred_val_raw,  y_med)\n    pred_test_raw = np.where(np.isfinite(pred_test_raw), pred_test_raw, y_med)\n\n    te_raw = pd.read_csv(cfg.TEST_PATH, low_memory=False)\n    id_col = \"ID\" if \"ID\" in te_raw.columns else None\n    te_ids = te_raw[id_col].values if id_col else np.arange(len(X_te))\n    part = pd.DataFrame({\"ID\": te_ids, \"Price\": pred_test_raw})\n    return part, results\n    \n# ================== 跨 seed 汇总 ==================\ndef _aggregate_results_across_seeds(per_seed_dicts: List[Dict]) -> pd.DataFrame:\n    \"\"\"\n    per_seed_dicts: [results_seed1, results_seed2, ...]\n    results_seedX: {\"OLS\":{\"in\":..,\"out\":..,\"cv6\":..}, \"Ridge\":{...}, ...}\n    返回：DataFrame(columns=[\"model\",\"in\",\"out\",\"cv6\"])  # cv6 为原始单位 MAE\n    \"\"\"\n    rows = []\n    order = [\"OLS\", \"Ridge\", \"Lasso\", \"ElasticNet\"]\n    for m in order:\n        ins  = [d[m][\"in\"]  for d in per_seed_dicts if m in d]\n        outs = [d[m][\"out\"] for d in per_seed_dicts if m in d]\n        cv6s = [d[m].get(\"cv6\", np.nan) for d in per_seed_dicts if m in d]\n        if ins:\n            rows.append({\n                \"model\": m,\n                \"in\":  float(np.nanmean(ins)),\n                \"out\": float(np.nanmean(outs)),\n                \"cv6\": float(np.nanmean(cv6s)),\n            })\n    return pd.DataFrame(rows)\n\n# ================== 单轨道运行（返回轨道汇总表） ==================\ndef run_one_track(cfg: CFG):\n    cfg = set_paths(cfg)\n    preds = []\n    seed_weights = []\n    all_results = []\n\n    for seed in cfg.SEEDS:\n        log(f\"[seed] = {seed}\")\n        part, res_dict = run_one_seed(cfg, seed)\n        preds.append(part[\"Price\"].values)\n        best_out_mae = min(m[\"out\"] for m in res_dict.values())\n        seed_weights.append(1.0 / max(best_out_mae, 1e-9))\n        all_results.append(res_dict)\n\n    # 加权融合\n    W = np.asarray(seed_weights, dtype=float)\n    W = W / W.sum()\n    P = np.vstack(preds)\n    ens = (P.T @ W).astype(float)\n\n    te = pd.read_csv(cfg.TEST_PATH, low_memory=False)\n    id_col = \"ID\" if \"ID\" in te.columns else None\n    out_df = pd.DataFrame({\n        \"ID\": te[id_col].values if id_col else np.arange(len(te)),\n        \"Price\": ens\n    })\n\n    # 轨道汇总表（cv6 为原始单位）\n    summary_df = _aggregate_results_across_seeds(all_results)\n    summary_df.insert(0, \"track\", cfg.TRACK)\n    summary_path = os.path.join(cfg.OUT_DIR, f\"metrics_summary_{cfg.TRACK}.csv\")\n    summary_df.to_csv(summary_path, index=False)\n    log(f\"[9/10] 该轨道汇总表已保存: {summary_path}\")\n\n    return out_df, cfg.OUT_DIR, summary_df\n\n# ================== 合并提交（格式不变） ==================\ndef write_combined_csv(price_df: pd.DataFrame, rent_df: pd.DataFrame, out_path: str):\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    price_df.to_csv(out_path, index=False)                # 第一段 price 带表头\n    rent_df.to_csv(out_path, index=False, header=False, mode=\"a\")  # 第二段 rent 无表头\n    log(f\"[done] 合并预测 -> {out_path}\")\n# ================== 主流程 ==================\ndef run_tracks(tracks=(\"price\",\"rent\")):\n    preds_by_track = {}\n    out_dir_used = None\n    all_tables = []\n\n    for t in tracks:\n        cfg = CFG(TRACK=t)\n        cfg = set_paths(cfg)\n        log(f\"[10/10] 运行轨道 {t} | cores={os.cpu_count() or 4}\")\n        part, out_dir, one_table = run_one_track(cfg)\n        preds_by_track[t] = part\n        all_tables.append(one_table)\n        if out_dir_used is None:\n            out_dir_used = out_dir\n\n    out_path = os.path.join(out_dir_used, \"predictions_combined.csv\")\n    if (\"price\" in preds_by_track) and (\"rent\" in preds_by_track):\n        write_combined_csv(preds_by_track[\"price\"], preds_by_track[\"rent\"], out_path)\n    else:\n        for k, v in preds_by_track.items():\n            v.to_csv(os.path.join(out_dir_used, f\"predictions_{k}.csv\"), index=False)\n\n    final_table = pd.concat(all_tables, ignore_index=True)\n    final_csv = os.path.join(out_dir_used, \"metrics_summary_all.csv\")\n    final_table.to_csv(final_csv, index=False)\n    print(final_table)\n    log(f\"[done] OLS/Ridge/Lasso/EN — in/out/cv6(原始单位) 汇总 -> {final_csv}\")\n\nif __name__ == \"__main__\":\n    run_tracks(tracks=(\"price\",\"rent\"))\n","outputs":[{"output_type":"stream","name":"stdout","text":"[paths] OUT_DIR = /home/mw/project/outputs\n[16:57:50] [10/10] 运行轨道 price | cores=64\n[paths] OUT_DIR = /home/mw/project/outputs\n[16:57:50] [seed] = 111\n[16:57:50] [1/10] 读取数据\n[train] shape=(103871, 61), dtypes summary: {dtype('O'): 41, dtype('float64'): 19, dtype('int64'): 1}\n[test]  shape=(34017, 61), dtypes summary: {dtype('O'): 41, dtype('float64'): 18, dtype('int64'): 2}\n[16:58:02] [feat] geo KMeans fitted (k=80)\n[16:58:02] [2/10] 训练/验证 80/20 划分\n[16:58:05] [feat] hier-TE added: 环线位置 (parent=环线)\n[16:58:05] [feat] hier-TE added: 区县 (parent=城市)\n[16:58:11] [feat] simple-TE added: ['房屋户型', '所在楼层', '房屋朝向', '梯户比例', '物业类别', '建筑年代', '开发商', '房屋总数', '楼栋总数', '物业公司', '产权描述', '燃气费', '供热费', '环线']\n[16:58:13] [text] added 5 SVD dims from 240 tfidf feats\n[16:58:13] [3/10] 数值特征筛选（log-y）\n[16:58:16]  kept: 67 / 84\n[16:58:16] [4/10] 构建预处理器（稀疏友好 & 可并行pickle）\n[16:58:16]  blocks: num=67, cats=12\n[16:58:16] [5/10] 训练与评估各模型\n[16:58:16]  -> OLS\n[16:58:38]  in 464205 | out 453407 | cv6 521330 | cv6_log 0.2136 | tuned=None\n[16:58:38]  -> Ridge\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_56/1186178380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mrun_tracks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_56/1186178380.py\u001b[0m in \u001b[0;36mrun_tracks\u001b[0;34m(tracks)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[10/10] 运行轨道 {t} | cores={os.cpu_count() or 4}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_track\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mpreds_by_track\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mall_tables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_56/1186178380.py\u001b[0m in \u001b[0;36mrun_one_track\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSEEDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[seed] = {seed}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_one_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mbest_out_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_56/1186178380.py\u001b[0m in \u001b[0;36mrun_one_seed\u001b[0;34m(cfg, seed)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 )\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mtuned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":27}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}