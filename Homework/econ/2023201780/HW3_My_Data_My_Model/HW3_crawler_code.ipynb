{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dabe3bcd-be3c-4c18-94e9-e7dfeb45b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\annn\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\annn\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\annn\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\annn\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\annn\\lib\\site-packages (from requests) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9007c2d-34e1-4585-8457-73f2cd2521ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in d:\\annn\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\annn\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\annn\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c738679e-a428-4d8b-a573-9983d78b1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a9bdf8-d4e4-411c-8cef-d92840ef7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECOND_HAND_URL = \"https://esf.fang.com/house-a010-b01515/i3{}\"    # Pagination template, {} Replace page numbers (1-20)\n",
    "SECOND_HAND_CSV = r\"D:\\pythonai\\yizhuang_sell.csv\"    # Storage path for house sale data\n",
    "\n",
    "RENTAL_URL = \"https://zu.fang.com/house-a010-b01515/i3{}\"  # Pagination template\n",
    "RENTAL_CSV = r\"D:\\pythonai\\yizhuang_rent.csv\"  # Storage path for rental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c3dfb4-1e62-4884-973b-30f5bddfa92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "    \"Cookie\": \"city=www; city.sig=OGYSb1kOr8YVFH0wBEXukpoi1DeOqwvdseB7aTrJ-zE; csrfToken=8VgsPpoSBQ1HMYiz1N5Lf5cu; global_cookie=wpb8xtl8e0x9pqia4hva7kkcl2qmgqb5rtt; __utma=147393320.941255132.1760430996.1760430996.1760430996.1; __utmc=147393320; __utmz=147393320.1760430996.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); g_sourcepage=esf_fy%5Elb_pc; otherid=379970e2caba3e43e708a017bd4ae66b; unique_cookie=U_wpb8xtl8e0x9pqia4hva7kkcl2qmgqb5rtt*10\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,*/*;q=0.8\",\n",
    "    \"Referer\": \"https://esf.fang.com/\"    # Simulate the jump from the homepage of Fangtianxia to reduce the risk of anti-crawling\n",
    "}\n",
    "\n",
    "PAGE_COUNT = 20    # Crawl the first 20 pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13c872c-355d-4714-94d0-038afe1952d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    dir_path = os.path.dirname(path)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Created folder：{dir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4007ea-51fc-4433-bd2c-8665b19b1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_second_hand(html):\n",
    "    \"\"\"Analysis of the house sale page: Extract 'Area' and 'House Price per Square Meter'\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    data = []\n",
    "    house_list = soup.select(\"div.shop_list.shop_list_4 dl.clearfix\")  \n",
    "    \n",
    "    for house in house_list:\n",
    "        # Extraction area\n",
    "        tel_shop = house.select_one(\"p.tel_shop\")\n",
    "        if tel_shop:\n",
    "            area_match = re.search(r\"(\\d+\\.?\\d*)㎡\", tel_shop.get_text(strip=True))\n",
    "        else:\n",
    "            area_match = None\n",
    "        \n",
    "        # Extract the housing price per square meter\n",
    "        price_right = house.select_one(\"dd.price_right\")\n",
    "        if price_right:\n",
    "            price_per_sqm_match = re.search(r\"(\\d+)元/㎡\", price_right.get_text(strip=True))\n",
    "        else:\n",
    "            price_per_sqm_match = None\n",
    "        \n",
    "        if area_match and price_per_sqm_match:\n",
    "            area = area_match.group(1)  # area\n",
    "            price_per_sqm = price_per_sqm_match.group(1)  # the housing price per square meter\n",
    "            data.append([area, price_per_sqm])\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_rental(html):\n",
    "    \"\"\"Analyze the rental page: Extract 'Area and 'Monthly Rent'\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    data = []\n",
    "    # List of location-based rentals\n",
    "    rental_list = soup.select(\"dd.info.rel p.font15.mt12.bold\") \n",
    "    \n",
    "    for rental in rental_list:\n",
    "        area_match = re.search(r\"(\\d+\\.?\\d*)㎡\", rental.get_text(strip=True))    # Extraction area\n",
    "        more_info = rental.find_next(\"div\", class_=\"moreInfo\")    # Extract the monthly rent\n",
    "        if more_info:\n",
    "            price_match = more_info.select_one(\"span.price\")\n",
    "            rent = price_match.get_text(strip=True) if price_match else None\n",
    "        else:\n",
    "            rent = None\n",
    "        \n",
    "        if area_match and rent:\n",
    "            area = area_match.group(1)  \n",
    "            data.append([area, rent])\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4101c6f-3bd2-4ca1-a174-6df7dcb29e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_data(url_template, parse_func, csv_path, page_count):\n",
    "    create_dir(csv_path)\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if \"sell\" in csv_path:\n",
    "            writer.writerow([\"area_sqm\", \"unit_price_yuan_per_sqm\"])\n",
    "        else:\n",
    "            writer.writerow([\"area_sqm\", \"rent_yuan_per_month\"])\n",
    "    \n",
    "    # Pagination crawling\n",
    "    total_data = 0  \n",
    "    for page in range(1, page_count + 1):\n",
    "        url = url_template.format(page)    # Construct the current page URL (e.g., Page 1: i31, Page 2: i32)\n",
    "        try:\n",
    "            # Send a request (add random delay to avoid reverse crawling)\n",
    "            time.sleep(random.uniform(1.5, 3.5))\n",
    "            response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            response.encoding = \"utf-8\"    # Enforce UTF-8 encoding to prevent garbled Chinese characters\n",
    "            \n",
    "            # Verify whether the request was successful\n",
    "            if response.status_code == 200:\n",
    "                page_data = parse_func(response.text)\n",
    "                total_data += len(page_data)\n",
    "                # Write to CSV (append mode)\n",
    "                with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerows(page_data)\n",
    "                print(f\" The crawling of page {page:2d} is completed | New {len(page_data)} data entries have been added | The total number of {total_data} entries has been accumulated\")\n",
    "            else:\n",
    "                print(f\" {page:2d} page request failed |  Status code: {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error was retrieved on page {page:2d} | Error message:{str(e)[:50]}\")\n",
    "            time.sleep(5)  # When an error occurs, extend the delay to avoid continuous triggering of anti-crawling\n",
    "    \n",
    "    print(f\"\\n The crawling has ended | Crawl the {page_count} page in total | A total of {total_data} pieces of data | Storage path:{csv_path}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "846a36a4-b8f3-499e-9645-b33e2b237f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== Start crawling the data of house sales ==============================\n",
      " The crawling of page  1 is completed | New 60 data entries have been added | The total number of 60 entries has been accumulated\n",
      " The crawling of page  2 is completed | New 60 data entries have been added | The total number of 120 entries has been accumulated\n",
      " The crawling of page  3 is completed | New 60 data entries have been added | The total number of 180 entries has been accumulated\n",
      " The crawling of page  4 is completed | New 60 data entries have been added | The total number of 240 entries has been accumulated\n",
      " The crawling of page  5 is completed | New 60 data entries have been added | The total number of 300 entries has been accumulated\n",
      " The crawling of page  6 is completed | New 60 data entries have been added | The total number of 360 entries has been accumulated\n",
      " The crawling of page  7 is completed | New 60 data entries have been added | The total number of 420 entries has been accumulated\n",
      " The crawling of page  8 is completed | New 60 data entries have been added | The total number of 480 entries has been accumulated\n",
      " The crawling of page  9 is completed | New 60 data entries have been added | The total number of 540 entries has been accumulated\n",
      " The crawling of page 10 is completed | New 60 data entries have been added | The total number of 600 entries has been accumulated\n",
      " The crawling of page 11 is completed | New 60 data entries have been added | The total number of 660 entries has been accumulated\n",
      " The crawling of page 12 is completed | New 60 data entries have been added | The total number of 720 entries has been accumulated\n",
      " The crawling of page 13 is completed | New 60 data entries have been added | The total number of 780 entries has been accumulated\n",
      " The crawling of page 14 is completed | New 60 data entries have been added | The total number of 840 entries has been accumulated\n",
      " The crawling of page 15 is completed | New 60 data entries have been added | The total number of 900 entries has been accumulated\n",
      " The crawling of page 16 is completed | New 60 data entries have been added | The total number of 960 entries has been accumulated\n",
      " The crawling of page 17 is completed | New 60 data entries have been added | The total number of 1020 entries has been accumulated\n",
      " The crawling of page 18 is completed | New 60 data entries have been added | The total number of 1080 entries has been accumulated\n",
      " The crawling of page 19 is completed | New 60 data entries have been added | The total number of 1140 entries has been accumulated\n",
      " The crawling of page 20 is completed | New 60 data entries have been added | The total number of 1200 entries has been accumulated\n",
      "\n",
      " The crawling has ended | Crawl the 20 page in total | A total of 1200 pieces of data | Storage path:D:\\pythonai\\yizhuang_sell.csv\n",
      "\n",
      "============================== Start crawling rental data ==============================\n",
      " The crawling of page  1 is completed | New 60 data entries have been added | The total number of 60 entries has been accumulated\n",
      " The crawling of page  2 is completed | New 60 data entries have been added | The total number of 120 entries has been accumulated\n",
      " The crawling of page  3 is completed | New 60 data entries have been added | The total number of 180 entries has been accumulated\n",
      " The crawling of page  4 is completed | New 60 data entries have been added | The total number of 240 entries has been accumulated\n",
      " The crawling of page  5 is completed | New 60 data entries have been added | The total number of 300 entries has been accumulated\n",
      " The crawling of page  6 is completed | New 60 data entries have been added | The total number of 360 entries has been accumulated\n",
      " The crawling of page  7 is completed | New 60 data entries have been added | The total number of 420 entries has been accumulated\n",
      " The crawling of page  8 is completed | New 60 data entries have been added | The total number of 480 entries has been accumulated\n",
      " The crawling of page  9 is completed | New 60 data entries have been added | The total number of 540 entries has been accumulated\n",
      " The crawling of page 10 is completed | New 60 data entries have been added | The total number of 600 entries has been accumulated\n",
      " The crawling of page 11 is completed | New 60 data entries have been added | The total number of 660 entries has been accumulated\n",
      " The crawling of page 12 is completed | New 60 data entries have been added | The total number of 720 entries has been accumulated\n",
      " The crawling of page 13 is completed | New 60 data entries have been added | The total number of 780 entries has been accumulated\n",
      " The crawling of page 14 is completed | New 60 data entries have been added | The total number of 840 entries has been accumulated\n",
      " The crawling of page 15 is completed | New 60 data entries have been added | The total number of 900 entries has been accumulated\n",
      " The crawling of page 16 is completed | New 60 data entries have been added | The total number of 960 entries has been accumulated\n",
      " The crawling of page 17 is completed | New 60 data entries have been added | The total number of 1020 entries has been accumulated\n",
      " The crawling of page 18 is completed | New 60 data entries have been added | The total number of 1080 entries has been accumulated\n",
      " The crawling of page 19 is completed | New 60 data entries have been added | The total number of 1140 entries has been accumulated\n",
      " The crawling of page 20 is completed | New 60 data entries have been added | The total number of 1200 entries has been accumulated\n",
      "\n",
      " The crawling has ended | Crawl the 20 page in total | A total of 1200 pieces of data | Storage path:D:\\pythonai\\yizhuang_rent.csv\n",
      "\n",
      "All crawling tasks have been completed! The data has been saved to the folder D:/pythonai.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    print(\"\\n\" + \"=\"*30 + \" Start crawling the data of house sales \" + \"=\"*30)\n",
    "    crawl_data(SECOND_HAND_URL, parse_second_hand, SECOND_HAND_CSV, PAGE_COUNT)\n",
    "    \n",
    "    print(\"=\"*30 + \" Start crawling rental data \" + \"=\"*30)\n",
    "    crawl_data(RENTAL_URL, parse_rental, RENTAL_CSV, PAGE_COUNT)\n",
    "    \n",
    "    print(\"All crawling tasks have been completed! The data has been saved to the folder D:/pythonai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067315e-64a8-4785-a99b-c8a3e6365d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bea4f5-f8bf-4f42-9ec7-1622d18a2ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
