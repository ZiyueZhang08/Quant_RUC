{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a43c2-eabf-4ada-b21e-931696aceef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge WebDriver initialized successfully\n",
      "Finished page 1. Moving to next page...\n",
      "Finished page 2. Moving to next page...\n",
      "Finished page 3. Moving to next page...\n",
      "Finished page 4. Moving to next page...\n",
      "Finished page 5. Moving to next page...\n",
      "Finished page 6. Moving to next page...\n",
      "Finished page 7. Moving to next page...\n",
      "Finished page 8. Moving to next page...\n",
      "Finished page 9. Moving to next page...\n",
      "Finished page 10. Moving to next page...\n",
      "Finished page 11. Moving to next page...\n",
      "Finished page 12. Moving to next page...\n",
      "Finished page 13. Moving to next page...\n",
      "Finished page 14. Moving to next page...\n",
      "Finished page 15. Moving to next page...\n",
      "Finished page 16. Moving to next page...\n",
      "Finished page 17. Moving to next page...\n",
      "Finished page 18. Moving to next page...\n",
      "Finished page 19. Moving to next page...\n",
      "Finished page 20. Moving to next page...\n",
      "Successfully scraped 1200 records\n",
      "Finished page 1. Moving to next page...\n",
      "Finished page 2. Moving to next page...\n",
      "Finished page 3. Moving to next page...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common import NoSuchElementException,StaleElementReferenceException\n",
    "from matplotlib import style\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "\n",
    "# Initialize Edge driver\n",
    "try:\n",
    "    driver = webdriver.Edge()\n",
    "    print(\"Edge WebDriver initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Edge WebDriver initialization failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize list of websites (Xisanqi, Qinghe)\n",
    "location_list = ['Xisanqi', 'Qinghe']\n",
    "url_price_list = ['https://esf.fang.com/house-a015277-b02314/i31/', 'https://esf.fang.com/house-a015277-b02315/i31/']\n",
    "url_rent_list = ['https://zu.fang.com/house-a015277-b02314/', 'https://zu.fang.com/house-a015277-b02315/']\n",
    "\n",
    "# Set max number of pages\n",
    "Num_Pages = 20\n",
    "\n",
    "\n",
    "# Scrape price info\n",
    "for count in range(len(location_list)):\n",
    "    # Log on website\n",
    "    driver.get(url_price_list[count])\n",
    "\n",
    "    # Initialize list to store data\n",
    "    data = []\n",
    "\n",
    "    i = 0\n",
    "    while i < Num_Pages:\n",
    "        # Find the container with house listings\n",
    "        shop_list = driver.find_element(By.CLASS_NAME, 'shop_list')\n",
    "    \n",
    "        # Extract all house items\n",
    "        house_items = shop_list.find_elements(By.TAG_NAME, 'dl')\n",
    "    \n",
    "        for item in house_items:\n",
    "            try:\n",
    "                # Extract area (m2)\n",
    "                tel_shop = item.find_element(By.CLASS_NAME, 'tel_shop').text\n",
    "                m2_match = re.search(r'(\\d+(?:\\.\\d+)?)㎡', tel_shop)\n",
    "                m2 = m2_match.group(1) if m2_match else 'N/A'\n",
    "                \n",
    "                # Extract price info\n",
    "                price_right = item.find_element(By.CLASS_NAME, 'price_right')\n",
    "                price = price_right.find_element(By.CSS_SELECTOR, 'span.red b').text\n",
    "                \n",
    "                # Organize extracted data into list\n",
    "                row_data = [m2, price, location_list[count]]\n",
    "                data.append(row_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data: {e}\")\n",
    "                continue\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Click next page button\n",
    "        try:\n",
    "            last = driver.find_elements(By.CLASS_NAME, 'last')\n",
    "            next_page_element = None\n",
    "            \n",
    "            for last_element in last:\n",
    "                if \"下一页\" in last_element.text:\n",
    "                    next_page_element = last_element.find_element(By.TAG_NAME, 'a')\n",
    "                    break\n",
    "            \n",
    "            if next_page_element:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_page_element)\n",
    "                sleep(0.5)  # Wait for page load\n",
    "                print(f\"Finished page {i}. Moving to next page...\")\n",
    "            else:\n",
    "                print(\"Next page link not found\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"No more pages or error finding next page: {e}\")\n",
    "            break\n",
    "\n",
    "    # Convert data to dataframe\n",
    "    columns = ['m2', 'price', 'location']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv(f'{location_list[count]}_Price.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Successfully scraped {len(data)} records\")\n",
    "\n",
    "\n",
    "# Scrape rent info\n",
    "for count in range(len(location_list)):\n",
    "    # Log on website\n",
    "    driver.get(url_rent_list[count])\n",
    "\n",
    "    # Initialize list to store data\n",
    "    data = []\n",
    "\n",
    "    i = 0\n",
    "    while i < Num_Pages:\n",
    "        # Find the container with house listings\n",
    "        houseList = driver.find_element(By.CLASS_NAME, 'houseList')\n",
    "        \n",
    "        # Extract all house items\n",
    "        house_items = houseList.find_elements(By.TAG_NAME, 'dl')\n",
    "    \n",
    "        for item in house_items:\n",
    "            try:\n",
    "                # Extract area (m2)\n",
    "                info = item.find_element(By.CSS_SELECTOR, '.font15.mt12.bold').text\n",
    "                m2_match = re.search(r'(\\d+(?:\\.\\d+)?)㎡', info)\n",
    "                m2 = m2_match.group(1) if m2_match else 'N/A'\n",
    "                \n",
    "                # Extract rent info\n",
    "                rent = item.find_element(By.CLASS_NAME, 'price').text\n",
    "                \n",
    "                # Organize extracted data into list\n",
    "                row_data = [m2, rent, location_list[count]]\n",
    "                data.append(row_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data: {e}\")\n",
    "                continue\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Click next page button\n",
    "        try:\n",
    "            fanye = driver.find_element(By.CLASS_NAME, 'fanye')\n",
    "            all_links = fanye.find_elements(By.TAG_NAME, 'a')\n",
    "            next_page_element = None\n",
    "            \n",
    "            for link in all_links:\n",
    "                if link.text == \"下一页\":\n",
    "                    next_page_element = link\n",
    "                    break\n",
    "            \n",
    "            if next_page_element:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_page_element)\n",
    "                sleep(0.5)  # Wait for page load\n",
    "                print(f\"Finished page {i}. Moving to next page...\")\n",
    "            else:\n",
    "                print(\"Next page link not found\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"No more pages or error finding next page: {e}\")\n",
    "            break\n",
    "\n",
    "    # Convert data to dataframe\n",
    "    columns = ['m2', 'rent', 'location']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv(f'{location_list[count]}_Rent.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Successfully scraped {len(data)} records\")\n",
    "\n",
    "\n",
    "# Close browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e834dfe-537f-4edb-a740-688c5ffa1330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
