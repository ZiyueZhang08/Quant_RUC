{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abc07e5-95f0-48f8-af30-6d70e0b22d75",
   "metadata": {},
   "source": [
    "## Priceæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2f15503-b4a3-4ca4-9703-22d915fe30e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œç¼–ç : utf-8-sigï¼Œåˆ†éš”ç¬¦: Commaã€‚\n",
      "âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œç¼–ç : utf-8-sigï¼Œåˆ†éš”ç¬¦: Commaã€‚\n",
      "Train shape: (103871, 55)\n",
      "Test shape: (34017, 55)\n",
      "\n",
      "--- ç¦»ç¾¤å€¼å¤„ç†ç»“æœ ---\n",
      "åŸå§‹æ ·æœ¬æ•°: 103871\n",
      "ç§»é™¤ç¦»ç¾¤å€¼åæ ·æœ¬æ•°: 102726 (è¯·æŠ¥å‘Šæ­¤æ•°å­—)\n",
      "\n",
      "--- ä¿®æ­£åçš„ç‰¹å¾ ---\n",
      "æ•°å€¼ç‰¹å¾ (20): ['å»ºç­‘é¢ç§¯', 'å¥—å†…é¢ç§¯', 'lon', 'lat', 'æˆ¿å±‹æ€»æ•°', 'æ¥¼æ ‹æ€»æ•°', 'ç»¿ åŒ– ç‡', 'å®¹ ç§¯ ç‡', 'ç‰© ä¸š è´¹_å‡å€¼', 'ç‡ƒæ°”è´¹_å‡å€¼', 'ä¾›çƒ­è´¹_å‡å€¼', 'åœè½¦è´¹ç”¨_å‡å€¼', 'å»ºç­‘å¹´ä»£_å‡å€¼', 'åœè½¦ä½_å‡å€¼', 'æ€»æ¥¼å±‚æ•°', 'å®¤', 'å…', 'å«', 'å¥—å†…é¢ç§¯_æ¯”', 'å®¹ç§¯ç‡_sq']\n",
      "åˆ†ç±»ç‰¹å¾ (19): ['ç¯çº¿', 'ä¾›ç”µ', 'ä¾›æ°´', 'å¹´ä»½', 'æ¢¯æˆ·æ¯”ä¾‹', 'äº§æƒæ‰€å±', 'æˆ¿å±‹æœå‘', 'ç‰©ä¸šç±»åˆ«', 'åŸå¸‚', 'æ¿å—', 'äº¤æ˜“æƒå±', 'ç¯çº¿ä½ç½®', 'å»ºç­‘ç»“æ„', 'åŒºåŸŸ', 'è£…ä¿®æƒ…å†µ', 'æ¥¼å±‚ä½ç½®', 'é…å¤‡ç”µæ¢¯', 'åŒºå¿', 'ä¾›æš–']\n",
      "\n",
      "--- æ­¥éª¤ B (ä¼˜åŒ–): ç«‹å³æ‰§è¡Œé¢„å¤„ç† ---\n",
      "--- Transforming Data ---\n",
      "Data transformed. Shape: (102726, 418)\n",
      "\n",
      "--- æ­¥éª¤ 9 (ä¼˜åŒ–): åœ¨å·²è½¬æ¢æ•°æ®ä¸Šè¿è¡Œ GSCV å’Œè¯„ä¼° ---\n",
      "\n",
      "--- å¼€å§‹å¤„ç†: OLS ---\n",
      "--- æ‹Ÿåˆ (æ—  GSCV): OLS ---\n",
      "--- OLS æ‰‹åŠ¨ CV (æ—  GSCV) ---\n",
      "--- OLS CV MAE: 410888.11 ---\n",
      "  Intercept: 14.3715\n",
      "  Coefficient Stats: Mean=-0.0101, Std=0.7568\n",
      "\n",
      "--- å¼€å§‹å¤„ç†: LASSO ---\n",
      "--- è¿è¡Œ GridSearchCV: LASSO ---\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "âœ… LASSO Best Params found: {'alpha': np.float64(0.0001)}\n",
      "--- LASSO CV MAE: 418347.83 ---\n",
      "  Intercept: 13.9289\n",
      "  Coefficient Stats: Mean=0.0111, Std=0.1882\n",
      "\n",
      "--- å¼€å§‹å¤„ç†: Ridge ---\n",
      "--- è¿è¡Œ GridSearchCV: Ridge ---\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "âœ… Ridge Best Params found: {'alpha': np.float64(0.01)}\n",
      "--- Ridge CV MAE: 410824.15 ---\n",
      "  Intercept: 14.3632\n",
      "  Coefficient Stats: Mean=-0.0099, Std=0.7424\n",
      "\n",
      "--- å¼€å§‹å¤„ç†: ElasticNet ---\n",
      "--- è¿è¡Œ GridSearchCV: ElasticNet ---\n",
      "Fitting 6 folds for each of 4 candidates, totalling 24 fits\n",
      "âœ… ElasticNet Best Params found: {'alpha': np.float64(0.0001), 'l1_ratio': 0.1}\n",
      "--- ElasticNet CV MAE: 416135.28 ---\n",
      "  Intercept: 14.1840\n",
      "  Coefficient Stats: Mean=-0.0004, Std=0.1893\n",
      "\n",
      "--- Determining Best Linear Model (based on Cross-validation MAE) ---\n",
      "æœ€ä½³æ¨¡å‹ï¼ˆåŸºäº CV MAEï¼‰: Ridge\n",
      "\n",
      "--- 10. Model Performance Summary (MAE and RMAE for Original Price Level) ---\n",
      "| Metrics           |   In-sample MAE |   In-sample RMAE |   Out-of-sample MAE |   Out-of-sample RMAE |   Cross-validation MAE |   Cross-validation RMAE |\n",
      "|:------------------|----------------:|-----------------:|--------------------:|---------------------:|-----------------------:|------------------------:|\n",
      "| OLS               |          410056 |             0.19 |              405741 |                 0.19 |                 410888 |                    0.19 |\n",
      "| LASSO             |          417957 |             0.2  |              414388 |                 0.19 |                 418348 |                    0.2  |\n",
      "| Best Linear Model |          410019 |             0.19 |              405729 |                 0.19 |                 410824 |                    0.19 |\n",
      "| ElasticNet        |          415565 |             0.2  |              411454 |                 0.19 |                 416135 |                    0.2  |\n",
      "\n",
      "--- Selecting Final Model for Prediction: Ridge ---\n",
      "\n",
      "--- ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹: Ridge ---\n",
      "é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼ (å‰ 5 ä¸ª): [17.03597019 14.99497064 15.60520153 14.76060152 16.40738321]\n",
      "å¯¹æ•°ä»·æ ¼ç»Ÿè®¡ (æœ‰é™å€¼): Min=11.74, Max=17.17, Mean=14.42, Std=0.80\n",
      "æœ€ç»ˆé¢„æµ‹ä»·æ ¼ (å‰ 5 ä¸ª): [25039625.67546035  3252616.58613584  5987600.86705289  2573046.42849863\n",
      " 13354755.99748335]\n",
      "ğŸ¯ Prediction file saved as **prediction_price.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\n",
      "ğŸ“Š Model performance saved as **performance_table_price.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\n"
     ]
    }
   ],
   "source": [
    "# 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer #FunctionTransformer: ç”¨äºå°†è‡ªå®šä¹‰å‡½æ•°ï¼ˆå¦‚ np.log1pï¼‰åº”ç”¨ä¸º Pipeline ä¸­çš„ä¸€ä¸ªæ­¥éª¤ã€‚\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import clone\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ğŸ¯è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°ï¼ˆè®¡ç®—åŸå§‹ä»·æ ¼å°ºåº¦ä¸Šçš„MAEä¸RMSEï¼‰\n",
    "def calculate_mae_rmse_original(y_log_true, y_log_pred):\n",
    "    mae, rmse = np.nan, np.nan\n",
    "    try:\n",
    "        y_log_true_np = np.asarray(y_log_true).flatten()\n",
    "        y_log_pred_np = np.asarray(y_log_pred).flatten()\n",
    "        #æ¸…ç†nan\\infï¼Œé˜²æ­¢åç»­æŒ‡æ•°è®¡ç®—æº¢å‡º\n",
    "        log_max = np.log1p(np.finfo(np.float64).max / 10)\n",
    "        y_log_true_np = np.nan_to_num(y_log_true_np, nan=0.0, posinf=log_max, neginf=-700)\n",
    "        y_log_pred_np = np.nan_to_num(y_log_pred_np, nan=0.0, posinf=log_max, neginf=-700)\n",
    "        y_log_true_np = np.clip(y_log_true_np, -700, log_max)\n",
    "        y_log_pred_np = np.clip(y_log_pred_np, -700, log_max)\n",
    "        \n",
    "        y_price_pred = np.expm1(y_log_pred_np)\n",
    "        y_price_true = np.expm1(y_log_true_np)\n",
    "        y_price_pred[y_price_pred < 0] = 0 #ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿï¼Œè£å‰ªä¸º0\n",
    "\n",
    "        large_finite_val = np.finfo(np.float64).max / 10\n",
    "        y_price_pred = np.nan_to_num(y_price_pred, nan=0.0, posinf=large_finite_val, neginf=0.0)\n",
    "        y_price_true = np.nan_to_num(y_price_true, nan=0.0, posinf=large_finite_val, neginf=0.0)\n",
    "        y_price_pred = np.clip(y_price_pred, 0, large_finite_val)\n",
    "        y_price_true = np.clip(y_price_true, 0, large_finite_val)\n",
    "\n",
    "        if np.isnan(y_price_pred).any() or np.isnan(y_price_true).any():\n",
    "            y_price_pred = np.nan_to_num(y_price_pred, nan=0.0)\n",
    "            y_price_true = np.nan_to_num(y_price_true, nan=0.0)\n",
    "\n",
    "        mae = mean_absolute_error(y_price_true, y_price_pred)\n",
    "        mse = mean_squared_error(y_price_true, y_price_pred)\n",
    "        if mse < 0 or not np.isfinite(mse): rmse = np.nan\n",
    "        else: rmse = np.sqrt(mse) # è®¡ç®—RMSE\n",
    "        return mae, rmse\n",
    "    except (ValueError, OverflowError, TypeError) as e:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# --- ğŸš€ ä¼˜åŒ–ç‚¹ 1: é‡æ–°å®šä¹‰ mae_metric (ç”¨äº make_scorer) ---\n",
    "# GridSearchCV é»˜è®¤è®¤ä¸ºâ€œåˆ†æ•°è¶Šé«˜è¶Šå¥½â€ï¼ŒMAEæ˜¯ä¸€ä¸ªè¯¯å·®æŒ‡æ ‡ï¼Œè¶Šä½è¶Šå¥½ã€‚\n",
    "def mae_metric(y_log_true, y_log_pred):\n",
    "    \"\"\"æŒ‡æ ‡ï¼šè¿”å›è´Ÿ MAE (éœ€æœ€å¤§åŒ–)\"\"\"\n",
    "    mae, _ = calculate_mae_rmse_original(y_log_true, y_log_pred)\n",
    "    if np.isnan(mae):\n",
    "        return -np.finfo(np.float64).max \n",
    "    return -mae\n",
    "\n",
    "# --- ğŸš€ ä¼˜åŒ–ç‚¹ 2: å‘é‡åŒ–ç‰¹å¾æ¸…ç†å‡½æ•° ---\n",
    "def clean_complex_features_optimized(df):\n",
    "    \"\"\"æ‰§è¡Œå•ä½è½¬æ¢ã€æ­£åˆ™æå–å’Œåˆå§‹ç‰¹å¾æ¸…ç† (å‘é‡åŒ–ç‰ˆæœ¬)\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. å•ä½è½¬æ¢ (ä¸å˜)\n",
    "    def clean_unit(series, unit):\n",
    "        cleaned = series.astype(str).str.replace(unit, '', regex=False).str.strip()\n",
    "        cleaned = cleaned.replace(['', 'æš‚æ— ', 'null', 'None'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    \n",
    "    for col, unit in [('å»ºç­‘é¢ç§¯', 'ã¡'), ('å¥—å†…é¢ç§¯', 'ã¡'), ('æˆ¿å±‹æ€»æ•°', 'æˆ·'), ('æ¥¼æ ‹æ€»æ•°', 'æ ‹')]:\n",
    "        if col in df.columns:\n",
    "            df[col] = clean_unit(df[col], unit)\n",
    "    if 'ç»¿ åŒ– ç‡' in df.columns:\n",
    "        df['ç»¿ åŒ– ç‡'] = clean_unit(df['ç»¿ åŒ– ç‡'], '%')\n",
    "        if df['ç»¿ åŒ– ç‡'] is not None:\n",
    "            df['ç»¿ åŒ– ç‡'] = df['ç»¿ åŒ– ç‡'] / 100\n",
    "\n",
    "    # 2. æå–æ•°å­—èŒƒå›´å‡å€¼ (ä½¿ç”¨ .str.extractall å‘é‡åŒ–)\n",
    "    cols_to_extract = ['ç‰© ä¸š è´¹', 'ç‡ƒæ°”è´¹', 'ä¾›çƒ­è´¹', 'åœè½¦è´¹ç”¨', 'å»ºç­‘å¹´ä»£', 'åœè½¦ä½']\n",
    "    for col in cols_to_extract:\n",
    "        if col in df.columns:\n",
    "            cleaned_series = df[col].astype(str).replace(['æš‚æ— ', 'null', 'None', ''], np.nan)\n",
    "            all_nums = cleaned_series.str.extractall(r\"(\\d+(?:\\.\\d+)?)\")[0].astype(float)\n",
    "            avg_series = all_nums.groupby(level=0).mean()\n",
    "            df[f'{col}_å‡å€¼'] = avg_series\n",
    "            df = df.drop(columns=[col], errors='ignore')\n",
    "    \n",
    "    for col in df.filter(like='_å‡å€¼').columns:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].dropna().median()\n",
    "            if pd.isna(median_val): median_val = 0\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "\n",
    "    # 3. æå–æ¥¼å±‚\n",
    "    if 'æ‰€åœ¨æ¥¼å±‚' in df.columns:\n",
    "        df['æ¥¼å±‚ä½ç½®'] = df['æ‰€åœ¨æ¥¼å±‚'].astype(str).str.extract(r'([A-Za-z\\u4e00-\\u9fa5]+)').fillna('æœªçŸ¥')\n",
    "        df['æ€»æ¥¼å±‚æ•°'] = pd.to_numeric(df['æ‰€åœ¨æ¥¼å±‚'].astype(str).str.extract(r'\\(å…±(\\d+)å±‚\\)')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df = df.drop(columns=['æ‰€åœ¨æ¥¼å±‚'])\n",
    "\n",
    "    # 4. æå–æˆ·å‹\n",
    "    if 'æˆ¿å±‹æˆ·å‹' in df.columns:\n",
    "        df['å®¤'] = pd.to_numeric(df['æˆ¿å±‹æˆ·å‹'].astype(str).str.extract(r'(\\d+)å®¤')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df['å…'] = pd.to_numeric(df['æˆ¿å±‹æˆ·å‹'].astype(str).str.extract(r'(\\d+)å…')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df['å«'] = pd.to_numeric(df['æˆ¿å±‹æˆ·å‹'].astype(str).str.extract(r'(\\d+)å«')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df = df.drop(columns=['æˆ¿å±‹æˆ·å‹'])\n",
    "\n",
    "    # 5. äº¤äº’é¡¹/å¤šé¡¹å¼ç‰¹å¾ (ä¸å˜)\n",
    "    if 'å»ºç­‘é¢ç§¯' in df.columns and 'å¥—å†…é¢ç§¯' in df.columns:\n",
    "        df['å»ºç­‘é¢ç§¯_clean'] = df['å»ºç­‘é¢ç§¯'].replace(0, np.nan)\n",
    "        df['å¥—å†…é¢ç§¯_æ¯”'] = (df['å¥—å†…é¢ç§¯'] / df['å»ºç­‘é¢ç§¯_clean']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        df = df.drop(columns=['å»ºç­‘é¢ç§¯_clean'])\n",
    "    if 'å®¹ ç§¯ ç‡' in df.columns:\n",
    "        df['å®¹ ç§¯ ç‡'] = pd.to_numeric(df['å®¹ ç§¯ ç‡'], errors='coerce').fillna(df['å®¹ ç§¯ ç‡'].median())\n",
    "        df['å®¹ç§¯ç‡_sq'] = df['å®¹ ç§¯ ç‡'] ** 2\n",
    "    \n",
    "    # å¡«å……æ•°å€¼å‹ç¼ºå¤± (å¯¹æ‰€æœ‰æ•°å€¼åˆ—è¿›è¡Œæœ€ç»ˆçš„ä¸­ä½æ•°å¡«å……)\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # 6. åˆ é™¤åˆ— (ä¸å˜)\n",
    "    drop_cols = [\n",
    "        'æŠµæŠ¼ä¿¡æ¯', 'åˆ«å¢…ç±»å‹', 'äº¤æ˜“æ—¶é—´', 'ä¸Šæ¬¡äº¤æ˜“',\n",
    "        'æˆ¿å±‹ä¼˜åŠ¿', 'æ ¸å¿ƒå–ç‚¹', 'æˆ·å‹ä»‹ç»', 'å‘¨è¾¹é…å¥—', 'äº¤é€šå‡ºè¡Œ', 'å®¢æˆ·åé¦ˆ', 'äº§æƒæè¿°', 'æˆ¿å±‹ç”¨é€”',\n",
    "        'coord_x', 'coord_y', 'ç‰©ä¸šåŠå…¬ç”µè¯', 'å¼€å‘å•†', 'ç‰©ä¸šå…¬å¸', 'æˆ¿å±‹å¹´é™',\n",
    "        'åœè½¦è´¹ç”¨' \n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # å¡«å……ç¼ºå¤±åˆ†ç±»ç‰¹å¾ (ä¸å˜)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].fillna('ç¼ºå¤±')\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2ï¸âƒ£ æ•°æ®è¯»å– (PRICE æ–‡ä»¶)\n",
    "TRAIN_SNIPPET = \"ruc_Class25Q2_train_price.csv\"\n",
    "TEST_SNIPPET = \"ruc_Class25Q2_test_price.csv\"\n",
    "TARGET_NAME = \"Price\"\n",
    "TRAIN_ENCODING = 'utf-8-sig'\n",
    "TEST_ENCODING = 'utf-8-sig'\n",
    "\n",
    "# (åŠ è½½é€»è¾‘ä¸å˜)\n",
    "try:\n",
    "    train_price = pd.read_csv(TRAIN_SNIPPET, low_memory=False, encoding=TRAIN_ENCODING, sep=',')\n",
    "    print(f\"âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TRAIN_ENCODING}ï¼Œåˆ†éš”ç¬¦: Commaã€‚\")\n",
    "except Exception:\n",
    "    train_price = pd.read_csv(TRAIN_SNIPPET, low_memory=False, encoding=TRAIN_ENCODING, sep='\\t')\n",
    "    print(f\"âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TRAIN_ENCODING}ï¼Œåˆ†éš”ç¬¦: Tabã€‚\")\n",
    "try:\n",
    "    test_price = pd.read_csv(TEST_SNIPPET, low_memory=False, encoding=TEST_ENCODING, sep=',')\n",
    "    print(f\"âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TEST_ENCODING}ï¼Œåˆ†éš”ç¬¦: Commaã€‚\")\n",
    "except Exception:\n",
    "    test_price = pd.read_csv(TEST_SNIPPET, low_memory=False, encoding=TEST_ENCODING, sep='\\t')\n",
    "    print(f\"âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TEST_ENCODING}ï¼Œåˆ†éš”ç¬¦: Tabã€‚\")\n",
    "\n",
    "train_data = train_price\n",
    "test_data = test_price\n",
    "\n",
    "# 3ï¸âƒ£ åˆæ­¥æ£€æŸ¥ä¸æ¸…æ´— \n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "target = TARGET_NAME\n",
    "if target not in train_data.columns:\n",
    "    raise ValueError(f\"ç›®æ ‡å˜é‡ '{target}' ä¸åœ¨è®­ç»ƒé›†åˆ—ä¸­ã€‚\")\n",
    "KEEP_COLS = [target, \"ç‰© ä¸š è´¹\", \"åœè½¦è´¹ç”¨\"]\n",
    "#è¯†åˆ«æ•°æ®æ³„éœ²ç‰¹å¾ï¼Œæ’é™¤åœ¨KEEP_COLSä¸­çš„é‡è¦åˆ—ï¼Œåˆ é™¤æ³„éœ²æœªæ¥ä¿¡æ¯çš„åˆ—\n",
    "leak_cols = [c for c in train_data.columns if (\"comm\" in c.lower() or \"price\" in c.lower()) and c not in KEEP_COLS]\n",
    "train_data.drop(columns=[col for col in leak_cols if col in train_data.columns], inplace=True, errors='ignore')\n",
    "test_data.drop(columns=[col for col in leak_cols if col in test_data.columns], inplace=True, errors='ignore')\n",
    "\n",
    "all_data = pd.concat([train_data.drop(columns=[target]), test_data], keys=['train', 'test'])\n",
    "all_data_cleaned = clean_complex_features_optimized(all_data) \n",
    "for col in all_data_cleaned.select_dtypes(include='object').columns:\n",
    "    all_data_cleaned[col] = all_data_cleaned[col].astype('category') #æ‰€æœ‰æ–‡æœ¬åˆ—æ¢ä¸ºåˆ†ç±»æ•°æ®ç±»å‹\n",
    "X_train_raw = all_data_cleaned.loc['train'].copy() #è®­ç»ƒç‰¹å¾\n",
    "X_test_final = all_data_cleaned.loc['test'].copy() #æµ‹è¯•ç‰¹å¾\n",
    "y_train_log = np.log1p(train_data[target])\n",
    "y_train_price = train_data[target]\n",
    "\n",
    "# 4ï¸âƒ£ ç¦»ç¾¤å€¼å¤„ç† \n",
    "Q1 = y_train_log.quantile(0.25)\n",
    "Q3 = y_train_log.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outlier_mask = (y_train_log >= lower_bound) & (y_train_log <= upper_bound)\n",
    "X_train_no_outliers = X_train_raw.loc[outlier_mask].copy()\n",
    "y_train_log_no_outliers = y_train_log.loc[outlier_mask].copy()\n",
    "y_train_price_no_outliers = y_train_price.loc[outlier_mask].copy() #ä½¿ç”¨æ©ç è¿‡æ»¤ç¦»ç¾¤å€¼å¯¹åº”çš„æ ·æœ¬\n",
    "initial_count = len(train_data)\n",
    "final_count = len(X_train_no_outliers)\n",
    "print(f\"\\n--- ç¦»ç¾¤å€¼å¤„ç†ç»“æœ ---\")\n",
    "print(f\"åŸå§‹æ ·æœ¬æ•°: {initial_count}\")\n",
    "print(f\"ç§»é™¤ç¦»ç¾¤å€¼åæ ·æœ¬æ•°: {final_count} (è¯·æŠ¥å‘Šæ­¤æ•°å­—)\")\n",
    "\n",
    "# 5ï¸âƒ£ åŒºåˆ†å˜é‡ç±»å‹ \n",
    "auto_numeric_features = X_train_no_outliers.select_dtypes(include=[np.number]).columns.tolist()\n",
    "auto_categorical_features = X_train_no_outliers.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "known_categorical_numerics = ['åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'åŒºå¿', 'å¹´ä»½']\n",
    "categorical_features = list(set(auto_categorical_features + [col for col in known_categorical_numerics if col in X_train_no_outliers.columns])) #set()ï¼šå»é‡ï¼Œé¿å…é‡å¤ç‰¹å¾/æ¡ä»¶åˆ¤æ–­ï¼šåªæ·»åŠ æ•°æ®é›†ä¸­å®é™…å­˜åœ¨çš„ç‰¹å¾\n",
    "numeric_features = [f for f in auto_numeric_features if f not in known_categorical_numerics]\n",
    "if 'ID' in numeric_features: numeric_features.remove('ID')\n",
    "if 'ID' in categorical_features: categorical_features.remove('ID')\n",
    "print(f\"\\n--- ä¿®æ­£åçš„ç‰¹å¾ ---\")\n",
    "print(f\"æ•°å€¼ç‰¹å¾ ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"åˆ†ç±»ç‰¹å¾ ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# 6ï¸âƒ£ ç‰¹å¾é¢„å¤„ç† Pipeline å®šä¹‰ \n",
    "log_transformer = FunctionTransformer(lambda x: np.log1p(np.maximum(x, 0)), validate=False)\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer1', SimpleImputer(strategy='median')), # ç¬¬ä¸€æ­¥ï¼šä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼\n",
    "    ('logtransform', log_transformer),              # ç¬¬äºŒæ­¥ï¼šå¯¹æ•°å˜æ¢\n",
    "    ('imputer2', SimpleImputer(strategy='median')), # ç¬¬ä¸‰æ­¥ï¼šå†æ¬¡å¡«å……ï¼Œå¤„ç†å˜æ¢åå¯èƒ½äº§ç”Ÿçš„å¼‚å¸¸å€¼\n",
    "    ('scaler', StandardScaler())                    # ç¬¬å››æ­¥ï¼šæ ‡å‡†åŒ–ï¼Œä½¿ç‰¹å¾å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œä¾¿äºæ¨¡å‹ä¼˜åŒ–\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='ç¼ºå¤±')),    # å¡«å……ç¼ºå¤±\n",
    "    ('astype_str', FunctionTransformer(lambda x: x.astype(str), validate=False)), # è½¬å­—ç¬¦ä¸²\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', max_categories=50)) # æœªè§è¿‡çš„ç±»åˆ«æ—¶å¿½ç•¥ï¼Œé¿å…æŠ¥é”™ï¼›é™åˆ¶æœ€å¤§ç±»åˆ«æ•°ï¼Œé˜²æ­¢ç»´åº¦çˆ†ç‚¸\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features), # æ•°å€¼ç‰¹å¾æµæ°´çº¿\n",
    "        ('cat', categorical_transformer, categorical_features) # åˆ†ç±»ç‰¹å¾æµæ°´çº¿\n",
    "    ],\n",
    "    remainder='drop',      # å¤„ç†å…¶ä»–åˆ—çš„æ–¹å¼\n",
    "    sparse_threshold=0.3   # ç¨€ç–çŸ©é˜µé˜ˆå€¼\n",
    ")\n",
    "\n",
    "# 7ï¸âƒ£ æ¨¡å‹å®šä¹‰ä¸è¶…å‚æ•°ï¼ˆé‡ç‚¹æ­¥éª¤ï¼‰\n",
    "models_base = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'LASSO': Lasso(max_iter=10000, random_state=111),\n",
    "    # --- â¬‡ï¸ æ ¸å¿ƒä¿®æ­£ï¼šå·²åˆ é™¤æ‰€æœ‰å¯¼è‡´å´©æºƒçš„å‚æ•° (tol, random_state, solver) â¬‡ï¸ ---\n",
    "    'Ridge': Ridge(max_iter=10000), \n",
    "    'ElasticNet': ElasticNet(max_iter=10000, random_state=111)\n",
    "}\n",
    "\n",
    "# ä¸ºéœ€è¦è°ƒä¼˜çš„æ¨¡å‹æŒ‡å®šè¶…å‚æ•°çš„æœç´¢èŒƒå›´\n",
    "param_grids = {\n",
    "    'LASSO': {'alpha': np.logspace(-4, 0, 3)}, #[0.0001, 0.01, 1]äº§ç”Ÿä¸€ç³»åˆ—å¯¹æ•°ç­‰è·çš„å€¼ï¼Œæœç´¢alphaå¸¸ç”¨æ–¹æ³•\n",
    "    'Ridge': {'alpha': np.logspace(-2, 2, 3)}, #[0.01, 1, 100]\n",
    "    'ElasticNet': {'alpha': np.logspace(-4, 0, 2), 'l1_ratio': [0.1, 0.9]},\n",
    "}\n",
    "\n",
    "# --- ğŸš€ ä¼˜åŒ–ç‚¹ 4: è°ƒæ¢é¡ºåºï¼Œå…ˆåšé¢„å¤„ç† (åŸæ­¥éª¤ B) ---\n",
    "print(\"\\n--- æ­¥éª¤ B (ä¼˜åŒ–): ç«‹å³æ‰§è¡Œé¢„å¤„ç† ---\")\n",
    "preprocessor_final = clone(preprocessor)\n",
    "preprocessor_final.fit(X_train_no_outliers)\n",
    "print(\"--- Transforming Data ---\")\n",
    "X_full_transformed = preprocessor_final.transform(X_train_no_outliers)\n",
    "X_test_transformed = preprocessor_final.transform(X_test_final)  #å°†è®­ç»ƒæ•°æ®ã€æµ‹è¯•æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ä½¿ç”¨çš„æ•°å€¼çŸ©é˜µ\n",
    "y_full_log_np = y_train_log_no_outliers.values\n",
    "y_full_price_np = y_train_price_no_outliers.values\n",
    "mean_price = np.mean(y_full_price_np)\n",
    "print(f\"Data transformed. Shape: {X_full_transformed.shape}\")\n",
    "\n",
    "# --- ğŸš€ ä¼˜åŒ–ç‚¹ 5: åœ¨é¢„å¤„ç†åçš„æ•°æ®ä¸Šè¿›è¡Œ GSCV å’Œè¯„ä¼° ---\n",
    "print(\"\\n--- æ­¥éª¤ 9 (ä¼˜åŒ–): åœ¨å·²è½¬æ¢æ•°æ®ä¸Šè¿è¡Œ GSCV å’Œè¯„ä¼° ---\")\n",
    "X_train_transformed, X_valid_transformed, y_train_log_split, y_valid_log_split = train_test_split(\n",
    "    X_full_transformed, y_full_log_np, test_size=0.2, random_state=111\n",
    ")\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111) #6æŠ˜äº¤å‰éªŒè¯ï¼šå¹³è¡¡åå·®/æ–¹å·®\n",
    "custom_mae_scorer = make_scorer(mae_metric, greater_is_better=True) \n",
    "\n",
    "results = []\n",
    "fitted_models = {}\n",
    "\n",
    "for name, model_base in models_base.items():\n",
    "    print(f\"\\n--- å¼€å§‹å¤„ç†: {name} ---\")\n",
    "    final_model = clone(model_base)\n",
    "    \n",
    "    if name in param_grids:\n",
    "        print(f\"--- è¿è¡Œ GridSearchCV: {name} ---\")\n",
    "        gscv = GridSearchCV(\n",
    "            final_model, param_grids[name], scoring=custom_mae_scorer,\n",
    "            cv=kf, n_jobs=-1, verbose=1, refit=True \n",
    "        )\n",
    "        gscv.fit(X_full_transformed, y_full_log_np) #ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯\n",
    "        print(f\"âœ… {name} Best Params found: {gscv.best_params_}\")\n",
    "        cv_mae = -gscv.best_score_  #å°†ç»“æœä¸­æå–æœ€ä½³äº¤å‰éªŒè¯\n",
    "        final_model = gscv.best_estimator_ \n",
    "        fitted_models[name] = final_model\n",
    "        \n",
    "    else: # (OLS é€»è¾‘ä¸å˜)\n",
    "        print(f\"--- æ‹Ÿåˆ (æ—  GSCV): {name} ---\")\n",
    "        final_model.fit(X_full_transformed, y_full_log_np)\n",
    "        print(f\"--- {name} æ‰‹åŠ¨ CV (æ—  GSCV) ---\")\n",
    "        fold_mae_scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_transformed, y_full_log_np)):\n",
    "            X_train_fold, X_val_fold = X_full_transformed[train_idx], X_full_transformed[val_idx]\n",
    "            y_train_fold, y_val_fold = y_full_log_np[train_idx], y_full_log_np[val_idx]\n",
    "            fold_model = clone(final_model).fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold_log = fold_model.predict(X_val_fold)\n",
    "            fold_mae_scores.append(-mae_metric(y_val_fold, y_pred_fold_log))\n",
    "        \n",
    "        cv_mae = np.nanmean(fold_mae_scores)\n",
    "        fitted_models[name] = final_model\n",
    "\n",
    "    print(f\"--- {name} CV MAE: {cv_mae:.2f} ---\")\n",
    "\n",
    "    # --- æ€§èƒ½è®¡ç®— (In-sample / Out-of-sample) ---\n",
    "    try:\n",
    "        y_pred_train_log = final_model.predict(X_train_transformed)\n",
    "        mae_train, _ = calculate_mae_rmse_original(y_train_log_split, y_pred_train_log)\n",
    "    except Exception: mae_train = np.nan\n",
    "    try:\n",
    "        y_pred_valid_log = final_model.predict(X_valid_transformed)\n",
    "        mae_valid, _ = calculate_mae_rmse_original(y_valid_log_split, y_pred_valid_log)\n",
    "    except Exception: mae_valid = np.nan\n",
    "    \n",
    "    results.append({\n",
    "        \"Metrics\": name,\n",
    "        \"In-sample MAE\": mae_train,     # è®­ç»ƒé›†æ€§èƒ½\n",
    "        \"In-sample RMAE\": mae_train / mean_price if mean_price != 0 else np.nan,\n",
    "        \"Out-of-sample MAE\": mae_valid, # éªŒè¯é›†æ€§èƒ½\n",
    "        \"Out-of-sample RMAE\": mae_valid / mean_price if mean_price != 0 else np.nan,\n",
    "        \"Cross-validation MAE\": cv_mae, # äº¤å‰éªŒè¯æ€§èƒ½\n",
    "        \"Cross-validation RMAE\": cv_mae / mean_price if mean_price != 0 and not np.isnan(cv_mae) else np.nan\n",
    "    })\n",
    "    \n",
    "    # è¯Šæ–­ (ä¸å˜)\n",
    "    print(f\"  Intercept: {final_model.intercept_:.4f}\")\n",
    "    if hasattr(final_model, 'coef_'):\n",
    "        coefs = final_model.coef_\n",
    "        if issparse(coefs): coefs = coefs.toarray().flatten()\n",
    "        print(f\"  Coefficient Stats: Mean={np.mean(coefs):.4f}, Std={np.std(coefs):.4f}\")\n",
    "        if np.allclose(coefs, 0, atol=1e-5):\n",
    "            print(f\"  âš ï¸ è­¦å‘Š: æ¨¡å‹ {name} çš„ç³»æ•°å‡ ä¹å…¨ä¸ºé›¶ï¼\")\n",
    "\n",
    "# ğŸ”Ÿ è¾“å‡ºç»“æœè¡¨ (ä¸å˜)\n",
    "result_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Determining Best Linear Model (based on Cross-validation MAE) ---\")\n",
    "valid_cv_mae_indices = result_df[\"Cross-validation MAE\"].dropna().index\n",
    "best_model_name_final = 'OLS'\n",
    "if not valid_cv_mae_indices.empty:\n",
    "    best_model_idx = result_df.loc[valid_cv_mae_indices, \"Cross-validation MAE\"].idxmin() #åœ¨æ‰€æœ‰æœ‰æ•ˆçš„äº¤å‰éªŒè¯ MAE ä¸­ï¼Œæ‰¾åˆ°æœ€å°å€¼å¯¹åº”çš„ç´¢å¼•ã€‚.idxmin() å‡½æ•°è¿”å›æœ€å°å€¼çš„ç´¢å¼•ã€‚\n",
    "    best_model_name_orig = result_df.loc[best_model_idx, \"Metrics\"] #æ ¹æ®æ‰¾åˆ°çš„æœ€å°MAEçš„ç´¢å¼•ï¼Œä»\"Metrics\"åˆ—ä¸­è·å–è¯¥æ¨¡å‹çš„åŸå§‹åç§°ï¼ˆæ¯”å¦‚'Ridge'ï¼‰ã€‚\n",
    "    if best_model_name_orig in fitted_models and fitted_models[best_model_name_orig] is not None: #æ£€æŸ¥è¿™ä¸ªæ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ï¼ˆæ¯”å¦‚ 'Ridge'ï¼‰æ˜¯å¦åœ¨å‰ä¸€æ­¥éª¤ä¸­æˆåŠŸè®­ç»ƒå¹¶è¢«ä¿å­˜åœ¨äº†fitted_modelså­—å…¸é‡Œã€‚\n",
    "        #å¦‚æœæˆåŠŸäº†\n",
    "        result_df.loc[result_df['Metrics'] == best_model_name_orig, 'Metrics'] = 'Best Linear Model' #åœ¨result_dfä¸­ï¼Œå°†è¿™ä¸ªæœ€ä½³æ¨¡å‹çš„åå­—ï¼ˆæ¯”å¦‚ 'Ridge'ï¼‰é‡å‘½åä¸º'Best Linear Model'ï¼Œæ–¹ä¾¿åœ¨æœ€ç»ˆè¡¨æ ¼ä¸­è¯†åˆ«ã€‚\n",
    "        best_model_name_final = best_model_name_orig \n",
    "        print(f\"æœ€ä½³æ¨¡å‹ï¼ˆåŸºäº CV MAEï¼‰: {best_model_name_final}\")\n",
    "    else:\n",
    "        #å¦‚æœå¤±è´¥äº†\n",
    "        print(f\"è­¦å‘Š: ç¡®å®šçš„æœ€ä½³æ¨¡å‹ '{best_model_name_orig}' (CV MAE) è®­ç»ƒå¤±è´¥æˆ–ä¸å­˜åœ¨ã€‚æ­£åœ¨å›é€€ã€‚\")\n",
    "        available_models = [m for m in ['OLS', 'Ridge', 'LASSO', 'ElasticNet'] if m in fitted_models and fitted_models[m] is not None]\n",
    "        #åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹ï¼ŒæŒ‰ OLS, Ridge, LASSO, ElasticNet çš„ä¼˜å…ˆé¡ºåºæ’åˆ—ã€‚\n",
    "        if available_models: #å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹æˆåŠŸè®­ç»ƒäº†\n",
    "            best_model_name_final = available_models[0] #é€‰æ‹©åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªï¼ˆå³ä¼˜å…ˆçº§æœ€é«˜çš„ï¼‰å¯ç”¨æ¨¡å‹ä½œä¸ºå¤‡é€‰çš„æœ€ä½³æ¨¡å‹\n",
    "            print(f\"å›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹: {best_model_name_final}\")\n",
    "            #åœ¨result_dfä¸­å°†è¿™ä¸ªå¤‡é€‰æ¨¡å‹çš„åå­—é‡å‘½åä¸º 'Best Linear Model (Fallback)'ã€‚\n",
    "            if best_model_name_final in result_df['Metrics'].values:\n",
    "                result_df.loc[result_df['Metrics'] == best_model_name_final, 'Metrics'] = 'Best Linear Model (Fallback)'\n",
    "            else: #å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½è®­ç»ƒå¤±è´¥äº†ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯ã€‚\n",
    "                fallback_idx = next((i for i, r in enumerate(results) if r['Metrics'] == best_model_name_final), None)\n",
    "                if fallback_idx is not None:\n",
    "                    result_df.loc[fallback_idx, 'Metrics'] = 'Best Linear Model (Fallback)'\n",
    "        else:\n",
    "            print(\"é”™è¯¯: æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥ã€‚æ— æ³•ç¡®å®šæœ€ä½³æ¨¡å‹ã€‚\")\n",
    "else:\n",
    "    print(\"è­¦å‘Š: æ‰€æœ‰æ¨¡å‹åœ¨ Cross-validation MAE ä¸Šå¤±è´¥ã€‚æ— æ³•ç¡®å®šæœ€ä½³æ¨¡å‹ã€‚\")\n",
    "\n",
    "result_df_formatted = result_df.copy() #æ ¼å¼åŒ–ç»“æœè¡¨æ ¼ï¼šåˆ›å»º result_df çš„ä¸€ä¸ªå‰¯æœ¬ï¼Œä»¥å…ä¿®æ”¹åŸå§‹æ•°æ®\n",
    "for col in result_df_formatted.columns:\n",
    "    if 'MAE' in col:\n",
    "        result_df_formatted[col] = result_df_formatted[col].apply(lambda x: f\"{x:.2f}\" if pd.notna(x) else 'NaN')\n",
    "    #å°†è¯¥åˆ—ä¸­çš„æ•°å€¼æ ¼å¼åŒ–ä¸ºä¿ç•™ä¸¤ä½å°æ•°çš„å­—ç¬¦ä¸²ã€‚å¦‚æœæ˜¯ç¼ºå¤±å€¼ (NaN)ï¼Œåˆ™æ˜¾ç¤º 'NaN'\n",
    "    elif 'RMAE' in col:\n",
    "        result_df_formatted[col] = result_df_formatted[col].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else 'NaN')\n",
    "    #å°†è¯¥åˆ—ä¸­çš„æ•°å€¼æ ¼å¼åŒ–ä¸ºä¿ç•™å››ä½å°æ•°çš„å­—ç¬¦ä¸²ã€‚å¦‚æœæ˜¯ç¼ºå¤±å€¼ (NaN)ï¼Œåˆ™æ˜¾ç¤º 'NaN'ã€‚\n",
    "print(\"\\n--- 10. Model Performance Summary (MAE and RMAE for Original Price Level) ---\")\n",
    "print(result_df_formatted.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# ğŸ”Ÿâ•1ï¸âƒ£ é€‰æ‹©æœ€ä¼˜æ¨¡å‹ (ä¸å˜)\n",
    "print(f\"\\n--- Selecting Final Model for Prediction: {best_model_name_final} ---\")\n",
    "if best_model_name_final in fitted_models and fitted_models[best_model_name_final] is not None:\n",
    "    best_model = fitted_models[best_model_name_final] #å¦‚æœä¸Šè¿°æ£€æŸ¥é€šè¿‡ï¼ˆå³æœ€ä½³æ¨¡å‹ç¡®å®æˆåŠŸè®­ç»ƒäº†ï¼‰ï¼Œå°±ä» fitted_models å­—å…¸ä¸­å–å‡ºé‚£ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡ï¼ˆæ¯”å¦‚è®­ç»ƒå¥½çš„ Ridge å®ä¾‹ï¼‰ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™å˜é‡ best_modelã€‚\n",
    "    best_model_name_predict = best_model_name_final #è®°å½•ä¸‹å®é™…ç”¨äºé¢„æµ‹çš„æ¨¡å‹åç§°\n",
    "else: #ç¨³å¥æ€§è®¾è®¡\n",
    "    available_models = [m for m in ['Ridge', 'OLS', 'LASSO', 'ElasticNet'] if m in fitted_models and fitted_models[m] is not None]\n",
    "    if available_models:\n",
    "        best_model_name_predict = available_models[0] #é€‰æ‹©åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªï¼ˆå³ä¼˜å…ˆçº§æœ€é«˜çš„ï¼‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹ä½œä¸ºå¤‡é€‰é¢„æµ‹æ¨¡å‹ã€‚\n",
    "        best_model = fitted_models[best_model_name_predict]\n",
    "        print(f\"è­¦å‘Š: CV MAE æœ€ä½³æ¨¡å‹ '{best_model_name_final}' å¤±è´¥æˆ–ä¸å¯ç”¨ï¼Œå›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹: {best_model_name_predict}ã€‚\")\n",
    "    else:\n",
    "        raise RuntimeError(\"æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•ç»§ç»­è¿›è¡Œé¢„æµ‹ã€‚\")\n",
    "\n",
    "\n",
    "# ğŸ”Ÿâ•2ï¸âƒ£ é¢„æµ‹æµ‹è¯•é›† (ä¸å˜)\n",
    "try:\n",
    "    print(f\"\\n--- ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹: {best_model_name_predict} ---\")\n",
    "    \n",
    "    input_data_is_sparse = issparse(X_test_transformed) #æ£€æŸ¥è¾“å…¥çš„æµ‹è¯•æ•°æ®æ˜¯å¦æ˜¯ç¨€ç–çŸ©é˜µæ ¼å¼\n",
    "    input_data_nan = (input_data_is_sparse and np.isnan(X_test_transformed.data).any()) or \\\n",
    "                     (not input_data_is_sparse and np.isnan(X_test_transformed).any())\n",
    "    input_data_inf = (input_data_is_sparse and np.isinf(X_test_transformed.data).any()) or \\\n",
    "                     (not input_data_is_sparse and np.isinf(X_test_transformed).any())\n",
    "    if input_data_nan or input_data_inf:\n",
    "        print(\"è­¦å‘Šï¼šæœ€ç»ˆé¢„æµ‹å‰åœ¨ X_test_transformed ä¸­æ£€æµ‹åˆ° NaN/Infï¼Œå°è¯•å¡«å…… 0ã€‚\")\n",
    "        if input_data_is_sparse:\n",
    "            X_test_transformed.data = np.nan_to_num(X_test_transformed.data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        else:\n",
    "            X_test_transformed = np.nan_to_num(X_test_transformed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    test_pred_log = best_model.predict(X_test_transformed) #ä½¿ç”¨é€‰å®šçš„ best_model å¯¹å¤„ç†åçš„æµ‹è¯•æ•°æ® X_test_transformed è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "    # (è¯Šæ–­é€»è¾‘ä¸å˜)\n",
    "    print(f\"é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼ (å‰ 5 ä¸ª): {test_pred_log[:5]}\")\n",
    "    finite_preds = test_pred_log[np.isfinite(test_pred_log)]\n",
    "    if finite_preds.size > 0:\n",
    "        print(f\"å¯¹æ•°ä»·æ ¼ç»Ÿè®¡ (æœ‰é™å€¼): Min={np.min(finite_preds):.2f}, Max={np.max(finite_preds):.2f}, Mean={np.mean(finite_preds):.2f}, Std={np.std(finite_preds):.2f}\")\n",
    "    if np.isnan(test_pred_log).any() or np.isinf(test_pred_log).any():\n",
    "        print(\"âŒ è­¦å‘Š: é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼åŒ…å« NaN æˆ– Infï¼æ­£åœ¨å°è¯•æ¸…ç†...\")\n",
    "        median_log_pred = np.nanmedian(finite_preds) if finite_preds.size > 0 else np.log1p(np.median(y_train_price_no_outliers))\n",
    "        test_pred_log = np.nan_to_num(test_pred_log, nan=median_log_pred, posinf=np.log1p(np.finfo(np.float64).max / 10), neginf=-700)\n",
    "    #é€†è½¬æ¢å›åŸå§‹ä»·æ ¼å°ºåº¦ä¸æœ€ç»ˆæ¸…ç†\n",
    "    test_pred_price = np.expm1(test_pred_log) #å¯¹æ•°å°ºåº¦çš„é¢„æµ‹å€¼ test_pred_log è½¬æ¢å›åŸå§‹çš„ä»·æ ¼å°ºåº¦\n",
    "    test_pred_price[test_pred_price < 0] = 0\n",
    "    large_finite_val = np.finfo(np.float64).max / 10 #æ•°å€¼ä¸Šé™\n",
    "    median_fallback = np.median(y_train_price_no_outliers) if len(y_train_price_no_outliers)>0 else 0 #è®¡ç®—è®­ç»ƒé›†åŸå§‹ä»·æ ¼çš„ä¸­ä½æ•°ï¼Œä½œä¸ºæœ€ç»ˆçš„å¤‡ç”¨å€¼ã€‚\n",
    "    test_pred_price = np.nan_to_num(test_pred_price, nan=median_fallback,\n",
    "                                      posinf=large_finite_val, neginf=0.0) #å¯¹ä»·æ ¼å°ºåº¦çš„é¢„æµ‹å€¼è¿›è¡Œæœ€åä¸€æ¬¡æ¸…ç†ï¼Œæ›¿æ¢å¯èƒ½å›  expm1 äº§ç”Ÿçš„ NaN æˆ– Infï¼Œå¹¶å°†è¶…å‡ºæå¤§èŒƒå›´çš„å€¼æ›¿æ¢æ‰ã€‚\n",
    "    test_pred_price = np.clip(test_pred_price, 0, large_finite_val) #å°†æ‰€æœ‰é¢„æµ‹å€¼é™åˆ¶åœ¨ 0 å’Œ large_finite_val ä¹‹é—´ï¼Œç¡®ä¿æ•°å€¼çš„åˆç†æ€§ã€‚\n",
    "    print(f\"æœ€ç»ˆé¢„æµ‹ä»·æ ¼ (å‰ 5 ä¸ª): {test_pred_price[:5]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"æœ€ç»ˆé¢„æµ‹å‡ºé”™: {e}\")\n",
    "    traceback.print_exc()\n",
    "    median_fallback = np.median(y_train_price_no_outliers) if len(y_train_price_no_outliers)>0 else 0\n",
    "    print(f\"é¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°é¢„æµ‹ä¸­ä½æ•°: {median_fallback}\")\n",
    "    test_pred_price = np.full(X_test_transformed.shape[0], median_fallback) #åˆ›å»ºä¸€ä¸ªæ•°ç»„ï¼Œå…¶é•¿åº¦ä¸æµ‹è¯•é›†æ ·æœ¬æ•°ç›¸åŒï¼Œå¹¶å°†æ‰€æœ‰é¢„æµ‹å€¼éƒ½è®¾ç½®ä¸ºè¿™ä¸ª median_fallback\n",
    "\n",
    "# ğŸ”Ÿâ•3ï¸âƒ£ ç”Ÿæˆæäº¤æ–‡ä»¶ (ä½¿ç”¨æ‚¨å·²ä¿®æ­£çš„è·¯å¾„)\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": test_data['ID'].values if 'ID' in test_data.columns and len(test_data['ID']) == X_test_transformed.shape[0] else np.arange(X_test_transformed.shape[0]),\n",
    "    \"prediction\": test_pred_price\n",
    "})\n",
    "submission_df.to_csv(\"prediction_price.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ğŸ¯ Prediction file saved as **prediction_price.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\")\n",
    "\n",
    "# ğŸ”Ÿâ•4ï¸âƒ£ ä¿å­˜æ¨¡å‹æ€§èƒ½è¡¨ (ä½¿ç”¨æ‚¨å·²ä¿®æ­£çš„è·¯å¾„)\n",
    "result_df.to_csv(\"performance_table_price.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ğŸ“Š Model performance saved as **performance_table_price.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229032cc-cf66-4443-8011-dfbd063d7ad1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## å…³äºç§Ÿä»·é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fca45fbb-a88d-42db-9857-68d59a9e7b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œç¼–ç : UTF-8ï¼Œåˆ†éš”ç¬¦: Commaã€‚\n",
      "âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œç¼–ç : UTF-8ï¼Œåˆ†éš”ç¬¦: Commaã€‚\n",
      "Train shape: (98899, 46)\n",
      "Test shape: (9773, 46)\n",
      "\n",
      "--- ç¦»ç¾¤å€¼å¤„ç†ç»“æœ ---\n",
      "åŸå§‹æ ·æœ¬æ•°: 98899\n",
      "ç§»é™¤ç¦»ç¾¤å€¼åæ ·æœ¬æ•°: 98205 (è¯·æŠ¥å‘Šæ­¤æ•°å­—)\n",
      "\n",
      "--- ä¿®æ­£åçš„ç‰¹å¾ ---\n",
      "æ•°å€¼ç‰¹å¾ (19): ['é¢ç§¯', 'lon', 'lat', 'æˆ¿å±‹æ€»æ•°', 'æ¥¼æ ‹æ€»æ•°', 'ç»¿ åŒ– ç‡', 'å®¹ ç§¯ ç‡', 'ç‰© ä¸š è´¹_å‡å€¼', 'ç‡ƒæ°”è´¹_å‡å€¼', 'ä¾›çƒ­è´¹_å‡å€¼', 'åœè½¦è´¹ç”¨_å‡å€¼', 'å»ºç­‘å¹´ä»£_å‡å€¼', 'åœè½¦ä½_å‡å€¼', 'æ€»æ¥¼å±‚æ•°', 'å®¤', 'å…', 'å«', 'é¢ç§¯_sq', 'å®¹ç§¯ç‡_sq']\n",
      "åˆ†ç±»ç‰¹å¾ (18): ['ç‡ƒæ°”', 'ç”¨æ°´', 'æœå‘', 'ç”¨ç”µ', 'ç¯çº¿ä½ç½®', 'ä¾›ç”µ', 'æ¥¼å±‚ä½ç½®', 'ç‰©ä¸šç±»åˆ«', 'åŸå¸‚', 'å»ºç­‘ç»“æ„', 'è½¦ä½', 'ä¾›æ°´', 'åŒºå¿', 'é‡‡æš–', 'å¹´ä»½', 'æ¿å—', 'ç”µæ¢¯', 'ä¾›æš–']\n",
      "\n",
      "--- 9. æ¨¡å‹è®­ç»ƒã€è¶…å‚æ•°è°ƒä¼˜ (GridSearchCV) ä¸è¯„ä¼° (æ‰‹åŠ¨ CV) ---\n",
      "\n",
      "--- æ­¥éª¤ A: ä½¿ç”¨ GridSearchCV å¯»æ‰¾æœ€ä½³å‚æ•° ---\n",
      "âœ… OLS ä¸éœ€è¦ GridSearchCVã€‚\n",
      "\n",
      "--- å¼€å§‹ GridSearchCV: LASSO ---\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "âœ… LASSO Best Params found: {'model__alpha': np.float64(0.0001)}\n",
      "\n",
      "--- å¼€å§‹ GridSearchCV: Ridge ---\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "âœ… Ridge Best Params found: {'model__alpha': np.float64(0.01)}\n",
      "\n",
      "--- å¼€å§‹ GridSearchCV: ElasticNet ---\n",
      "Fitting 6 folds for each of 4 candidates, totalling 24 fits\n",
      "âœ… ElasticNet Best Params found: {'model__alpha': np.float64(0.0001), 'model__l1_ratio': 0.1}\n",
      "\n",
      "--- æ­¥éª¤ B: åˆ†ç¦»é¢„å¤„ç† ---\n",
      "--- Transforming Data ---\n",
      "Data transformed. Shape: (98205, 301)\n",
      "\n",
      "--- æ­¥éª¤ C: ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹å¹¶æ‰‹åŠ¨ CV è¯„ä¼° ---\n",
      "--- Fitting OLS ---\n",
      "âœ… OLS Trained using best/default params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}\n",
      "   Intercept: 12.8462\n",
      "   Coefficient Stats: Mean=0.0010, Std=0.3911, Min=-4.0894, Max=4.4693\n",
      "--- å¼€å§‹æ‰‹åŠ¨ 6-Fold Cross-Validation: OLS ---\n",
      "--- OLS Manual CV finished. Average MAE: 104894.02051643231 ---\n",
      "--- Fitting LASSO ---\n",
      "âœ… LASSO Trained using best/default params: {'alpha': np.float64(0.0001), 'copy_X': True, 'fit_intercept': True, 'max_iter': 10000, 'positive': False, 'precompute': False, 'random_state': 111, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n",
      "   Intercept: 12.6577\n",
      "   Coefficient Stats: Mean=0.0134, Std=0.1994, Min=-1.2823, Max=1.6636\n",
      "--- å¼€å§‹æ‰‹åŠ¨ 6-Fold Cross-Validation: LASSO ---\n",
      "--- LASSO Manual CV finished. Average MAE: 106126.39284800114 ---\n",
      "--- Fitting Ridge ---\n",
      "âœ… Ridge Trained using best/default params: {'alpha': np.float64(0.01), 'copy_X': True, 'fit_intercept': True, 'max_iter': 10000, 'positive': False, 'random_state': 111, 'solver': 'sag', 'tol': 0.001}\n",
      "   Intercept: 12.9164\n",
      "   Coefficient Stats: Mean=0.0014, Std=0.3669, Min=-3.7660, Max=4.1450\n",
      "--- å¼€å§‹æ‰‹åŠ¨ 6-Fold Cross-Validation: Ridge ---\n",
      "--- Ridge Manual CV finished. Average MAE: 105069.15781744685 ---\n",
      "--- Fitting ElasticNet ---\n",
      "âœ… ElasticNet Trained using best/default params: {'alpha': np.float64(0.0001), 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.1, 'max_iter': 10000, 'positive': False, 'precompute': False, 'random_state': 111, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n",
      "   Intercept: 12.9115\n",
      "   Coefficient Stats: Mean=0.0028, Std=0.1915, Min=-1.1394, Max=1.5203\n",
      "--- å¼€å§‹æ‰‹åŠ¨ 6-Fold Cross-Validation: ElasticNet ---\n",
      "--- ElasticNet Manual CV finished. Average MAE: 105683.17946456534 ---\n",
      "\n",
      "--- Determining Best Linear Model (based on Cross-validation MAE) ---\n",
      "æœ€ä½³æ¨¡å‹ï¼ˆåŸºäº CV MAEï¼‰: OLS\n",
      "\n",
      "--- 10. Model Performance Summary (MAE and RMAE for Original Rent Level) ---\n",
      "| Metrics           |   In-sample MAE |   In-sample RMAE |   Out-of-sample MAE |   Out-of-sample RMAE |   Cross-validation MAE |   Cross-validation RMAE |\n",
      "|:------------------|----------------:|-----------------:|--------------------:|---------------------:|-----------------------:|------------------------:|\n",
      "| Best Linear Model |          104515 |             0.19 |              104587 |                 0.19 |                 104894 |                    0.19 |\n",
      "| LASSO             |          105819 |             0.19 |              106099 |                 0.19 |                 106126 |                    0.19 |\n",
      "| Ridge             |          104717 |             0.19 |              104800 |                 0.19 |                 105069 |                    0.19 |\n",
      "| ElasticNet        |          105344 |             0.19 |              105516 |                 0.19 |                 105683 |                    0.19 |\n",
      "\n",
      "--- Selecting Final Model based on CV MAE: OLS ---\n",
      "\n",
      "--- ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹: OLS ---\n",
      "é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼ (å‰ 5 ä¸ª): [12.02437078 13.03284144 12.98116211 14.31975746 13.7928045 ]\n",
      "å¯¹æ•°ä»·æ ¼ç»Ÿè®¡ (æœ‰é™å€¼): Min=10.81, Max=15.45, Mean=12.94, Std=0.70\n",
      "æœ€ç»ˆé¢„æµ‹ä»·æ ¼ (å‰ 5 ä¸ª): [ 166768.98017207  457183.10478763  434156.26383824 1655737.1221806\n",
      "  977548.78618685]\n",
      "ğŸ¯ Prediction file saved as **prediction_rent.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\n",
      "ğŸ“Š Model performance saved as **performance_table_rent.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\n"
     ]
    }
   ],
   "source": [
    "# 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "# Add scipy sparse matrix check\n",
    "from scipy.sparse import issparse # ä» scipy.sparse æ¨¡å—å¯¼å…¥ issparse å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥æ•°æ®æ˜¯å¦æ˜¯ç¨€ç–çŸ©é˜µæ ¼å¼\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer \n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline # Pipeline ç”¨äºå®šä¹‰è½¬æ¢å™¨å’Œ GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer # å¯¼å…¥ make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import clone # éœ€è¦ clone\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# ğŸ¯è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°\n",
    "# (calculate_mae_rmse_original å‡½æ•°ä¿æŒä¸å˜ï¼ŒåŒ…å«å¥å£®æ€§å¤„ç†) \n",
    "def calculate_mae_rmse_original(y_log_true, y_log_pred):\n",
    "    mae, rmse = np.nan, np.nan\n",
    "    try:\n",
    "        y_log_true_np = np.asarray(y_log_true).flatten()\n",
    "        y_log_pred_np = np.asarray(y_log_pred).flatten()\n",
    "\n",
    "        log_max = np.log1p(np.finfo(np.float64).max / 10)\n",
    "        y_log_true_np = np.nan_to_num(y_log_true_np, nan=0.0, posinf=log_max, neginf=-700)\n",
    "        y_log_pred_np = np.nan_to_num(y_log_pred_np, nan=0.0, posinf=log_max, neginf=-700) \n",
    "        y_log_true_np = np.clip(y_log_true_np, -700, log_max)\n",
    "        y_log_pred_np = np.clip(y_log_pred_np, -700, log_max)\n",
    "\n",
    "        y_price_pred = np.expm1(y_log_pred_np) #è¿›è¡ŒæŒ‡æ•°è¿ç®—çš„é€†æ“ä½œï¼šä½¿ç”¨ np.expm1(x) å‡½æ•°ï¼ˆè®¡ç®— exp(x) - 1ï¼‰å°†æ¸…ç†è¿‡çš„**é¢„æµ‹å¯¹æ•°å€¼**è½¬æ¢å›**åŸå§‹ä»·æ ¼å°ºåº¦**ã€‚è¿™æ˜¯ np.log1p() çš„ç²¾ç¡®é€†è¿ç®—ã€‚\n",
    "        y_price_true = np.expm1(y_log_true_np)\n",
    "        y_price_pred[y_price_pred < 0] = 0\n",
    "\n",
    "        large_finite_val = np.finfo(np.float64).max / 10\n",
    "        y_price_pred = np.nan_to_num(y_price_pred, nan=0.0, posinf=large_finite_val, neginf=0.0)\n",
    "        y_price_true = np.nan_to_num(y_price_true, nan=0.0, posinf=large_finite_val, neginf=0.0)\n",
    "        y_price_pred = np.clip(y_price_pred, 0, large_finite_val)\n",
    "        y_price_true = np.clip(y_price_true, 0, large_finite_val)\n",
    "\n",
    "        if np.isnan(y_price_pred).any() or np.isnan(y_price_true).any():\n",
    "             y_price_pred = np.nan_to_num(y_price_pred, nan=0.0)\n",
    "             y_price_true = np.nan_to_num(y_price_true, nan=0.0)\n",
    "\n",
    "        mae = mean_absolute_error(y_price_true, y_price_pred)\n",
    "        mse = mean_squared_error(y_price_true, y_price_pred)\n",
    "        if mse < 0 or not np.isfinite(mse): rmse = np.nan\n",
    "        else: rmse = np.sqrt(mse)\n",
    "        return mae, rmse\n",
    "    except (ValueError, OverflowError, TypeError) as e:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# è‡ªå®šä¹‰ MAE è¯„åˆ†å™¨ (ç”¨äº GridSearchCV)\n",
    "def mae_scorer(estimator, X, y_log_true):\n",
    "    \"\"\"è¯„åˆ†å™¨ï¼šè¿”å›è´Ÿ MAE (éœ€æœ€å¤§åŒ–)\"\"\"\n",
    "    y_log_pred = estimator.predict(X)\n",
    "    mae, _ = calculate_mae_rmse_original(y_log_true, y_log_pred)\n",
    "    if np.isnan(mae):\n",
    "        return -np.finfo(np.float64).max # è¿”å›æå·®åˆ†æ•°\n",
    "    return -mae\n",
    "\n",
    "# âš™ï¸ å¤æ‚ç‰¹å¾æ¸…ç†å‡½æ•° (æ ¸å¿ƒä¿®æ­£ï¼šé€‚é… RENT å¹¶ä¿ç•™å…³é”®ç‰¹å¾)\n",
    "def clean_complex_features(df):\n",
    "    \"\"\"æ‰§è¡Œå•ä½è½¬æ¢ã€æ­£åˆ™æå–å’Œåˆå§‹ç‰¹å¾æ¸…ç†ã€‚å·²é’ˆå¯¹ RENT æ•°æ®é›†åˆ—åé€‚é…ã€‚\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. å•ä½è½¬æ¢\n",
    "    def clean_unit(series, unit):\n",
    "        cleaned = series.astype(str).str.replace(unit, '', regex=False).str.strip()\n",
    "        cleaned = cleaned.replace(['', 'æš‚æ— ', 'null', 'None'], np.nan)\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "\n",
    "    # *** é€‚é… RENT: 'é¢ç§¯', 'æˆ¿å±‹æ€»æ•°', 'æ¥¼æ ‹æ€»æ•°' ***\n",
    "    for col, unit in [('é¢ç§¯', 'ã¡'), ('æˆ¿å±‹æ€»æ•°', 'æˆ·'), ('æ¥¼æ ‹æ€»æ•°', 'æ ‹')]:\n",
    "        if col in df.columns:\n",
    "            df[col] = clean_unit(df[col], unit)\n",
    "    if 'ç»¿ åŒ– ç‡' in df.columns:\n",
    "        df['ç»¿ åŒ– ç‡'] = clean_unit(df['ç»¿ åŒ– ç‡'], '%')\n",
    "        if df['ç»¿ åŒ– ç‡'] is not None:\n",
    "            df['ç»¿ åŒ– ç‡'] = df['ç»¿ åŒ– ç‡'] / 100\n",
    "\n",
    "    # 2. æå–æ•°å­—èŒƒå›´å‡å€¼\n",
    "    def extract_avg_value(text):\n",
    "        if pd.isna(text) or str(text).strip() in ['æš‚æ— ', 'null', 'None','']: return np.nan\n",
    "        match = re.findall(r\"(\\d+(?:\\.\\d+)?)\", str(text)) ## ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ–‡æœ¬ä¸­çš„æ‰€æœ‰æ•°å­—ï¼ˆåŒ…æ‹¬æ•´æ•°å’Œå°æ•°ï¼‰\n",
    "        nums = [float(n) for n in match]\n",
    "        return np.mean(nums) if nums else np.nan\n",
    "\n",
    "    # é€‚é… RENT: ç¡®ä¿ 'åœè½¦ä½' å’Œ 'åœè½¦è´¹ç”¨' éƒ½è¢«æ¸…ç†\n",
    "    for col in ['ç‰© ä¸š è´¹', 'ç‡ƒæ°”è´¹', 'ä¾›çƒ­è´¹', 'åœè½¦è´¹ç”¨', 'å»ºç­‘å¹´ä»£', 'åœè½¦ä½']:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_å‡å€¼'] = df[col].apply(extract_avg_value)\n",
    "            df = df.drop(columns=[col], errors='ignore')\n",
    "    \n",
    "    for col in df.filter(like='_å‡å€¼').columns:\n",
    "         if df[col].isnull().any():\n",
    "             median_val = df[col].dropna().median()\n",
    "             if pd.isna(median_val): median_val = 0\n",
    "             df[col] = df[col].fillna(median_val)\n",
    "\n",
    "    # 3. æå–æ¥¼å±‚ (é€‚é… RENT åˆ—å 'æ¥¼å±‚' å’Œæ ¼å¼ \"ä½æ¥¼å±‚/18å±‚\")\n",
    "    if 'æ¥¼å±‚' in df.columns:\n",
    "        df['æ¥¼å±‚ä½ç½®'] = df['æ¥¼å±‚'].astype(str).str.extract(r'([A-Za-z\\u4e00-\\u9fa5]+)').fillna('æœªçŸ¥')\n",
    "        df['æ€»æ¥¼å±‚æ•°'] = pd.to_numeric(df['æ¥¼å±‚'].astype(str).str.extract(r'/(\\d+)å±‚')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df = df.drop(columns=['æ¥¼å±‚'])\n",
    "    \n",
    "    # 4. æå–æˆ·å‹ (é€‚é… RENT åˆ—å 'æˆ·å‹')\n",
    "    if 'æˆ·å‹' in df.columns:\n",
    "        df['å®¤'] = pd.to_numeric(df['æˆ·å‹'].astype(str).str.extract(r'(\\d+)å®¤')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df['å…'] = pd.to_numeric(df['æˆ·å‹'].astype(str).str.extract(r'(\\d+)å…')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df['å«'] = pd.to_numeric(df['æˆ·å‹'].astype(str).str.extract(r'(\\d+)å«')[0], errors='coerce').fillna(0).astype(int)\n",
    "        df = df.drop(columns=['æˆ·å‹'])\n",
    "\n",
    "    # 5. äº¤äº’é¡¹/å¤šé¡¹å¼ç‰¹å¾ (é€‚é… RENTï¼šä»…ä½¿ç”¨ 'é¢ç§¯' å’Œ 'å®¹ ç§¯ ç‡')\n",
    "    if 'é¢ç§¯' in df.columns:\n",
    "        df['é¢ç§¯'] = pd.to_numeric(df['é¢ç§¯'], errors='coerce') # ç¡®ä¿ 'é¢ç§¯' æ˜¯æ•°å€¼\n",
    "        df['é¢ç§¯_sq'] = df['é¢ç§¯'] ** 2\n",
    "    if 'å®¹ ç§¯ ç‡' in df.columns:\n",
    "        df['å®¹ ç§¯ ç‡'] = pd.to_numeric(df['å®¹ ç§¯ ç‡'], errors='coerce').fillna(df['å®¹ ç§¯ ç‡'].median())\n",
    "        df['å®¹ç§¯ç‡_sq'] = df['å®¹ ç§¯ ç‡'] ** 2\n",
    "    \n",
    "    # ä¿®æ­£ï¼šå¯¹æ‰€æœ‰æ•°å€¼åˆ—å¡«å……ä¸­ä½æ•°\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # 6. åˆ é™¤åˆ— (RENT åˆ—è¡¨)\n",
    "    # æ ¸å¿ƒä¿®æ­£ï¼šä¿ç•™ 'åŸå¸‚', 'æ¿å—', 'åŒºå¿', 'lon', 'lat' \n",
    "    drop_cols = [\n",
    "        # RENT ç‹¬æœ‰éœ€åˆ é™¤çš„ç‰¹å¾\n",
    "        'è£…ä¿®', 'äº¤æ˜“æ—¶é—´', 'ä»˜æ¬¾æ–¹å¼', 'ç§Ÿèµæ–¹å¼', 'ç§ŸæœŸ', 'é…å¥—è®¾æ–½', \n",
    "        # 'åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'åŒºå¿', # <-- ä¿ç•™\n",
    "        # 'lon', 'lat', # <-- ä¿ç•™\n",
    "        # 'æˆ¿å±‹æ€»æ•°', 'åœè½¦ä½_å‡å€¼', # <-- ä¿ç•™\n",
    "        'æ ¸å¿ƒå–ç‚¹', 'æˆ·å‹ä»‹ç»', 'å‘¨è¾¹é…å¥—', 'äº¤é€šå‡ºè¡Œ', 'å®¢æˆ·åé¦ˆ', 'äº§æƒæè¿°', 'æˆ¿å±‹ç”¨é€”', \n",
    "        'coord_x', 'coord_y', 'ç‰©ä¸šåŠå…¬ç”µè¯', 'å¼€å‘å•†', 'ç‰©ä¸šå…¬å¸', 'æˆ¿å±‹å¹´é™',\n",
    "        'åœè½¦è´¹ç”¨'\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # å¡«å……ç¼ºå¤±åˆ†ç±»ç‰¹å¾\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "         df[col] = df[col].fillna('ç¼ºå¤±')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 2ï¸âƒ£ æ•°æ®è¯»å– (RENT æ–‡ä»¶)\n",
    "TRAIN_SNIPPET = \"ruc_Class25Q2_train_rent.csv\"\n",
    "TEST_SNIPPET = \"ruc_Class25Q2_test_rent.csv\"\n",
    "TARGET_NAME = \"Price\" \n",
    "\n",
    "TRAIN_ENCODING = 'UTF-8'\n",
    "TEST_ENCODING = 'UTF-8'\n",
    "\n",
    "train_rent = None\n",
    "test_rent = None\n",
    "\n",
    "# æ ¸å¿ƒä¿®æ­£ï¼šRENT è®­ç»ƒé›†ä¼˜å…ˆå°è¯• Comma (',') åˆ†éš”ç¬¦\n",
    "# 1. åŠ è½½è®­ç»ƒé›†\n",
    "try:\n",
    "    train_rent = pd.read_csv(TRAIN_SNIPPET, low_memory=False, encoding=TRAIN_ENCODING, sep=',')\n",
    "    print(f\"âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TRAIN_ENCODING}ï¼Œåˆ†éš”ç¬¦: Commaã€‚\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        train_rent = pd.read_csv(TRAIN_SNIPPET, low_memory=False, encoding=TRAIN_ENCODING, sep='\\t')\n",
    "        print(f\"âœ… è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TRAIN_ENCODING}ï¼Œåˆ†éš”ç¬¦: Tabã€‚\")\n",
    "    except Exception as e_inner:\n",
    "        print(f\"âŒ è®­ç»ƒé›†åŠ è½½å¤±è´¥ã€‚é”™è¯¯: {e_inner}\")\n",
    "        raise RuntimeError(\"æ— æ³•åŠ è½½ RENT æ¨¡å‹çš„è®­ç»ƒé›†ã€‚\")\n",
    "\n",
    "# 2. åŠ è½½æµ‹è¯•é›† (ä¿®æ­£ï¼šRENT æµ‹è¯•é›†ä¹Ÿä¼˜å…ˆå°è¯• Comma (','))\n",
    "try:\n",
    "    test_rent = pd.read_csv(TEST_SNIPPET, low_memory=False, encoding=TEST_ENCODING, sep=',')\n",
    "    print(f\"âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TEST_ENCODING}ï¼Œåˆ†éš”ç¬¦: Commaã€‚\")\n",
    "except Exception:\n",
    "    try:\n",
    "        test_rent = pd.read_csv(TEST_SNIPPET, low_memory=False, encoding=TEST_ENCODING, sep='\\t')\n",
    "        print(f\"âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œç¼–ç : {TEST_ENCODING}ï¼Œåˆ†éš”ç¬¦: Tabã€‚\")\n",
    "    except Exception as e_inner:\n",
    "         print(f\"âŒ æµ‹è¯•é›†åŠ è½½å¤±è´¥ã€‚é”™è¯¯: {e_inner}\")\n",
    "         raise RuntimeError(\"æ— æ³•åŠ è½½ RENT æ¨¡å‹çš„æµ‹è¯•é›†ã€‚\")\n",
    "\n",
    "train_data = train_rent\n",
    "test_data = test_rent\n",
    "\n",
    "\n",
    "# 3ï¸âƒ£ åˆæ­¥æ£€æŸ¥ä¸æ¸…æ´—\n",
    "# (æ¸…æ´—é€»è¾‘ä¿æŒä¸å˜)\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "\n",
    "target = TARGET_NAME\n",
    "if target not in train_data.columns:\n",
    "    raise ValueError(f\"ç›®æ ‡å˜é‡ '{target}' ä¸åœ¨è®­ç»ƒé›†åˆ—ä¸­ã€‚\")\n",
    "\n",
    "KEEP_COLS = [target, \"ç‰© ä¸š è´¹\", \"åœè½¦è´¹ç”¨\"] # åŸå§‹æ–‡æœ¬åˆ—\n",
    "leak_cols_potential = [\n",
    "    c for c in train_data.columns\n",
    "    if \"comm\" in c.lower() or \"price\" in c.lower() or \"rent\" in c.lower() \n",
    "]\n",
    "leak_cols = [c for c in leak_cols_potential if c not in KEEP_COLS] # ä»æ½œåœ¨æ³„éœ²åˆ—ä¸­ï¼Œæ’é™¤æ‰KEEP_COLSä¸­æŒ‡å®šçš„åˆ—ï¼Œå¾—åˆ°æœ€ç»ˆè¦åˆ é™¤çš„æ³„éœ²åˆ—åˆ—è¡¨ã€‚\n",
    "\n",
    "train_data.drop(columns=[col for col in leak_cols if col in train_data.columns],\n",
    "                inplace=True, errors='ignore')\n",
    "#ä»è®­ç»ƒæ•°æ®ä¸­åˆ é™¤leak_colsåˆ—è¡¨é‡Œå­˜åœ¨çš„åˆ—ã€‚inplace=Trueè¡¨ç¤ºç›´æ¥åœ¨åŸ DataFrameä¸Šä¿®æ”¹ã€‚\n",
    "test_data.drop(columns=[col for col in leak_cols if col in test_data.columns],\n",
    "               inplace=True, errors='ignore')\n",
    "\n",
    "all_data = pd.concat([train_data.drop(columns=[target]), test_data], keys=['train', 'test'])\n",
    "all_data_cleaned = clean_complex_features(all_data)\n",
    "\n",
    "for col in all_data_cleaned.select_dtypes(include='object').columns:\n",
    "    all_data_cleaned[col] = all_data_cleaned[col].astype('category')\n",
    "    \n",
    "X_train_raw = all_data_cleaned.loc['train'].copy()\n",
    "X_test_final = all_data_cleaned.loc['test'].copy()\n",
    "\n",
    "y_train_log = np.log1p(train_data[target])\n",
    "y_train_price = train_data[target]\n",
    "\n",
    "# 4ï¸âƒ£ ç¦»ç¾¤å€¼å¤„ç†\n",
    "Q1 = y_train_log.quantile(0.25)\n",
    "Q3 = y_train_log.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outlier_mask = (y_train_log >= lower_bound) & (y_train_log <= upper_bound)\n",
    "X_train_no_outliers = X_train_raw.loc[outlier_mask].copy()\n",
    "y_train_log_no_outliers = y_train_log.loc[outlier_mask].copy()\n",
    "y_train_price_no_outliers = y_train_price.loc[outlier_mask].copy()\n",
    "\n",
    "initial_count = len(train_data)\n",
    "final_count = len(X_train_no_outliers)\n",
    "print(f\"\\n--- ç¦»ç¾¤å€¼å¤„ç†ç»“æœ ---\")\n",
    "print(f\"åŸå§‹æ ·æœ¬æ•°: {initial_count}\")\n",
    "print(f\"ç§»é™¤ç¦»ç¾¤å€¼åæ ·æœ¬æ•°: {final_count} (è¯·æŠ¥å‘Šæ­¤æ•°å­—)\")\n",
    "\n",
    "# 5ï¸âƒ£ è®­ç»ƒé›†ä¸éªŒè¯é›†åˆ’åˆ†(ä¿æŒpandasç±»å‹)\n",
    "X_train, X_valid, y_train_log_split, y_valid_log_split = train_test_split(\n",
    "    X_train_no_outliers, y_train_log_no_outliers, test_size=0.2, random_state=111\n",
    ")\n",
    "_, _, y_train_price_split, y_valid_price_split = train_test_split(\n",
    "    X_train_no_outliers, y_train_price_no_outliers, test_size=0.2, random_state=111\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ åŒºåˆ†å˜é‡ç±»å‹ (åœ¨å®Œæ•´æ— ç¦»ç¾¤å€¼æ•°æ®ä¸Šå®šä¹‰)\n",
    "# æ ¸å¿ƒä¿®æ­£ï¼šæ‰‹åŠ¨å®šä¹‰å“ªäº›â€œæ•°å­—â€åˆ—åº”è¯¥æ˜¯åˆ†ç±»çš„\n",
    "auto_numeric_features = X_train_no_outliers.select_dtypes(include=[np.number]).columns.tolist()\n",
    "auto_categorical_features = X_train_no_outliers.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "known_categorical_numerics = ['åŸå¸‚', 'åŒºåŸŸ', 'æ¿å—', 'åŒºå¿', 'å¹´ä»½'] # 'åŒºåŸŸ' åœ¨ RENT æ•°æ®ä¸­å¯èƒ½ä¸å­˜åœ¨\n",
    "\n",
    "categorical_features = list(set(auto_categorical_features + [col for col in known_categorical_numerics if col in X_train_no_outliers.columns]))\n",
    "numeric_features = [f for f in auto_numeric_features if f not in known_categorical_numerics]\n",
    "# ç¡®ä¿ ID åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ä¸è¢«ç”¨ä½œç‰¹å¾\n",
    "if 'ID' in numeric_features:\n",
    "    numeric_features.remove('ID')\n",
    "if 'ID' in categorical_features:\n",
    "    categorical_features.remove('ID')\n",
    "\n",
    "print(f\"\\n--- ä¿®æ­£åçš„ç‰¹å¾ ---\")\n",
    "print(f\"æ•°å€¼ç‰¹å¾ ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"åˆ†ç±»ç‰¹å¾ ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# 7ï¸âƒ£ ç‰¹å¾é¢„å¤„ç† Pipeline å®šä¹‰\n",
    "# æ ¸å¿ƒä¿®æ­£ï¼šå¯¹æ•°å€¼ç‰¹å¾å…ˆ Log å˜æ¢(å¤„ç†éè´Ÿ)ï¼Œå†Imputeå’Œ Scale\n",
    "log_transformer = FunctionTransformer(lambda x: np.log1p(np.maximum(x, 0)), validate=False)\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer1', SimpleImputer(strategy='median')),\n",
    "    ('logtransform', log_transformer),\n",
    "    ('imputer2', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ä¿æŒæ”¾æ¾çš„ OHE é™åˆ¶ (max_categories=50)\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='ç¼ºå¤±')),\n",
    "    ('astype_str', FunctionTransformer(lambda x: x.astype(str), validate=False)), # ç¡®ä¿ OHE æ¥æ”¶å­—ç¬¦ä¸²\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', max_categories=50)) # ç‹¬çƒ­ç¼–ç \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# 8ï¸âƒ£ æ¨¡å‹å®šä¹‰ä¸è¶…å‚æ•° (ç”¨äº GridSearchCV - æé€Ÿç‰ˆ)\n",
    "# å®šä¹‰åŸºç¡€æ¨¡å‹\n",
    "models_base = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'LASSO': Lasso(max_iter=10000, random_state=111),\n",
    "    'Ridge': Ridge(max_iter=10000, random_state=111, solver='sag', tol=1e-3), # ä½¿ç”¨ 'sag' æ±‚è§£å™¨\n",
    "    'ElasticNet': ElasticNet(max_iter=10000, random_state=111)\n",
    "}\n",
    "# å®šä¹‰æœ€å°åŒ–çš„å‚æ•°ç½‘æ ¼\n",
    "param_grids = {\n",
    "    'LASSO': {'model__alpha': np.logspace(-4, 0, 3)},\n",
    "    'Ridge': {'model__alpha': np.logspace(-2, 2, 3)},\n",
    "    'ElasticNet': {'model__alpha': np.logspace(-4, 0, 2), 'model__l1_ratio': [0.1, 0.9]},\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_params_found = {} # å­˜å‚¨ GridSearchCV æ‰¾åˆ°çš„æœ€ä½³å‚æ•°\n",
    "fitted_models = {} # å­˜å‚¨æœ€ç»ˆæ‹Ÿåˆçš„æ¨¡å‹ (ä¸æ˜¯ Pipeline)\n",
    "\n",
    "# 9ï¸âƒ£ æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° (GridSearchCV è°ƒä¼˜ + åˆ†ç¦»é¢„å¤„ç† + æ‰‹åŠ¨ CV è¯„ä¼°)\n",
    "print(\"\\n--- 9. æ¨¡å‹è®­ç»ƒã€è¶…å‚æ•°è°ƒä¼˜ (GridSearchCV) ä¸è¯„ä¼° (æ‰‹åŠ¨ CV) ---\")\n",
    "full_data_X = X_train_no_outliers\n",
    "full_data_y_log = y_train_log_no_outliers\n",
    "mean_price = np.mean(y_train_price_no_outliers)\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=111)\n",
    "\n",
    "# åˆ›å»ºè‡ªå®šä¹‰è¯„åˆ†å™¨å®ä¾‹\n",
    "custom_mae_scorer = make_scorer(mae_scorer, greater_is_better=False)\n",
    "\n",
    "# æ­¥éª¤ A: ä½¿ç”¨ GridSearchCV å¯»æ‰¾æœ€ä½³å‚æ•°\n",
    "print(\"\\n--- æ­¥éª¤ A: ä½¿ç”¨ GridSearchCV å¯»æ‰¾æœ€ä½³å‚æ•° ---\")\n",
    "for name, model_base in models_base.items():\n",
    "    pipe_for_gridsearch = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_base)])\n",
    "    best_params_found[name] = {}\n",
    "\n",
    "    if name in param_grids:\n",
    "        print(f\"\\n--- å¼€å§‹ GridSearchCV: {name} ---\")\n",
    "        gscv = GridSearchCV(\n",
    "            pipe_for_gridsearch, param_grids[name], scoring=custom_mae_scorer,\n",
    "            cv=kf, n_jobs=-1, verbose=1\n",
    "        )\n",
    "        try:\n",
    "            gscv.fit(full_data_X, full_data_y_log)\n",
    "            best_params_found[name] = gscv.best_params_\n",
    "            print(f\"âœ… {name} Best Params found: {best_params_found[name]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {name} GridSearchCV failed: {e}. å°†ä½¿ç”¨é»˜è®¤å‚æ•°ã€‚\")\n",
    "            best_params_found[name] = {}\n",
    "    else:\n",
    "        print(f\"âœ… {name} ä¸éœ€è¦ GridSearchCVã€‚\")\n",
    "\n",
    "# æ­¥éª¤ B: åˆ†ç¦»é¢„å¤„ç†\n",
    "print(\"\\n--- æ­¥éª¤ B: åˆ†ç¦»é¢„å¤„ç† ---\")\n",
    "# 1. æ‹Ÿåˆé¢„å¤„ç†å™¨\n",
    "preprocessor_final = clone(preprocessor)\n",
    "preprocessor_final.fit(X_train_no_outliers)\n",
    "# 2. è½¬æ¢æ•°æ®\n",
    "print(\"--- Transforming Data ---\")\n",
    "try:\n",
    "    X_full_transformed = preprocessor_final.transform(X_train_no_outliers)\n",
    "    X_train_transformed = preprocessor_final.transform(X_train)\n",
    "    X_valid_transformed = preprocessor_final.transform(X_valid)\n",
    "    X_test_transformed = preprocessor_final.transform(X_test_final)\n",
    "    full_data_y_log_np = y_train_log_no_outliers.values # è½¬æ¢ä¸º NumPy array\n",
    "    print(f\"Data transformed. Shape: {X_full_transformed.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ é”™è¯¯: æ•°æ®è½¬æ¢å¤±è´¥: {e}\")\n",
    "    raise\n",
    "\n",
    "# æ­¥éª¤ C: ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œæ‰‹åŠ¨ CV è¯„ä¼°\n",
    "print(\"\\n--- æ­¥éª¤ C: ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹å¹¶æ‰‹åŠ¨ CV è¯„ä¼° ---\")\n",
    "for name, model_base in models_base.items():\n",
    "    params_for_model = {k.split('__')[1]: v for k, v in best_params_found[name].items()}\n",
    "    \n",
    "    final_model = clone(model_base)\n",
    "    # ä¿ Ridge ä½¿ç”¨ 'sag' æ±‚è§£å™¨\n",
    "    if name == 'Ridge' and 'solver' not in params_for_model:\n",
    "        # å¦‚æœ GridSearchCV æ²¡æœç´¢ solverï¼Œæ‰‹åŠ¨æ·»åŠ  'sag'\n",
    "        final_model.set_params(solver='sag', tol=1e-3, **params_for_model)\n",
    "    else:\n",
    "        final_model.set_params(**params_for_model)\n",
    "\n",
    "    # 1. åœ¨å®Œæ•´çš„è½¬æ¢åæ•°æ®ä¸Šæ‹Ÿåˆæœ€ç»ˆæ¨¡å‹\n",
    "    try:\n",
    "        # (è®­ç»ƒå‰æ£€æŸ¥æ•°æ®) \n",
    "        is_sparse_full = issparse(X_full_transformed) # æ£€æŸ¥æ•°æ®æ˜¯å¦ç¨€ç–\n",
    "        if (is_sparse_full and (np.isnan(X_full_transformed.data).any() or np.isinf(X_full_transformed.data).any())) or \\\n",
    "           (not is_sparse_full and (np.isnan(X_full_transformed).any() or np.isinf(X_full_transformed).any())):\n",
    "             raise ValueError(f\"NaN or Inf detected in X_full_transformed before fitting {name}\")\n",
    "        if np.isnan(full_data_y_log_np).any() or np.isinf(full_data_y_log_np).any():\n",
    "             raise ValueError(f\"NaN or Inf detected in y_log before fitting {name}\")\n",
    "\n",
    "        print(f\"--- Fitting {name} ---\")\n",
    "        current_model = final_model.fit(X_full_transformed, full_data_y_log_np)\n",
    "        print(f\"âœ… {name} Trained using best/default params: {current_model.get_params()}\")\n",
    "        fitted_models[name] = current_model\n",
    "\n",
    "        # ä¿æŒç³»æ•°è¯Šæ–­\n",
    "        print(f\"   Intercept: {current_model.intercept_:.4f}\")\n",
    "        if hasattr(current_model, 'coef_'): # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ coef_ å±æ€§\n",
    "             coefs = current_model.coef_\n",
    "             if issparse(coefs): coefs = coefs.toarray().flatten() # å¦‚æœç³»æ•°æ˜¯ç¨€ç–çš„ï¼Œè½¬æ¢ä¸ºå¯†é›†æ•°ç»„\n",
    "             print(f\"   Coefficient Stats: Mean={np.mean(coefs):.4f}, Std={np.std(coefs):.4f}, Min={np.min(coefs):.4f}, Max={np.max(coefs):.4f}\")\n",
    "             if np.allclose(coefs, 0, atol=1e-5): # æ£€æŸ¥ç³»æ•°æ˜¯å¦å‡ ä¹éƒ½ä¸ºé›¶ï¼ˆLasso å¯èƒ½å‡ºç°ï¼‰\n",
    "                 print(f\"   âš ï¸ è­¦å‘Š: æ¨¡å‹ {name} çš„ç³»æ•°å‡ ä¹å…¨ä¸ºé›¶ï¼\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name} Failed to train on full data: {e}\")\n",
    "        fitted_models[name] = None\n",
    "        results.append({\n",
    "            \"Metrics\": name, \"In-sample MAE\": np.nan, \"In-sample RMAE\": np.nan,\n",
    "            \"Out-of-sample MAE\": np.nan, \"Out-of-sample RMAE\": np.nan,\n",
    "            \"Cross-validation MAE\": np.nan, \"Cross-validation RMAE\": np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    current_model = fitted_models[name]\n",
    "    if current_model is None: continue\n",
    "\n",
    "    # æ€§èƒ½è®¡ç®— (åŸå§‹ä»·æ ¼æ°´å¹³)\n",
    "    # 1. In-sample\n",
    "    try:\n",
    "        y_pred_train_log = current_model.predict(X_train_transformed)\n",
    "        mae_train, _ = calculate_mae_rmse_original(y_train_log_split, y_pred_train_log)\n",
    "    except Exception as e: mae_train = np.nan\n",
    "\n",
    "    # 2. Out-of-sample\n",
    "    try:\n",
    "        y_pred_valid_log = current_model.predict(X_valid_transformed)\n",
    "        mae_valid, _ = calculate_mae_rmse_original(y_valid_log_split, y_pred_valid_log)\n",
    "    except Exception as e: mae_valid = np.nan\n",
    "\n",
    "    # æ‰‹åŠ¨å®ç° 6-Fold Cross-Validation (åœ¨è½¬æ¢åæ•°æ®ä¸Š) \n",
    "    print(f\"--- å¼€å§‹æ‰‹åŠ¨ 6-Fold Cross-Validation: {name} ---\")\n",
    "    fold_mae_scores = [] # ç”¨äºå­˜å‚¨æ¯ä¸€æŠ˜çš„ MAE\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_transformed, full_data_y_log_np)): # éå† 6 ä¸ªæŠ˜çš„ç´¢å¼•\n",
    "        X_train_fold, X_val_fold = X_full_transformed[train_idx], X_full_transformed[val_idx]      # è·å–å½“å‰æŠ˜çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®\n",
    "        y_train_fold, y_val_fold = full_data_y_log_np[train_idx], full_data_y_log_np[val_idx]      # è·å–å¯¹åº”çš„ç›®æ ‡å€¼\n",
    "\n",
    "        fold_model = clone(model_base).set_params(**params_for_model) # åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹å¹¶è®¾ç½®å‚æ•°\n",
    "        if name == 'Ridge' and 'solver' not in params_for_model:\n",
    "             fold_model.set_params(solver='sag', tol=1e-3)\n",
    "\n",
    "        try:\n",
    "            # (æ•°æ®æ£€æŸ¥æ˜¯å¦æœ‰ NaN/Inf)\n",
    "            is_sparse_train_fold = issparse(X_train_fold)\n",
    "            if (is_sparse_train_fold and (np.isnan(X_train_fold.data).any() or np.isinf(X_train_fold.data).any())) or \\\n",
    "               (not is_sparse_train_fold and (np.isnan(X_train_fold).any() or np.isinf(X_train_fold).any())) or \\\n",
    "               np.isnan(y_train_fold).any() or np.isinf(y_train_fold).any():\n",
    "                 raise ValueError(\"NaN or Inf in fold training data\")\n",
    "\n",
    "            fold_model.fit(X_train_fold, y_train_fold) # åœ¨å½“å‰æŠ˜çš„è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆæ¨¡å‹\n",
    "\n",
    "            is_sparse_val_fold = issparse(X_val_fold)\n",
    "            if (is_sparse_val_fold and (np.isnan(X_val_fold.data).any() or np.isinf(X_val_fold.data).any())) or \\\n",
    "               (not is_sparse_val_fold and (np.isnan(X_val_fold).any() or np.isinf(X_val_fold).any())):\n",
    "                raise ValueError(\"NaN or Inf in fold validation data for prediction\")\n",
    "\n",
    "            y_pred_fold_log = fold_model.predict(X_val_fold) # åœ¨å½“å‰æŠ˜çš„éªŒè¯æ•°æ®ä¸Šé¢„æµ‹\n",
    "            mae_fold, _ = calculate_mae_rmse_original(y_val_fold, y_pred_fold_log) # è®¡ç®—å½“å‰æŠ˜çš„ MAE\n",
    "            fold_mae_scores.append(mae_fold)\n",
    "        except Exception as e:\n",
    "            # print(f\"  Fold {fold+1} failed: {e}\") # å¯é€‰\n",
    "            fold_mae_scores.append(np.nan)\n",
    "\n",
    "    cv_mae = np.nanmean(fold_mae_scores) if np.isfinite(fold_mae_scores).any() else np.nan\n",
    "    print(f\"--- {name} Manual CV finished. Average MAE: {cv_mae if not np.isnan(cv_mae) else 'NaN'} ---\")\n",
    "\n",
    "    results.append({\n",
    "        \"Metrics\": name,\n",
    "        \"In-sample MAE\": mae_train,\n",
    "        \"In-sample RMAE\": mae_train / mean_price if mean_price != 0 else np.nan,\n",
    "        \"Out-of-sample MAE\": mae_valid,\n",
    "        \"Out-of-sample RMAE\": mae_valid / mean_price if mean_price != 0 else np.nan,\n",
    "        \"Cross-validation MAE\": cv_mae,\n",
    "        \"Cross-validation RMAE\": cv_mae / mean_price if mean_price != 0 and not np.isnan(cv_mae) else np.nan\n",
    "    })\n",
    "\n",
    "# ğŸ”Ÿ è¾“å‡ºç»“æœè¡¨\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# æ ¸å¿ƒä¿®æ­£ï¼šæ ¹æ® Cross-validation MAE è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "print(\"\\n--- Determining Best Linear Model (based on Cross-validation MAE) ---\")\n",
    "valid_cv_mae_indices = result_df[\"Cross-validation MAE\"].dropna().index\n",
    "best_model_name_final = 'OLS' # Default fallback\n",
    "if not valid_cv_mae_indices.empty:\n",
    "    best_model_idx = result_df.loc[valid_cv_mae_indices, \"Cross-validation MAE\"].idxmin() # *** ä½¿ç”¨ CV MAE æ’åº ***\n",
    "    best_model_name_orig = result_df.loc[best_model_idx, \"Metrics\"]\n",
    "    \n",
    "    if best_model_name_orig in fitted_models and fitted_models[best_model_name_orig] is not None:\n",
    "        result_df.loc[result_df['Metrics'] == best_model_name_orig, 'Metrics'] = 'Best Linear Model'\n",
    "        best_model_name_final = best_model_name_orig # ä½¿ç”¨å®é™…çš„æœ€ä½³æ¨¡å‹åç§°\n",
    "        print(f\"æœ€ä½³æ¨¡å‹ï¼ˆåŸºäº CV MAEï¼‰: {best_model_name_final}\")\n",
    "    else:\n",
    "        print(f\"è­¦å‘Š: ç¡®å®šçš„æœ€ä½³æ¨¡å‹ '{best_model_name_orig}' (CV MAE) è®­ç»ƒå¤±è´¥æˆ–ä¸å­˜åœ¨ã€‚æ­£åœ¨å›é€€ã€‚\")\n",
    "        # å›é€€é€»è¾‘ï¼šå¯»æ‰¾ OOS MAE æœ€å°çš„\n",
    "        valid_oos_mae_indices = result_df[\"Out-of-sample MAE\"].dropna().index\n",
    "        if not valid_oos_mae_indices.empty:\n",
    "            best_oos_model_idx = result_df.loc[valid_oos_mae_indices, \"Out-of-sample MAE\"].idxmin()\n",
    "            best_model_name_final = result_df.loc[best_oos_model_idx, \"Metrics\"]\n",
    "            print(f\"å›é€€åˆ° OOS MAE æœ€ä½³æ¨¡å‹: {best_model_name_final}\")\n",
    "            if best_model_name_final in result_df['Metrics'].values:\n",
    "                 result_df.loc[result_df['Metrics'] == best_model_name_final, 'Metrics'] = 'Best Linear Model (Fallback OOS)'\n",
    "            else: \n",
    "                 pass \n",
    "        else:\n",
    "             print(\"é”™è¯¯: æ‰€æœ‰æ¨¡å‹åœ¨ OOS å’Œ CV MAE ä¸Šå‡å¤±è´¥ã€‚å›é€€åˆ° OLSã€‚\")\n",
    "             best_model_name_final = 'OLS' # æœ€åæ‰‹æ®µ\n",
    "else:\n",
    "    print(\"è­¦å‘Š: æ‰€æœ‰æ¨¡å‹åœ¨ Cross-validation MAE ä¸Šå¤±è´¥ã€‚å›é€€åˆ° OOS MAE æœ€ä½³æ¨¡å‹ã€‚\")\n",
    "    valid_oos_mae_indices = result_df[\"Out-of-sample MAE\"].dropna().index\n",
    "    if not valid_oos_mae_indices.empty:\n",
    "        best_oos_model_idx = result_df.loc[valid_oos_mae_indices, \"Out-of-sample MAE\"].idxmin()\n",
    "        best_model_name_final = result_df.loc[best_oos_model_idx, \"Metrics\"]\n",
    "        print(f\"å›é€€åˆ° OOS MAE æœ€ä½³æ¨¡å‹: {best_model_name_final}\")\n",
    "        if best_model_name_final in result_df['Metrics'].values: # Check if it's not already 'Best Linear Model'\n",
    "             result_df.loc[result_df['Metrics'] == best_model_name_final, 'Metrics'] = 'Best Linear Model (Fallback OOS)'\n",
    "    else:\n",
    "        print(\"é”™è¯¯: æ‰€æœ‰æ¨¡å‹åœ¨ OOS å’Œ CV MAE ä¸Šå‡å¤±è´¥ã€‚å›é€€åˆ° OLSã€‚\")\n",
    "        best_model_name_final = 'OLS'\n",
    "\n",
    "\n",
    "result_df_formatted = result_df.copy()\n",
    "for col in result_df_formatted.columns:\n",
    "    if 'MAE' in col:\n",
    "        result_df_formatted[col] = result_df_formatted[col].apply(lambda x: f\"{x:.2f}\" if pd.notna(x) else 'NaN')\n",
    "    elif 'RMAE' in col:\n",
    "        result_df_formatted[col] = result_df_formatted[col].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else 'NaN')\n",
    "\n",
    "print(\"\\n--- 10. Model Performance Summary (MAE and RMAE for Original Rent Level) ---\")\n",
    "print(result_df_formatted.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# ğŸ”Ÿâ•1ï¸âƒ£ é€‰æ‹©æœ€ä¼˜æ¨¡å‹\n",
    "# æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨ç”± CV MAE è‡ªåŠ¨é€‰æ‹©çš„ best_model_name_final\n",
    "print(f\"\\n--- Selecting Final Model based on CV MAE: {best_model_name_final} ---\")\n",
    "if best_model_name_final in fitted_models and fitted_models[best_model_name_final] is not None:\n",
    "    best_model = fitted_models[best_model_name_final]\n",
    "    best_model_name_predict = best_model_name_final # è®°å½•å®é™…ä½¿ç”¨çš„æ¨¡å‹\n",
    "else:\n",
    "     # å¦‚æœ CV MAE é€‰æ‹©çš„æ¨¡å‹å¤±è´¥äº†\n",
    "     available_models = [m for m in ['Ridge', 'OLS', 'LASSO', 'ElasticNet'] if m in fitted_models and fitted_models[m] is not None]\n",
    "     if available_models:\n",
    "         best_model_name_predict = available_models[0]\n",
    "         best_model = fitted_models[best_model_name_predict]\n",
    "         print(f\"è­¦å‘Š: CV MAE æœ€ä½³æ¨¡å‹ '{best_model_name_final}' å¤±è´¥æˆ–ä¸å¯ç”¨ï¼Œå›é€€åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹ (ä¼˜å…ˆ Ridge/OLS): {best_model_name_predict}ã€‚\")\n",
    "     else:\n",
    "        raise RuntimeError(\"æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•ç»§ç»­è¿›è¡Œé¢„æµ‹ã€‚\")\n",
    "\n",
    "\n",
    "# ğŸ”Ÿâ•2ï¸âƒ£ é¢„æµ‹æµ‹è¯•é›† (ä½¿ç”¨è½¬æ¢åçš„ X_test)\n",
    "try:\n",
    "    print(f\"\\n--- ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹: {best_model_name_predict} ---\")\n",
    "    # (é¢„æµ‹å‰æ£€æŸ¥)\n",
    "    input_data_is_sparse = issparse(X_test_transformed)\n",
    "    input_data_nan = (input_data_is_sparse and np.isnan(X_test_transformed.data).any()) or \\\n",
    "                     (not input_data_is_sparse and np.isnan(X_test_transformed).any())\n",
    "    input_data_inf = (input_data_is_sparse and np.isinf(X_test_transformed.data).any()) or \\\n",
    "                     (not input_data_is_sparse and np.isinf(X_test_transformed).any())\n",
    "    if input_data_nan or input_data_inf:\n",
    "        # å°è¯•æœ€åä¸€æ¬¡æ¸…ç†\n",
    "        print(\"è­¦å‘Šï¼šæœ€ç»ˆé¢„æµ‹å‰åœ¨ X_test_transformed ä¸­æ£€æµ‹åˆ° NaN/Infï¼Œå°è¯•å¡«å…… 0ã€‚\")\n",
    "        if input_data_is_sparse:\n",
    "            X_test_transformed.data = np.nan_to_num(X_test_transformed.data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        else:\n",
    "            X_test_transformed = np.nan_to_num(X_test_transformed, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    test_pred_log = best_model.predict(X_test_transformed)\n",
    "\n",
    "    # ä¿æŒé¢„æµ‹åè¯Šæ–­\n",
    "    print(f\"é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼ (å‰ 5 ä¸ª): {test_pred_log[:5]}\")\n",
    "    finite_preds = test_pred_log[np.isfinite(test_pred_log)]\n",
    "    if finite_preds.size > 0:\n",
    "        print(f\"å¯¹æ•°ä»·æ ¼ç»Ÿè®¡ (æœ‰é™å€¼): Min={np.min(finite_preds):.2f}, Max={np.max(finite_preds):.2f}, Mean={np.mean(finite_preds):.2f}, Std={np.std(finite_preds):.2f}\")\n",
    "        if np.allclose(finite_preds, finite_preds[0], atol=1e-6): # ä½¿ç”¨æ›´å°çš„å®¹å¿åº¦æ£€æŸ¥\n",
    "            print(\"âš ï¸ è­¦å‘Š: æ‰€æœ‰é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼å‡ ä¹ç›¸åŒï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ è­¦å‘Š: é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼ä¸åŒ…å«ä»»ä½•æœ‰é™å€¼ï¼\")\n",
    "    if np.isnan(test_pred_log).any() or np.isinf(test_pred_log).any():\n",
    "        print(\"âŒ è­¦å‘Š: é¢„æµ‹çš„å¯¹æ•°ä»·æ ¼åŒ…å« NaN æˆ– Infï¼æ­£åœ¨å°è¯•æ¸…ç†...\")\n",
    "        median_log_pred = np.nanmedian(finite_preds) if finite_preds.size > 0 else np.log1p(np.median(y_train_price_no_outliers))\n",
    "        test_pred_log = np.nan_to_num(test_pred_log, nan=median_log_pred, posinf=np.log1p(np.finfo(np.float64).max / 10), neginf=-700)\n",
    "\n",
    "    test_pred_price = np.expm1(test_pred_log)\n",
    "    test_pred_price[test_pred_price < 0] = 0\n",
    "    large_finite_val = np.finfo(np.float64).max / 10\n",
    "    median_fallback = np.median(y_train_price_no_outliers) if len(y_train_price_no_outliers)>0 else 0\n",
    "    test_pred_price = np.nan_to_num(test_pred_price, nan=median_fallback,\n",
    "                                   posinf=large_finite_val, neginf=0.0)\n",
    "    test_pred_price = np.clip(test_pred_price, 0, large_finite_val)\n",
    "\n",
    "    # ä¿æŒæœ€ç»ˆä»·æ ¼é¢„æµ‹è¯Šæ–­\n",
    "    print(f\"æœ€ç»ˆé¢„æµ‹ä»·æ ¼ (å‰ 5 ä¸ª): {test_pred_price[:5]}\")\n",
    "    if test_pred_price.size > 0 and np.allclose(test_pred_price, test_pred_price[0], atol=1e-2): # å…è®¸å¾®å°å·®å¼‚\n",
    "         print(\"âŒ è­¦å‘Š: æ‰€æœ‰æœ€ç»ˆé¢„æµ‹ä»·æ ¼å‡ ä¹ç›¸åŒï¼\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    # ... (Fallback ä»£ç ä¿æŒä¸å˜) ...\n",
    "    import traceback\n",
    "    print(f\"æœ€ç»ˆé¢„æµ‹å‡ºé”™: {e}\")\n",
    "    traceback.print_exc() # <--- *** æ ¸å¿ƒä¿®æ­£ï¼šå–æ¶ˆæ³¨é‡Šä»¥æ‰“å°å®Œæ•´é”™è¯¯ ***\n",
    "    median_fallback = np.median(y_train_price_no_outliers) if len(y_train_price_no_outliers)>0 else 0\n",
    "    print(f\"é¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°é¢„æµ‹ä¸­ä½æ•°: {median_fallback}\")\n",
    "    test_pred_price = np.full(X_test_transformed.shape[0], median_fallback)\n",
    "\n",
    "# ğŸ”Ÿâ•3ï¸âƒ£ ç”Ÿæˆæäº¤æ–‡ä»¶\n",
    "# (ä»£ç ä¸å˜)\n",
    "submission_df = pd.DataFrame({\n",
    "    # ä¿®æ­£ç‚¹ï¼šä½¿ç”¨å¤§å†™ \"ID\"\n",
    "    \"ID\": test_data['ID'].values if 'ID' in test_data.columns and len(test_data['ID']) == X_test_transformed.shape[0] else np.arange(X_test_transformed.shape[0]),\n",
    "    \"prediction\": test_pred_price\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"prediction_rent.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ğŸ¯ Prediction file saved as **prediction_rent.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\")\n",
    "\n",
    "# ğŸ”Ÿâ•4ï¸âƒ£ ä¿å­˜æ¨¡å‹æ€§èƒ½è¡¨\n",
    "result_df.to_csv(\"performance_table_rent.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ğŸ“Š Model performance saved as **performance_table_rent.csv** (ä½¿ç”¨é€—å·åˆ†éš”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7753c-518e-42fe-8f25-3862bfbce00d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a1cdfab-5fd1-4e1d-9baf-cbb1c7f844b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- å¼€å§‹åˆå¹¶ ---\n",
      "æ­£åœ¨è¯»å– prediction_price.csv...\n",
      "âœ… prediction_price.csv åŠ è½½æˆåŠŸï¼ŒåŒ…å« 34017 è¡Œã€‚\n",
      "æ­£åœ¨è¯»å– prediction_rent.csv...\n",
      "âœ… prediction_rent.csv åŠ è½½æˆåŠŸï¼ŒåŒ…å« 9773 è¡Œã€‚\n",
      "æ­£åœ¨åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶...\n",
      "âœ… åˆå¹¶å®Œæˆï¼Œæ€»å…± 43790 è¡Œã€‚\n",
      "æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶åˆ° prediction.csv...\n",
      "ğŸ‰ æˆåŠŸï¼åˆå¹¶åçš„æäº¤æ–‡ä»¶å·²ä¿å­˜ä¸º prediction.csvã€‚\n",
      "--- åˆå¹¶è„šæœ¬æ‰§è¡Œå®Œæ¯• ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "price_file = \"prediction_price.csv\"  \n",
    "rent_file = \"prediction_rent.csv\"    \n",
    "output_file = \"prediction.csv\"       \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"--- å¼€å§‹åˆå¹¶ ---\")\n",
    "\n",
    "try:\n",
    "    # 1. è¯»å–æˆ¿ä»·é¢„æµ‹æ–‡ä»¶\n",
    "    print(f\"æ­£åœ¨è¯»å– {price_file}...\")\n",
    "    df_price = pd.read_csv(price_file)\n",
    "    print(f\"âœ… {price_file} åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(df_price)} è¡Œã€‚\")\n",
    "\n",
    "    # æª¢æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    if 'ID' not in df_price.columns or 'prediction' not in df_price.columns:\n",
    "        raise ValueError(f\"é”™è¯¯ï¼š{price_file} æ–‡ä»¶ç¼ºå°‘ 'ID' æˆ– 'prediction' åˆ—ã€‚\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {price_file}ã€‚è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹ã€‚\")\n",
    "    exit() # é€€å‡ºè…³æœ¬\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¯»å– {price_file} æ—¶å‡ºé”™ï¼š{e}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # 2. è¯»å–ç§Ÿé‡‘é¢„æµ‹æ–‡ä»¶\n",
    "    print(f\"æ­£åœ¨è¯»å– {rent_file}...\")\n",
    "    df_rent = pd.read_csv(rent_file)\n",
    "    print(f\"âœ… {rent_file} åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(df_rent)} è¡Œã€‚\")\n",
    "\n",
    "    # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨\n",
    "    if 'ID' not in df_rent.columns or 'prediction' not in df_rent.columns:\n",
    "        raise ValueError(f\"é”™è¯¯ï¼š{rent_file} æ–‡ä»¶ç¼ºå°‘ 'ID' æˆ– 'prediction' åˆ—ã€‚\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {rent_file}ã€‚è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹ã€‚\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¯»å– {rent_file} æ—¶å‡ºé”™ï¼š{e}\")\n",
    "    exit()\n",
    "\n",
    "# 3. åˆå¹¶\n",
    "#    pd.concat ç¡®è®¤æŒ‰è¡Œå †å  (axis=0)\n",
    "#    ignore_index=True ä¼šé‡æ–°ç”Ÿæˆä¸€ä¸ªè¿ç»­ç´¢å¼• (0, 1, 2, ...)\n",
    "print(f\"æ­£åœ¨åˆå¹¶ä¸¤ä¸ªæ–‡ä»¶...\")\n",
    "df_combined = pd.concat([df_price, df_rent], ignore_index=True)\n",
    "total_rows = len(df_combined)\n",
    "print(f\"âœ… åˆå¹¶å®Œæˆï¼Œæ€»å…± {total_rows} è¡Œã€‚\")\n",
    "\n",
    "# å¯é€‰ï¼šæ£€æŸ¥åˆå¹¶åçš„æ–‡ä»¶æ˜¯å¦æœ‰é‡å¤çš„ ID \n",
    "if df_combined['ID'].duplicated().any():\n",
    "    print(f\"âš ï¸ è­¦å‘Šï¼šåˆå¹¶åçš„æ–‡ä»¶ä¸­æ£€æµ‹åˆ°é‡å¤çš„ IDï¼è¯·æ£€æŸ¥æ‚¨çš„åŸå§‹é¢„æµ‹æ–‡ä»¶ã€‚\")\n",
    "    duplicate_ids = df_combined[df_combined['ID'].duplicated()]['ID'].unique()\n",
    "    print(f\"   é‡å¤çš„ ID ç¤ºä¾‹: {list(duplicate_ids[:5])}...\") # åªé¡¯ç¤ºå‰5å€‹\n",
    "\n",
    "# 4. ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶\n",
    "try:\n",
    "    print(f\"æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶åˆ° {output_file}...\")\n",
    "    # index=False: ä¸å°† DataFrame çš„ç´¢å¼•å†™å…¥ CSV æ–‡ä»¶\n",
    "    df_combined[['ID', 'prediction']].to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ‰ æˆåŠŸï¼åˆå¹¶åçš„æäº¤æ–‡ä»¶å·²ä¿å­˜ä¸º {output_file}ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜æ–‡ä»¶ {output_file} æ—¶å‡ºé”™ï¼š{e}\")\n",
    "\n",
    "print(f\"--- åˆå¹¶è„šæœ¬æ‰§è¡Œå®Œæ¯• ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
